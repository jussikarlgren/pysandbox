{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import corenlp\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import codecs\n",
    "\n",
    "import inspect # for logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensionality=2000\n",
    "denseness=10//dimensionality\n",
    "indexspace = {}\n",
    "globalfrequency = {}\n",
    "bign = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(msg,level=False):\n",
    "    if level:\n",
    "        print(inspect.stack()[1][3],\"(): \",msg,sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparseadd(onevec,othvec,weight=1,normalised=False):\n",
    "    if normalised:\n",
    "        onevec = normalise(onevec)\n",
    "        othvec = normalise(othvec)        \n",
    "    result={}\n",
    "    try:\n",
    "        for l in onevec:\n",
    "            result[l] = onevec[l]\n",
    "        for k in othvec:\n",
    "            if k in result:\n",
    "                result[k] = result[k]+othvec[k]*float(weight)\n",
    "            else:\n",
    "                result[k] = othvec[k]*float(weight)\n",
    "    except:\n",
    "        print(\"sparseadd(): error\")\n",
    "        raise\n",
    "    return result\n",
    "def sparsemultiply(onevec,othvec,weight=1):\n",
    "    result={}\n",
    "    try:\n",
    "        for l in onevec:\n",
    "            if l in othvec:\n",
    "                result[l] = onevec[l]*othvec[l]*float(weight)\n",
    "    except:\n",
    "        print(\"sparsemultiply(): error \")\n",
    "    return result\n",
    "def sparsexor(onevec,othvec):\n",
    "    result={}\n",
    "    try:\n",
    "        for l in range(len(onevec)):\n",
    "            if ((l in onevec) and not (l in othvec)):\n",
    "                result[l] = 1\n",
    "            if (not (l in onevec) and (l in othvec)):\n",
    "                result[l] = 1        \n",
    "    except:\n",
    "        print(\"sparsexor(): error \")\n",
    "    return result\n",
    "\n",
    "def newrandomvector(n,denseness):\n",
    "    vec = {}\n",
    "    k = int(n * denseness)\n",
    "    if k % 2 != 0:\n",
    "        k += 1\n",
    "    if (k > 0):# no need to be careful about this, right? and k % 2 == 0):\n",
    "        nonzeros = random.sample(list(range(n)),k)\n",
    "        negatives = random.sample(nonzeros,k//2)\n",
    "        for i in nonzeros:\n",
    "            vec[str(i)] = 1;\n",
    "        for i in negatives:\n",
    "            vec[str(i)] = -1;\n",
    "    return vec\n",
    "\n",
    "def newoperator(n):\n",
    "    k = 0.1\n",
    "    return newrandomvector(n,k)\n",
    "\n",
    "def sparsecosine(xvec,yvec,rounding=True,decimals=4):\n",
    "    x2 = 0\n",
    "    y2 = 0\n",
    "    xy = 0\n",
    "    try:\n",
    "        for i in xvec:\n",
    "            x2 += xvec[i]*xvec[i]\n",
    "    except KeyError:\n",
    "        print(\"sparsecosine(): error at position \",i)\n",
    "    try:\n",
    "        for j in yvec:\n",
    "            y2 += yvec[j]*yvec[j]\n",
    "            if j in xvec:\n",
    "                xy += xvec[j]*yvec[j]\n",
    "    except:\n",
    "        print(\"sparsecosine(): errors at position \",j)\n",
    "    if (x2*y2 == 0):\n",
    "        cos = 0\n",
    "    else:\n",
    "        cos = xy/(math.sqrt(x2)*math.sqrt(y2))\n",
    "    if (rounding):\n",
    "        cos=round(cos,decimals)\n",
    "    return cos\n",
    "\n",
    "def sparselength(vec,rounding=True):\n",
    "    x2 = 0\n",
    "    length=0\n",
    "    try:\n",
    "        for i in vec:\n",
    "            x2 += vec[i]*vec[i]\n",
    "    except KeyError:\n",
    "        print(\"sparselength(): error at position \",i)\n",
    "    if (x2 > 0):\n",
    "        length = math.sqrt(x2)\n",
    "    if (rounding):\n",
    "        length=round(length,4)\n",
    "    return length\n",
    "\n",
    "def comb(vec,k=0.1):\n",
    "    newvector={}\n",
    "    n=int(k*dimensionality/2)\n",
    "    sorted_items=sorted(vec.items(), key=lambda x:x[1])\n",
    "    bot=sorted_items[:n]\n",
    "    top=sorted_items[-n:]\n",
    "    for l in bot:\n",
    "        newvector[l[0]]=l[1]\n",
    "    for l in top:\n",
    "        newvector[l[0]]=l[1]\n",
    "    return newvector\n",
    "\n",
    "def sparsesum(vec):\n",
    "    s=0\n",
    "    for i in vec:\n",
    "        s += float(vec[i])\n",
    "    return s\n",
    "\n",
    "def normalise(vec):\n",
    "    newvector={}\n",
    "    vlen=sparselength(vec,False)\n",
    "    if (vlen > 0):\n",
    "        for i in vec:\n",
    "            newvector[i]=vec[i]/math.sqrt(vlen*vlen)\n",
    "    else:\n",
    "        newvector=vec\n",
    "    return newvector\n",
    "\n",
    "def modify(vec,factor):\n",
    "    newvector={}\n",
    "    for i in vec:\n",
    "        if (random.random() > factor):\n",
    "            newvector[i]=vec[i]\n",
    "        else:\n",
    "            newvector[i]=float(vec[i])*(0.5-random.random())*2.0\n",
    "    return newvector\n",
    "\n",
    "def createpermutation(k):\n",
    "    permutation=random.sample(range(k), k)\n",
    "    return permutation\n",
    "    \n",
    "def permute(vector,permutation):\n",
    "    newvector={}\n",
    "    try:\n",
    "        for i in range(len(permutation)):\n",
    "            if str(i) in vector:\n",
    "                newvector[str(permutation[i])]=vector[str(i)]\n",
    "    except:\n",
    "        newvector=vector\n",
    "        print(\"permute(): no permutation done, something wrong\")\n",
    "    return newvector\n",
    "\n",
    "def vectorsaturation(vector):\n",
    "    d = 0\n",
    "    for c in vector:\n",
    "        d += 1\n",
    "    return d\n",
    "\n",
    "def frequencyweight(word):\n",
    "    try:\n",
    "        w = math.exp(-300*math.pi* globalfrequency[word] / bign)\n",
    "    except KeyError:\n",
    "        w = 0.5\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chkwordspace(words,debug=False):\n",
    "    global globalfrequency\n",
    "    global indexspace\n",
    "    global bign\n",
    "    i = 0\n",
    "    for w in words:\n",
    "        bign += 1\n",
    "        if w in indexspace:\n",
    "            globalfrequency[w] += 1\n",
    "        else:\n",
    "            indexspace[w] = newrandomvector(dimensionality,denseness)\n",
    "            if debug:\n",
    "                print(\"chkwordspace(): \",w,\" is new and is now hallucinated.\")\n",
    "        globalfrequency[w] = 1\n",
    "chkwordspace([\"epsilon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clause:\n",
    "    cleanuppattern=re.compile(r'[\\.\\'\\!\\*\\?\\+,;\\:\\-\\/]+')\n",
    "    def __init__(self,string):\n",
    "        self.surfacestring=string\n",
    "        self.cleanedutterance=re.sub(clause.cleanuppattern,\"\",string)\n",
    "        self.tokens=self.cleanedutterance.lower().split()\n",
    "        self.agent=None\n",
    "        self.event=None\n",
    "        self.patient=None\n",
    "        self.instrument=None\n",
    "        self.location=None\n",
    "        self.manner=None\n",
    "        self.wps = 0.0\n",
    "        self.cpw = 0.0\n",
    "    def __str__(self):\n",
    "        return self.surfacestring\n",
    "    \n",
    "class referent:\n",
    "    def __init__(self, string):\n",
    "        self.surfacestring=string\n",
    "        self.definite=True\n",
    "        self.number=1\n",
    "        \n",
    "class event:\n",
    "    def __init__(self, string):\n",
    "        self.surfacestring=string    \n",
    "        self.negated=False    \n",
    "        self.adverbial=None    \n",
    "        self.tense=None #past present or future\n",
    "        self.aspect=None #ongoing perfect or pointwise\n",
    "        self.mood=None #indicative, irreal, potential, optative, not, imperative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.parse.stanford import StanfordDependencyParser\n",
    "#path_to_stanford_corenlp = '/usr/share/stanford-corenlp-full/'\n",
    "#parser_jar='stanford-corenlp.jar'\n",
    "#path_to_models = path_to_stanford_corenlp+'models/'\n",
    "#model_jar='stanford-english-corenlp-models.jar'\n",
    "#dependency_parser = StanfordDependencyParser(path_to_jar=path_to_stanford_corenlp+parser_jar, path_to_models_jar=path_to_models+model_jar)\n",
    "\n",
    "parser_client = corenlp.CoreNLPClient(annotators=\"tokenize ssplit pos natlog lemma depparse\".split())\n",
    "    \n",
    "    \n",
    "def semanticdepparse(string,debug=False,verbose=False):\n",
    "    depgraph=parser_client.annotate(string)\n",
    "    utterances=[]\n",
    "    for ss in depgraph.sentence:\n",
    "        for w in ss.token:\n",
    "            if w.lemma not in indexspace:\n",
    "                chkwordspace([w.lemma])\n",
    "        utterances.append(semanticdepparseprocess(string,ss,debug,verbose))\n",
    "    return utterances\n",
    "        \n",
    "def semanticdepparseprocess(string,ss,debug=False,verbose=False):\n",
    "    utterance=clause(string)\n",
    "    utterance.agent=referent(\"epsilon\")\n",
    "    utterance.patient=referent(\"epsilon\")\n",
    "    utterance.event=event(\"epsilon\")\n",
    "    scratch={}\n",
    "    negated=False\n",
    "    adverbial=None\n",
    "    if verbose:\n",
    "        print(ss)\n",
    "    if debug:        \n",
    "        i=1\n",
    "        print(\"root:\",ss.basicDependencies.root)\n",
    "        for w in ss.token:\n",
    "            print(i,\" \",w.lemma,\" \",w.pos)\n",
    "            i += 1\n",
    "        for e in ss.basicDependencies.edge:\n",
    "            print(e.source,ss.token[e.source-1].lemma,\"-\",e.dep,\"->\",e.target,ss.token[e.target-1].lemma) \n",
    "    i=1\n",
    "    cpw = 0\n",
    "    sentenceitems={}\n",
    "    sentencepos={}\n",
    "    for w in ss.token:\n",
    "        sentenceitems[i] = w.lemma\n",
    "        sentencepos[i] = w.pos\n",
    "        scratch[i] = False\n",
    "        i += 1\n",
    "        cpw += len(w.originalText)\n",
    "    utterance.wps = i\n",
    "    if i > 0: \n",
    "        utterance.cpw = cpw/i\n",
    "    root = ss.basicDependencies.root[0] #only one root for now fix this!\n",
    "    utterance.event=event(sentenceitems[root])  \n",
    "    utterance.event.tense=\"PRESENT\"\n",
    "    if sentencepos[root] == \"VBD\":\n",
    "        utterance.event.tense=\"PAST\" \n",
    "    if sentencepos[root] == \"VBN\":\n",
    "        utterance.event.tense=\"PAST\" \n",
    "    for edge in ss.basicDependencies.edge:\n",
    "        if debug:\n",
    "            print(edge.source,sentenceitems[edge.source],\"-\",edge.dep,\"->\",edge.target,sentenceitems[edge.target])\n",
    "        if edge.dep == 'neg':\n",
    "            negated=True\n",
    "        elif edge.dep == 'nsubj':\n",
    "            utterance.agent=referent(sentenceitems[edge.target])\n",
    "            if edge.target in scratch: \n",
    "                if scratch[edge.target]==\"def\":\n",
    "                    utterance.agent.definite=True\n",
    "                elif scratch[edge.target]==\"indef\":\n",
    "                    utterance.agent.definite=False\n",
    "        elif edge.dep == 'dobj':\n",
    "                utterance.patient=referent(sentenceitems[edge.target])\n",
    "                if edge.target in scratch: \n",
    "                    if scratch[edge.target]==\"def\":\n",
    "                        utterance.patient.definite=True\n",
    "                    elif scratch[edge.target]==\"indef\":\n",
    "                        utterance.patient.definite=False\n",
    "        elif edge.dep == 'advmod':\n",
    "                adverbial=sentenceitems[edge.target]            \n",
    "        elif edge.dep == 'det':\n",
    "                if (sentenceitems[edge.target]==\"a\" or sentenceitems[edge.target]==\"an\"):\n",
    "                    scratch[edge.source]=\"indef\"\n",
    "                elif (sentenceitems[edge.target]==\"the\"):\n",
    "                    scratch[edge.target]=\"def\"\n",
    "        elif edge.dep == 'nmod:poss':\n",
    "                scratch[edge.target]=\"def\"\n",
    "        elif edge.dep == 'aux':\n",
    "                if (sentenceitems[edge.target]==\"have\"):\n",
    "                    scratch['aux']=\"have\"\n",
    "                if (sentenceitems[edge.target]==\"do\"):\n",
    "                    scratch['aux']=\"do\"\n",
    "                    if (sentencepos[edge.target] == \"VBD\"):\n",
    "                        utterance.event.tense = \"PAST\"\n",
    "                if (sentenceitems[edge.target]==\"will\"):\n",
    "                    scratch['aux']=\"will\"\n",
    "                if (sentenceitems[edge.target]==\"shall\"):\n",
    "                    scratch['aux']=\"shall\"\n",
    "    try:\n",
    "        if debug: \n",
    "            print(sentenceitems[root],\" \",sentencepos[root])\n",
    "        if sentencepos[root] == \"VB\" and ('aux' in scratch and (scratch['aux'] == \"will\" or scratch['aux'] == \"shall\")):\n",
    "            utterance.event.tense=\"FUTURE\"\n",
    "    except:\n",
    "        print(\"semanticdepparse(): tense situation in \"+string)\n",
    "    try:\n",
    "        utterance.event.adverbial=adverbial\n",
    "    except:\n",
    "        print(\"semanticdepparse(): adverbial blowout in \"+string)\n",
    "    try:\n",
    "        utterance.event.negated=negated\n",
    "    except:\n",
    "        print(\"semanticdepparse(): negation mismatch in \"+string)\n",
    "    return utterance\n",
    "\n",
    "\n",
    "def qudepparse(string,debug=False,verbose=False):\n",
    "    depgraph=parser_client.annotate(string)\n",
    "    utterances=[]\n",
    "    for ss in depgraph.sentence:\n",
    "        for w in ss.token:\n",
    "            if w.lemma not in indexspace:\n",
    "                chkwordspace([w.lemma])\n",
    "        utterances.append(qudepparseprocess(string,ss,debug))\n",
    "    return utterances\n",
    "        \n",
    "def qudepparseprocess(string,ss,debug=False):\n",
    "    negated=False\n",
    "    target = \"epsilon\"\n",
    "    adverbial=\"epsilon\"\n",
    "    subject=\"epsilon\"\n",
    "    verb=\"epsilon\"\n",
    "    qu=\"epsilon\"\n",
    "    scratch = {}\n",
    "    question = {}\n",
    "    if debug:        \n",
    "        i=1\n",
    "        print(\"root:\",ss.basicDependencies.root)\n",
    "        for w in ss.token:\n",
    "            print(i,\" \",w.lemma,\" \",w.pos)\n",
    "            i += 1\n",
    "        for e in ss.basicDependencies.edge:\n",
    "            print(e.source,ss.token[e.source-1].lemma,\"-\",e.dep,\"->\",e.target,ss.token[e.target-1].lemma) \n",
    "\n",
    "    sentenceitems={}\n",
    "    sentenceitems[\"epsilon\"] = None\n",
    "    sentencepos={}\n",
    "    root = ss.basicDependencies.root[0] #only one root for now fix this!\n",
    "    qu = root\n",
    "    target = root\n",
    "    verb = root\n",
    "    i=1\n",
    "    for w in ss.token:\n",
    "        sentenceitems[i] = w.lemma\n",
    "        sentencepos[i] = w.pos\n",
    "        scratch[i] = False\n",
    "        if w.pos == \"WP\":\n",
    "            qu = i\n",
    "        if w.pos == \"WRB\":\n",
    "            qu = i\n",
    "        i += 1\n",
    "#        if w.pos == \"VBD\":\n",
    "#            verb = i\n",
    "#        if w.pos == \"VBN\":\n",
    "#            verb = i\n",
    "#        if w.pos == \"VBZ\":\n",
    "#            verb = i\n",
    "#        if w.pos == \"VBP\":\n",
    "#            verb = i            \n",
    "    tense=\"PRESENT\"\n",
    "    if sentencepos[root] == \"VBD\":\n",
    "        tense=\"PAST\" \n",
    "    if sentencepos[root] == \"VBN\":\n",
    "        tense=\"PAST\" \n",
    "    for edge in ss.basicDependencies.edge:\n",
    "        if debug:\n",
    "            print(edge.source,sentenceitems[edge.source],\"-\",edge.dep,\"->\",edge.target,sentenceitems[edge.target])\n",
    "        if edge.dep == 'nsubj':\n",
    "            subject= edge.target\n",
    "        elif edge.dep == 'neg':\n",
    "            negated=True\n",
    "        elif edge.dep == 'advmod':\n",
    "            if edge.target == qu:\n",
    "                if edge.source == root:\n",
    "                    target = \"epsilon\"\n",
    "                else: \n",
    "                    target = edge.source\n",
    "            else:\n",
    "                adverbial = edge.target\n",
    "        elif edge.dep == 'cop':\n",
    "            if edge.target == qu:\n",
    "                target = edge.source\n",
    "            else:\n",
    "                adverbial = edge.target\n",
    "        elif edge.dep == 'aux':\n",
    "            if (sentenceitems[edge.target]==\"have\"):\n",
    "                scratch['aux']=\"have\"\n",
    "            if (sentenceitems[edge.target]==\"do\"):\n",
    "                scratch['aux']=\"do\"\n",
    "            if (sentencepos[edge.target] == \"VBD\"):\n",
    "                utterance.event.tense = \"PAST\"\n",
    "            if (sentenceitems[edge.target]==\"will\"):\n",
    "                scratch['aux']=\"will\"\n",
    "            if (sentenceitems[edge.target]==\"shall\"):\n",
    "                scratch['aux']=\"shall\"\n",
    "    if target == \"epsilon\":\n",
    "        if subject != \"epsilon\":\n",
    "            target = subject\n",
    "    try:\n",
    "        if debug: \n",
    "            print(sentenceitems[root],\" \",sentencepos[root])\n",
    "        if sentencepos[root] == \"VB\" and ('aux' in scratch and (scratch['aux'] == \"will\" or scratch['aux'] == \"shall\")):\n",
    "            tense=\"FUTURE\"\n",
    "    except:\n",
    "        logger(\"tense situation in \"+string,True)\n",
    "    question[\"question\"] = sentenceitems[qu]\n",
    "    question[\"target\"] = sentenceitems[target]\n",
    "    question[\"verb\"] = sentenceitems[verb]\n",
    "    question[\"adverbial\"] = sentenceitems[adverbial]\n",
    "    question[\"subject\"] = sentenceitems[subject]\n",
    "    question[\"tense\"] = tense\n",
    "    question[\"negated\"] = negated\n",
    "    if debug:\n",
    "        print(question[\"question\"],question[\"target\"],question[\"verb\"],question[\"adverbial\"],question[\"subject\"],question[\"tense\"],question[\"negated\"],sep=\"\\t\")\n",
    "    return question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=qudepparse(\"How long is the river Peugeot\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'be', 'Peugeot', False, 'PRESENT', None, 'long']"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a[0].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initconstantsandpermutations(dimensionality=2000,densenessforabstracts=0.20):\n",
    "\n",
    "    global definitepermutation\n",
    "    global indefinitepermutation\n",
    "    global futurepermutation\n",
    "    global pasttensepermutation\n",
    "    global presenttensepermutation\n",
    "    global agentpermutation\n",
    "    global patientpermutation\n",
    "    global eventpermutation\n",
    "    global instrumentpermutation\n",
    "    global locationpermutation\n",
    "    global mannerpermutation\n",
    "    global negationpermutation\n",
    "    global adverbialpermutation\n",
    "\n",
    "    global previouspermutation\n",
    "\n",
    "    global tensepermutation\n",
    "\n",
    "    global agentabstract\n",
    "    global patientabstract\n",
    "    global eventabstract\n",
    "    global instrumentabstract\n",
    "    global locationabstract\n",
    "    global mannerabstract\n",
    "    global negationabstract\n",
    "\n",
    "    global presentabstract\n",
    "    global pastabstract\n",
    "    global progressiveabstract\n",
    "    global futureabstract\n",
    "    \n",
    "    global tagpermutation\n",
    "\n",
    "    definitepermutation=createpermutation(dimensionality)\n",
    "    indefinitepermutation=createpermutation(dimensionality)\n",
    "    futurepermutation=createpermutation(dimensionality)\n",
    "    pasttensepermutation=createpermutation(dimensionality)\n",
    "    presenttensepermutation=createpermutation(dimensionality)\n",
    "    agentpermutation=createpermutation(dimensionality)\n",
    "    patientpermutation=createpermutation(dimensionality)\n",
    "    eventpermutation=createpermutation(dimensionality)\n",
    "    instrumentpermutation=createpermutation(dimensionality)\n",
    "    locationpermutation=createpermutation(dimensionality)\n",
    "    mannerpermutation=createpermutation(dimensionality)\n",
    "    negationpermutation=createpermutation(dimensionality)\n",
    "    adverbialpermutation=createpermutation(dimensionality)\n",
    "    #sequence\n",
    "    previouspermutation=createpermutation(dimensionality)\n",
    "\n",
    "    tensepermutation=createpermutation(dimensionality)\n",
    "\n",
    "    agentabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    patientabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    eventabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    instrumentabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    locationabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    mannerabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    negationabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "\n",
    "    presentabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    pastabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    progressiveabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    futureabstract=newrandomvector(dimensionality,densenessforabstracts)\n",
    "    \n",
    "    tagpermutation=createpermutation(dimensionality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utterancevector(utterance,debug=False,lexical=True,morphology=True,semanticroles=True,constructional=True):\n",
    "    baremorphology=morphology\n",
    "    combinedmorphology=lexical\n",
    "    uttvec={}\n",
    "    momvec = {}\n",
    "    if debug:\n",
    "        debugvec={}\n",
    "    if lexical:     # add in referent, using its context vector meaning more or less the concept it refers to\n",
    "        if (utterance.patient): # took out comb\n",
    "            if debug:\n",
    "                momvec = uttvec\n",
    "            uttvec=sparseadd(uttvec, indexspace[utterance.patient.surfacestring],frequencyweight(utterance.patient.surfacestring)) #, math.log( bign / globalfrequency[utterance.patient.surfacestring]))\n",
    "            logger(\"adding \"+utterance.patient.surfacestring,debug)\n",
    "            logger(\"shift \"+str(sparsecosine(momvec,uttvec)),debug)\n",
    "        if (utterance.event):\n",
    "            if debug:\n",
    "                momvec = uttvec\n",
    "            uttvec=sparseadd(uttvec,indexspace[utterance.event.surfacestring],frequencyweight(utterance.event.surfacestring)) #, math.log( bign / globalfrequency[utterance.event.surfacestring]))\n",
    "            logger(\"adding \"+utterance.event.surfacestring,debug)\n",
    "            logger(\"shift \"+str(sparsecosine(momvec,uttvec)),debug)\n",
    "        if (utterance.agent):\n",
    "            if debug:\n",
    "                momvec = uttvec\n",
    "            uttvec=sparseadd(uttvec,indexspace[utterance.agent.surfacestring],frequencyweight(utterance.agent.surfacestring)) #, math.log( bign / globalfrequency[utterance.agent.surfacestring]))\n",
    "            logger(\"adding \"+utterance.agent.surfacestring,debug)\n",
    "            logger(\"shift \"+str(sparsecosine(momvec,uttvec)),debug)\n",
    "    if debug:\n",
    "        print(\"utterancevec() lexicon: \",sparsecosine(uttvec,debugvec))\n",
    "        debugvec=uttvec\n",
    "    if morphology:     # add in morphological information about the roles of the referent\n",
    "        morphvec={}\n",
    "        if (utterance.agent):\n",
    "            if (utterance.agent.definite):\n",
    "                morphvec=sparseadd(morphvec,permute(indexspace[utterance.agent.surfacestring],definitepermutation))\n",
    "            else:\n",
    "                morphvec=sparseadd(morphvec,permute(indexspace[utterance.agent.surfacestring],indefinitepermutation))\n",
    "        if (utterance.patient):\n",
    "            if (utterance.patient.definite):\n",
    "                    morphvec=sparseadd(morphvec,permute(indexspace[utterance.patient.surfacestring],definitepermutation))\n",
    "            else:\n",
    "     #       uttvec=sparseadd(uttvec,normalise(permute(comb(vectorspace[utterance.patient.surfacestring]),indefinite)))  \n",
    "                    morphvec=sparseadd(morphvec,permute(indexspace[utterance.patient.surfacestring],indefinitepermutation))\n",
    "        if (utterance.event):\n",
    "            if (utterance.event.tense==\"FUTURE\"):\n",
    "                    morphvec=sparseadd(morphvec,permute(indexspace[utterance.event.surfacestring],futuretensepermutation))\n",
    "            if (utterance.event.tense==\"PAST\"):\n",
    "                    morphvec=sparseadd(morphvec,permute(indexspace[utterance.event.surfacestring],pasttensepermutation))\n",
    "            else:\n",
    "                    morphvec=sparseadd(morphvec,permute(indexspace[utterance.event.surfacestring],presenttensepermutation))\n",
    "        uttvec=sparseadd(normalise(uttvec),normalise(morphvec))\n",
    "        if debug:\n",
    "            print(\"utterancevec() morphology: \",sparsecosine(uttvec,debugvec))\n",
    "            debugvec=uttvec\n",
    "    if semanticroles:\n",
    "        semroles = {}\n",
    "        if (utterance.patient):\n",
    "                semroles=sparseadd(semroles,permute(indexspace[utterance.patient.surfacestring],patientpermutation))\n",
    "        if (utterance.event):\n",
    "                semroles=sparseadd(semroles,permute(indexspace[utterance.event.surfacestring],eventpermutation))\n",
    "        if (utterance.event.negated):\n",
    "                semroles=sparseadd(semroles,permute(indexspace[utterance.event.surfacestring],negationpermutation))\n",
    "        if (utterance.event.adverbial):\n",
    "                semroles=sparseadd(semroles,permute(indexspace[utterance.event.adverbial],adverbialpermutation))\n",
    "        if (utterance.agent):\n",
    "                semroles=sparseadd(semroles,permute(indexspace[utterance.agent.surfacestring],agentpermutation))\n",
    "        uttvec=sparseadd(normalise(uttvec),normalise(semroles))\n",
    "        if debug:\n",
    "            print(\"utterancevec() semanticrole: \",sparsecosine(uttvec,debugvec))\n",
    "            debugvec=uttvec\n",
    "    if constructional:    # add in morphological information about the roles, no account of referent\n",
    "        constrvec={}\n",
    "        if (utterance.agent):\n",
    "            if (utterance.agent.definite):\n",
    "                constrvec=sparseadd(constrvec,permute(agentabstract,definitepermutation))\n",
    "            else:\n",
    "                constrvec=sparseadd(constrvec,permute(agentabstract,indefinitepermutation))\n",
    "        if (utterance.patient):\n",
    "            if (utterance.patient.definite):\n",
    "                constrvec=sparseadd(constrvec,permute(patientabstract,definitepermutation))\n",
    "            else:\n",
    "                constrvec=sparseadd(constrvec,permute(patientabstract,indefinitepermutation))\n",
    "        if (utterance.event):\n",
    "            if (utterance.event.tense==\"FUTURE\"):\n",
    "                constrvec=sparseadd(constrvec,permute(futureabstract,tensepermutation))\n",
    "            elif (utterance.event.tense==\"PAST\"):\n",
    "                constrvec=sparseadd(constrvec,permute(pastabstract,tensepermutation))\n",
    "            else:\n",
    "                constrvec=sparseadd(constrvec,permute(presentabstract,tensepermutation)) \n",
    "        if (utterance.event.negated):\n",
    "            uttvec=sparseadd(normalise(uttvec),normalise(permute(negationabstract,negationpermutation)))\n",
    "        uttvec=sparseadd(normalise(uttvec),normalise(constrvec))\n",
    "        if debug:\n",
    "            print(\"utterancevec() constructions: \",sparsecosine(uttvec,debugvec))\n",
    "            debugvec=uttvec\n",
    "    return uttvec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gettags(quinfilename=questionfile):\n",
    "    global taglist\n",
    "    global speclist\n",
    "    global tagspec\n",
    "    global spectag\n",
    "    taglist = []\n",
    "    speclist = []\n",
    "    tagspec = {}\n",
    "    spectag = {}\n",
    "    qupattern = re.compile(r'(\\w+):(\\w+)\\s+(.*)$')\n",
    "    with codecs.open(quinfilename,\"r\", encoding='utf-8') as infile:        \n",
    "        quline = infile.readline().rstrip()\n",
    "        while quline:\n",
    "            m = qupattern.match(quline)\n",
    "            if m:\n",
    "                text = m.groups()[2]\n",
    "                tag = m.groups()[0]\n",
    "                spec = m.groups()[1]\n",
    "                if tag not in taglist:\n",
    "                    taglist.append(tag)\n",
    "                    spectag[tag] = []\n",
    "                if spec not in speclist:\n",
    "                    speclist.append(spec)\n",
    "                    tagspec[spec] = tag\n",
    "                    spectag[tag].append(spec)\n",
    "            quline = infile.readline().rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['manner',\n",
       " 'cremat',\n",
       " 'animal',\n",
       " 'exp',\n",
       " 'ind',\n",
       " 'gr',\n",
       " 'title',\n",
       " 'def',\n",
       " 'date',\n",
       " 'reason',\n",
       " 'event',\n",
       " 'state',\n",
       " 'desc',\n",
       " 'count',\n",
       " 'other',\n",
       " 'letter',\n",
       " 'religion',\n",
       " 'food',\n",
       " 'country',\n",
       " 'color',\n",
       " 'termeq',\n",
       " 'city',\n",
       " 'body',\n",
       " 'dismed',\n",
       " 'mount',\n",
       " 'money',\n",
       " 'product',\n",
       " 'period',\n",
       " 'substance',\n",
       " 'sport',\n",
       " 'plant',\n",
       " 'techmeth',\n",
       " 'volsize',\n",
       " 'instru',\n",
       " 'abb',\n",
       " 'speed',\n",
       " 'word',\n",
       " 'lang',\n",
       " 'perc',\n",
       " 'code',\n",
       " 'dist',\n",
       " 'temp',\n",
       " 'symbol',\n",
       " 'ord',\n",
       " 'veh',\n",
       " 'weight',\n",
       " 'currency']"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speclist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightfunction(word):\n",
    "    if word in globalfrequency:\n",
    "        return globalfrequency[word]\n",
    "    elif word == \"be\":\n",
    "        return 0.1\n",
    "    else: \n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addconfusion(facit,predicted):\n",
    "    global confusionmatrix\n",
    "    if facit in confusionmatrix:\n",
    "        if predicted in confusionmatrix[facit]:\n",
    "            confusionmatrix[facit][predicted] += 1\n",
    "        else:\n",
    "            confusionmatrix[facit][predicted] = 1\n",
    "    else: \n",
    "        confusionmatrix[facit] = {}\n",
    "        confusionmatrix[facit][predicted] = 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESC\n",
      "manner\n",
      "How did serfdom develop in and then leave Russia ?\n",
      "{}\n",
      "{'115': -0.035444060250417034, '1037': -0.035444060250417034, '1525': -0.035444060250417034, '1876': 0.035444060250417034, '434': 0.035444060250417034, '1022': -0.035444060250417034, '941': -0.035444060250417034, '336': -0.035444060250417034, '1049': -0.035444060250417034, '587': 0.035444060250417034, '1821': 0.035444060250417034, '1777': -0.035444060250417034, '1613': 0.035444060250417034, '1193': -0.07088812050083407, '1080': 0.035444060250417034, '1595': -0.035444060250417034, '1158': -0.035444060250417034, '1693': 0.0, '1594': 0.035444060250417034, '865': -0.035444060250417034, '42': -0.035444060250417034, '400': 0.035444060250417034, '387': 0.035444060250417034, '782': -0.035444060250417034, '1235': -0.035444060250417034, '639': 0.035444060250417034, '1833': 0.07088812050083407, '1722': 0.035444060250417034, '1998': 0.035444060250417034, '499': -0.035444060250417034, '893': 0.035444060250417034, '1272': 0.035444060250417034, '20': -0.035444060250417034, '1346': 0.0, '599': 0.035444060250417034, '1901': 0.035444060250417034, '416': 0.0, '104': 0.0, '1661': 0.035444060250417034, '991': -0.035444060250417034, '1014': -0.035444060250417034, '1442': -0.035444060250417034, '180': 0.035444060250417034, '945': 0.035444060250417034, '736': 0.035444060250417034, '1674': -0.035444060250417034, '328': -0.035444060250417034, '546': 0.035444060250417034, '1449': -0.035444060250417034, '1863': -0.035444060250417034, '568': 0.035444060250417034, '213': -0.035444060250417034, '329': -0.035444060250417034, '914': 0.035444060250417034, '1013': 0.035444060250417034, '1879': 0.035444060250417034, '532': -0.035444060250417034, '849': 0.035444060250417034, '1811': 0.035444060250417034, '1072': 0.035444060250417034, '105': 0.035444060250417034, '1269': -0.035444060250417034, '85': -0.035444060250417034, '1510': -0.035444060250417034, '1921': -0.07088812050083407, '729': -0.035444060250417034, '171': -0.035444060250417034, '507': 0.035444060250417034, '479': 0.035444060250417034, '82': 0.035444060250417034, '466': 0.035444060250417034, '739': -0.035444060250417034, '957': -0.035444060250417034, '338': 0.035444060250417034, '1552': -0.035444060250417034, '1302': -0.035444060250417034, '1968': 0.035444060250417034, '1409': 0.07088812050083407, '1540': 0.035444060250417034, '1754': 0.07088812050083407, '807': 0.0, '537': -0.035444060250417034, '1286': -0.035444060250417034, '701': -0.035444060250417034, '420': 0.035444060250417034, '218': -0.07088812050083407, '689': 0.0, '523': -0.035444060250417034, '274': -0.035444060250417034, '1728': -0.035444060250417034, '618': 0.0, '225': -0.035444060250417034, '1812': -0.035444060250417034, '541': -0.035444060250417034, '60': 0.035444060250417034, '1119': -0.07088812050083407, '932': -0.035444060250417034, '1262': -0.035444060250417034, '1208': 0.035444060250417034, '177': 0.035444060250417034, '361': -0.035444060250417034, '1800': 0.035444060250417034, '1476': 0.035444060250417034, '191': -0.035444060250417034, '1989': 0.035444060250417034, '1200': 0.035444060250417034, '1736': -0.07088812050083407, '457': -0.035444060250417034, '1738': -0.035444060250417034, '1899': 0.035444060250417034, '1177': 0.035444060250417034, '510': 0.035444060250417034, '1328': 0.035444060250417034, '1880': -0.035444060250417034, '1848': -0.035444060250417034, '482': -0.035444060250417034, '1402': 0.0, '881': 0.035444060250417034, '1741': -0.035444060250417034, '887': 0.035444060250417034, '1365': -0.035444060250417034, '943': -0.035444060250417034, '1922': 0.035444060250417034, '717': 0.035444060250417034, '790': 0.035444060250417034, '506': 0.035444060250417034, '1577': -0.035444060250417034, '1335': -0.035444060250417034, '1336': 0.07088812050083407, '677': -0.035444060250417034, '1856': -0.035444060250417034, '942': 0.035444060250417034, '1066': 0.035444060250417034, '308': 0.035444060250417034, '663': 0.035444060250417034, '1228': 0.035444060250417034, '193': -0.035444060250417034, '1490': 0.035444060250417034, '877': 0.07088812050083407, '1678': 0.035444060250417034, '1134': 0.035444060250417034, '1097': 0.035444060250417034, '207': 0.0, '688': -0.035444060250417034, '934': 0.035444060250417034, '445': 0.0, '964': -0.035444060250417034, '33': 0.035444060250417034, '1850': 0.035444060250417034, '972': 0.0, '1699': 0.0, '1165': -0.035444060250417034, '64': 0.035444060250417034, '1373': 0.035444060250417034, '364': -0.035444060250417034, '1481': -0.035444060250417034, '409': -0.035444060250417034, '831': -0.035444060250417034, '23': -0.035444060250417034, '1508': -0.07088812050083407, '21': -0.035444060250417034, '1896': -0.035444060250417034, '43': 0.035444060250417034, '1267': 0.0, '1089': -0.035444060250417034, '494': 0.035444060250417034, '1497': -0.035444060250417034, '83': -0.035444060250417034, '825': -0.035444060250417034, '481': 0.035444060250417034, '559': -0.035444060250417034, '1176': 0.035444060250417034, '1027': -0.035444060250417034, '312': -0.035444060250417034, '453': 0.035444060250417034, '1559': -0.035444060250417034, '648': -0.035444060250417034, '1333': 0.035444060250417034, '1301': -0.035444060250417034, '172': -0.035444060250417034, '262': -0.035444060250417034, '169': -0.07088812050083407, '1232': 0.035444060250417034, '1941': 0.07088812050083407, '1394': -0.035444060250417034, '575': -0.035444060250417034, '1513': 0.035444060250417034, '1798': -0.035444060250417034, '1564': 0.035444060250417034, '1638': 0.035444060250417034, '1737': 0.035444060250417034, '1331': 0.035444060250417034, '1172': 0.07088812050083407, '796': 0.035444060250417034, '244': 0.035444060250417034, '651': 0.035444060250417034, '714': 0.035444060250417034, '257': -0.035444060250417034, '1766': -0.035444060250417034, '1910': -0.035444060250417034, '716': 0.035444060250417034, '1015': -0.035444060250417034, '1281': -0.035444060250417034, '1834': 0.035444060250417034, '596': -0.035444060250417034, '92': -0.035444060250417034, '1827': -0.035444060250417034, '1904': 0.035444060250417034, '1137': 0.035444060250417034, '211': 0.035444060250417034, '91': -0.035444060250417034, '256': 0.035444060250417034, '1530': 0.035444060250417034, '1702': 0.07088812050083407, '1204': -0.035444060250417034, '1294': -0.035444060250417034, '299': -0.035444060250417034, '1280': 0.035444060250417034, '124': 0.035444060250417034, '1888': -0.035444060250417034, '48': 0.035444060250417034, '359': 0.035444060250417034, '1147': -0.035444060250417034, '841': -0.035444060250417034, '296': -0.035444060250417034, '1044': 0.035444060250417034, '1491': 0.07088812050083407, '1054': 0.035444060250417034, '498': 0.035444060250417034, '1683': -0.035444060250417034, '1384': -0.035444060250417034, '1579': 0.035444060250417034, '1327': -0.035444060250417034, '159': 0.035444060250417034, '1562': 0.035444060250417034, '571': -0.035444060250417034, '616': -0.035444060250417034, '703': 0.035444060250417034, '829': 0.035444060250417034, '221': -0.035444060250417034, '1264': -0.035444060250417034, '108': 0.035444060250417034, '756': 0.0, '1450': -0.035444060250417034, '1690': -0.035444060250417034, '1785': -0.035444060250417034, '1359': -0.07088812050083407, '743': 0.035444060250417034, '1953': 0.035444060250417034, '1345': 0.035444060250417034, '1435': 0.035444060250417034, '1342': -0.035444060250417034, '1033': 0.07088812050083407, '406': 0.07088812050083407, '1428': -0.035444060250417034, '1214': 0.035444060250417034, '98': -0.035444060250417034, '1486': 0.0, '251': 0.035444060250417034, '1416': -0.035444060250417034, '1843': -0.035444060250417034, '1322': -0.035444060250417034, '971': -0.035444060250417034, '983': -0.035444060250417034, '1649': 0.035444060250417034, '1688': -0.035444060250417034, '1644': 0.035444060250417034, '120': -0.035444060250417034, '6': 0.035444060250417034, '1691': -0.035444060250417034, '1706': 0.07088812050083407, '415': 0.035444060250417034, '655': 0.035444060250417034, '647': -0.035444060250417034, '997': -0.035444060250417034, '38': -0.035444060250417034, '1753': 0.07088812050083407, '1310': -0.035444060250417034, '1807': -0.035444060250417034, '396': -0.035444060250417034, '418': 0.035444060250417034, '768': 0.035444060250417034, '617': -0.035444060250417034, '1816': 0.035444060250417034, '925': -0.035444060250417034, '1959': -0.035444060250417034, '96': 0.035444060250417034, '1115': 0.035444060250417034, '1482': -0.035444060250417034, '71': -0.035444060250417034, '754': 0.035444060250417034, '181': -0.035444060250417034, '665': 0.0, '903': 0.035444060250417034, '1401': 0.035444060250417034, '1979': 0.035444060250417034, '793': -0.035444060250417034, '1937': 0.035444060250417034, '306': 0.035444060250417034, '1069': 0.035444060250417034, '608': 0.035444060250417034, '1960': 0.035444060250417034, '313': 0.0, '45': 0.035444060250417034, '1271': -0.035444060250417034, '1556': 0.035444060250417034, '1454': -0.035444060250417034, '106': 0.035444060250417034, '263': -0.035444060250417034, '1000': 0.035444060250417034, '1828': 0.035444060250417034, '1632': 0.035444060250417034, '1131': -0.035444060250417034, '1207': 0.035444060250417034, '138': 0.035444060250417034, '35': 0.035444060250417034, '73': -0.035444060250417034, '1866': 0.035444060250417034, '1783': -0.035444060250417034, '561': 0.035444060250417034, '196': 0.035444060250417034, '627': -0.07088812050083407, '1225': -0.035444060250417034, '102': 0.035444060250417034, '1217': -0.035444060250417034, '1735': -0.035444060250417034, '774': -0.035444060250417034, '333': 0.035444060250417034, '723': 0.035444060250417034, '410': 0.035444060250417034, '441': 0.035444060250417034, '1211': 0.035444060250417034, '1913': 0.035444060250417034, '814': 0.035444060250417034, '1083': -0.035444060250417034, '1916': -0.035444060250417034, '1682': 0.035444060250417034, '1460': -0.035444060250417034, '1356': 0.035444060250417034, '868': 0.035444060250417034, '712': -0.035444060250417034, '1881': 0.035444060250417034, '454': 0.0, '233': -0.035444060250417034, '534': -0.035444060250417034, '799': -0.035444060250417034, '1201': -0.035444060250417034, '911': -0.035444060250417034, '528': 0.035444060250417034, '1284': -0.035444060250417034, '746': 0.035444060250417034, '234': 0.035444060250417034, '1245': 0.0, '50': -0.035444060250417034, '572': 0.035444060250417034, '1157': 0.035444060250417034, '935': -0.035444060250417034, '1320': 0.035444060250417034, '67': 0.035444060250417034, '1521': -0.035444060250417034, '853': -0.035444060250417034, '956': 0.035444060250417034, '1489': -0.035444060250417034, '1174': 0.035444060250417034, '1351': -0.035444060250417034, '332': 0.035444060250417034, '1762': -0.035444060250417034, '1883': -0.035444060250417034, '1711': 0.0, '1091': -0.035444060250417034, '797': 0.035444060250417034, '1801': 0.035444060250417034, '927': 0.035444060250417034, '1963': 0.035444060250417034, '1747': 0.035444060250417034, '745': 0.035444060250417034, '567': -0.035444060250417034, '858': 0.035444060250417034, '1270': -0.035444060250417034, '1065': 0.035444060250417034, '1389': 0.035444060250417034, '107': -0.035444060250417034, '403': 0.035444060250417034, '1368': -0.035444060250417034, '1809': 0.035444060250417034, '1764': -0.035444060250417034, '192': -0.035444060250417034, '1077': -0.07088812050083407, '384': -0.035444060250417034, '1060': 0.035444060250417034, '1189': -0.035444060250417034, '1949': -0.035444060250417034, '1565': 0.035444060250417034, '93': 0.035444060250417034, '512': 0.035444060250417034, '1295': 0.035444060250417034, '940': 0.035444060250417034, '362': 0.035444060250417034, '612': -0.035444060250417034, '200': -0.035444060250417034, '521': -0.035444060250417034, '405': -0.035444060250417034, '592': 0.035444060250417034, '1519': 0.035444060250417034, '413': 0.035444060250417034, '1862': -0.035444060250417034, '1062': 0.035444060250417034, '1318': -0.035444060250417034, '1704': -0.035444060250417034, '1669': -0.035444060250417034, '1837': -0.035444060250417034, '1282': -0.035444060250417034, '1885': 0.035444060250417034, '173': -0.035444060250417034, '1590': -0.035444060250417034, '178': 0.035444060250417034, '952': 0.035444060250417034, '1587': 0.035444060250417034, '857': 0.035444060250417034, '530': -0.035444060250417034, '915': 0.035444060250417034, '888': -0.035444060250417034, '509': 0.035444060250417034, '1841': -0.035444060250417034, '730': -0.035444060250417034, '1774': -0.035444060250417034, '391': 0.035444060250417034, '1034': -0.035444060250417034, '346': 0.0, '77': 0.035444060250417034, '1215': 0.035444060250417034, '1857': -0.035444060250417034, '37': -0.035444060250417034, '811': 0.035444060250417034, '504': -0.035444060250417034, '1610': -0.035444060250417034, '936': -0.035444060250417034, '1357': 0.035444060250417034, '340': -0.035444060250417034, '1716': 0.0, '1964': 0.035444060250417034, '533': 0.035444060250417034, '786': -0.035444060250417034, '1227': -0.035444060250417034, '986': 0.035444060250417034, '1817': 0.035444060250417034, '1971': -0.035444060250417034, '1721': 0.035444060250417034, '261': 0.035444060250417034, '773': 0.035444060250417034, '1068': 0.035444060250417034, '1233': -0.035444060250417034, '1425': 0.035444060250417034, '886': -0.035444060250417034, '879': -0.035444060250417034, '1612': -0.035444060250417034, '815': 0.035444060250417034, '1081': -0.035444060250417034, '1493': 0.035444060250417034, '1660': 0.035444060250417034, '977': -0.035444060250417034, '275': -0.035444060250417034, '194': -0.035444060250417034, '1543': 0.0, '1122': -0.035444060250417034, '1012': -0.035444060250417034, '235': -0.035444060250417034, '520': -0.035444060250417034, '783': 0.035444060250417034, '1381': -0.035444060250417034, '822': 0.035444060250417034, '337': 0.035444060250417034, '1216': -0.035444060250417034, '762': -0.035444060250417034, '497': 0.035444060250417034, '1969': -0.035444060250417034, '1285': -0.07088812050083407, '287': -0.035444060250417034, '269': 0.035444060250417034, '1864': 0.035444060250417034, '800': 0.035444060250417034, '1298': 0.035444060250417034, '66': -0.07088812050083407, '1461': -0.035444060250417034, '1718': -0.035444060250417034, '1086': -0.07088812050083407, '216': -0.035444060250417034, '1761': -0.035444060250417034, '1087': 0.035444060250417034, '1179': -0.035444060250417034, '185': -0.035444060250417034, '560': -0.035444060250417034, '252': -0.035444060250417034, '1192': 0.035444060250417034, '508': -0.035444060250417034, '975': -0.035444060250417034, '1739': 0.035444060250417034, '540': 0.035444060250417034, '1350': 0.035444060250417034, '871': 0.035444060250417034, '283': 0.035444060250417034, '1986': 0.035444060250417034, '1740': 0.0, '1230': -0.035444060250417034, '720': -0.035444060250417034, '1064': 0.035444060250417034, '1794': 0.035444060250417034, '1144': -0.035444060250417034, '707': -0.035444060250417034, '1078': 0.0, '619': 0.035444060250417034, '157': -0.035444060250417034, '1092': -0.035444060250417034, '1802': -0.035444060250417034, '854': -0.035444060250417034, '386': 0.035444060250417034, '1701': -0.035444060250417034, '478': 0.035444060250417034, '1977': -0.035444060250417034, '1997': 0.035444060250417034, '1308': -0.035444060250417034, '851': -0.035444060250417034, '1191': 0.035444060250417034, '1724': -0.035444060250417034, '1743': -0.035444060250417034, '1283': 0.035444060250417034, '1900': -0.035444060250417034, '798': 0.035444060250417034, '184': 0.035444060250417034, '496': 0.035444060250417034, '1659': -0.035444060250417034, '1890': 0.035444060250417034, '1009': -0.035444060250417034, '624': -0.035444060250417034, '1299': -0.035444060250417034, '182': 0.035444060250417034, '1274': 0.035444060250417034, '1620': 0.035444060250417034, '197': 0.035444060250417034, '446': -0.035444060250417034, '741': -0.035444060250417034, '1035': 0.035444060250417034, '1973': -0.035444060250417034, '314': 0.035444060250417034, '492': 0.035444060250417034, '1114': -0.035444060250417034, '1803': 0.035444060250417034, '1584': 0.0, '776': 0.035444060250417034, '1985': 0.0, '1456': -0.035444060250417034, '1810': 0.035444060250417034, '718': 0.035444060250417034, '1793': -0.035444060250417034, '1218': -0.035444060250417034, '437': 0.035444060250417034, '56': 0.0, '1844': -0.035444060250417034, '1148': 0.035444060250417034, '458': -0.035444060250417034, '679': 0.035444060250417034, '1877': 0.035444060250417034, '414': 0.035444060250417034, '919': 0.035444060250417034, '14': 0.035444060250417034, '460': 0.0, '433': 0.035444060250417034, '1838': 0.035444060250417034, '518': 0.035444060250417034, '580': 0.035444060250417034, '1098': -0.035444060250417034, '4': 0.035444060250417034, '643': 0.035444060250417034, '1458': -0.035444060250417034, '372': -0.035444060250417034, '2': 0.035444060250417034, '869': 0.035444060250417034, '1249': -0.035444060250417034, '1048': -0.035444060250417034, '222': 0.035444060250417034, '1161': 0.035444060250417034, '772': -0.035444060250417034, '242': -0.035444060250417034, '1118': 0.035444060250417034, '1893': 0.035444060250417034, '1805': -0.035444060250417034, '501': -0.035444060250417034, '321': -0.035444060250417034, '819': 0.035444060250417034, '489': 0.035444060250417034, '1004': 0.035444060250417034, '1024': -0.035444060250417034, '1324': 0.035444060250417034, '586': -0.035444060250417034, '1385': -0.035444060250417034, '1939': 0.035444060250417034, '760': -0.035444060250417034, '589': -0.035444060250417034, '1786': -0.035444060250417034, '660': 0.035444060250417034, '844': -0.035444060250417034, '1536': 0.0, '675': -0.035444060250417034, '459': -0.035444060250417034, '1934': 0.035444060250417034, '1343': 0.035444060250417034, '1938': -0.035444060250417034, '1223': -0.035444060250417034, '126': 0.035444060250417034, '490': -0.07088812050083407, '1029': 0.0, '1051': 0.0, '738': 0.035444060250417034, '119': 0.035444060250417034, '240': 0.035444060250417034, '1039': 0.035444060250417034, '75': -0.035444060250417034, '1154': 0.035444060250417034, '958': -0.07088812050083407, '1353': -0.035444060250417034, '605': -0.035444060250417034, '1209': -0.035444060250417034, '1102': 0.035444060250417034, '1516': -0.035444060250417034, '1707': 0.035444060250417034, '1658': 0.035444060250417034, '1585': -0.035444060250417034, '374': -0.035444060250417034, '1947': 0.035444060250417034, '377': -0.035444060250417034, '1673': 0.035444060250417034, '131': 0.0, '315': -0.035444060250417034, '1221': 0.035444060250417034, '1873': -0.035444060250417034, '450': 0.035444060250417034, '669': 0.035444060250417034, '16': -0.07088812050083407, '988': 0.035444060250417034, '1467': 0.0, '999': -0.035444060250417034, '63': -0.035444060250417034, '1258': 0.035444060250417034, '1990': 0.035444060250417034, '389': 0.035444060250417034, '1382': 0.035444060250417034, '270': 0.035444060250417034, '894': 0.035444060250417034, '1776': -0.035444060250417034, '992': -0.035444060250417034, '973': 0.035444060250417034, '1088': -0.035444060250417034, '30': 0.035444060250417034, '1160': 0.07088812050083407, '1617': 0.035444060250417034, '524': -0.035444060250417034, '1575': 0.035444060250417034, '70': -0.035444060250417034, '1689': -0.035444060250417034, '606': 0.035444060250417034, '465': -0.035444060250417034, '65': 0.035444060250417034, '1100': -0.035444060250417034, '834': 0.07088812050083407, '735': -0.035444060250417034, '1698': -0.035444060250417034, '1130': -0.07088812050083407, '1151': 0.0, '467': 0.0, '1583': -0.07088812050083407, '1440': 0.0, '666': -0.035444060250417034, '966': -0.035444060250417034, '301': 0.035444060250417034, '1206': 0.035444060250417034, '590': 0.035444060250417034, '864': 0.035444060250417034, '1560': 0.035444060250417034, '929': -0.035444060250417034, '1474': 0.035444060250417034, '623': 0.035444060250417034, '1670': -0.035444060250417034, '266': -0.035444060250417034, '866': 0.035444060250417034, '628': -0.035444060250417034, '1641': -0.035444060250417034, '1297': 0.035444060250417034, '1824': -0.035444060250417034, '476': 0.035444060250417034, '1799': 0.035444060250417034, '1008': -0.07088812050083407, '1337': 0.035444060250417034, '1074': -0.035444060250417034, '1915': -0.035444060250417034, '277': -0.035444060250417034, '1818': 0.035444060250417034, '603': 0.035444060250417034, '1366': -0.035444060250417034, '734': -0.035444060250417034, '176': 0.035444060250417034, '923': 0.035444060250417034, '285': -0.035444060250417034, '1991': 0.07088812050083407, '705': 0.035444060250417034, '1907': 0.035444060250417034, '1105': 0.035444060250417034, '1319': 0.035444060250417034, '254': -0.035444060250417034, '1488': -0.035444060250417034, '765': -0.035444060250417034, '1763': -0.035444060250417034, '1545': -0.035444060250417034, '654': -0.035444060250417034, '979': 0.035444060250417034, '1152': -0.035444060250417034, '668': 0.035444060250417034, '578': 0.035444060250417034, '335': 0.035444060250417034, '640': -0.035444060250417034, '564': -0.035444060250417034, '1544': 0.035444060250417034, '112': 0.035444060250417034, '659': 0.035444060250417034, '515': -0.035444060250417034, '55': -0.035444060250417034, '431': 0.035444060250417034, '930': 0.035444060250417034, '543': -0.035444060250417034}\n"
     ]
    }
   ],
   "source": [
    "qupattern = re.compile(r'(\\w+):(\\w+)\\s+(.*)$')\n",
    "with codecs.open(questionfile,\"r\", encoding='utf-8') as infile:        \n",
    "    quline = infile.readline().rstrip()\n",
    "    m = qupattern.match(quline)\n",
    "    if m:\n",
    "        print(m.groups()[0])\n",
    "        print(m.groups()[1])\n",
    "        print(m.groups()[2])\n",
    "        text = m.groups()[2]\n",
    "        tag = m.groups()[0]\n",
    "        spec = m.groups()[1]\n",
    "        sents = sent_tokenize(text)\n",
    "##        print(permute(indexspace[tag],tagpermutation))\n",
    "##        print(permute(indexspace[spec],tagpermutation))\n",
    "        for sentence in sents:\n",
    "            analyses = qudepparse(sentence)\n",
    "            for analysis in analyses:\n",
    "                uvector = getvector(analysis,sentence,semroles,selective)\n",
    "                print(uvector)\n",
    "                uvector = sparseadd(uvector,\n",
    "                                    sparseadd(permute(indexspace[tag],tagpermutation),\n",
    "                                              permute(indexspace[spec],tagpermutation),1,True),1,True)\n",
    "        print(uvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testquestions(debug=False,moredebug=False,semroles=False,selective=False):\n",
    "    semroles = False\n",
    "    selective = False\n",
    "    global ctx\n",
    "    qupattern = re.compile(r'(\\w+):(\\w+)\\s+(.*)$')\n",
    "    ticker = 0\n",
    "    batch = 100\n",
    "    with codecs.open(questionfile,\"r\", encoding='utf-8') as infile:        \n",
    "        quline = infile.readline().rstrip()\n",
    "        while quline:\n",
    "            ticker += 1\n",
    "            if ticker >= batch:\n",
    "                print(\".\",end=\"\")\n",
    "                ticker = 0\n",
    "            idx = 0\n",
    "            m = qupattern.match(quline)\n",
    "            if m:\n",
    "                logger(m.groups()[0] + \" \" + m.groups()[1] + \" \" + m.groups()[2],moredebug)\n",
    "                text = m.groups()[2]\n",
    "                tag = m.groups()[0]\n",
    "                spec = m.groups()[1]\n",
    "                if tag not in taglist:\n",
    "                    taglist.append(tag)\n",
    "                    spectag[tag] = []\n",
    "                    ctx[tag] = {}\n",
    "                    indexspace[tag] = newrandomvector(dimensionality,denseness)\n",
    "                    globalfrequency[tag] = 1\n",
    "                if spec not in speclist:\n",
    "                    speclist.append(spec)\n",
    "                    tagspec[spec] = tag\n",
    "                    spectag[tag].append(spec)\n",
    "                    ctx[spec] = {}\n",
    "                    indexspace[spec] = newrandomvector(dimensionality,denseness)\n",
    "                    globalfrequency[spec] = 1\n",
    "                txts[idx] = text\n",
    "                tags[idx] = tag\n",
    "                spex[idx] = spec\n",
    "                sents = sent_tokenize(text)\n",
    "                i = 0\n",
    "                for sentence in sents:\n",
    "                    if debug: \n",
    "                        logger(sentence,debug)\n",
    "                    analyses = qudepparse(sentence)\n",
    "                    kk = 0\n",
    "                    for analysis in analyses:\n",
    "                        uvector = getvector(analysis,sentence,semroles,selective)\n",
    "                        ctx[tag] = sparseadd(ctx[tag],normalise(uvector))\n",
    "                        ctx[spec] = sparseadd(ctx[spec],normalise(uvector))\n",
    "                        logger(str(idx)+\" ====================\",debug)\n",
    "                        uvector = sparseadd(uvector,\n",
    "                                            sparseadd(permute(indexspace[tag],tagpermutation),\n",
    "                                                      permute(indexspace[spec],tagpermutation),1,True),1,True)\n",
    "                        logger(str(sparsecosine(uvector,permute(sparseadd(indexspace[tag],indexspace[spec]),tagpermutation)))+\" \"+str(sparsecosine(uvector,indexspace[tag]))+\" \"+str(sparsecosine(uvector,indexspace[spec])),debug)\n",
    "                        prev = uvector\n",
    "                        if (kk > 0): # there was more than one analysis\n",
    "                            idx += 1\n",
    "                            txts[idx] = text\n",
    "                            tags[idx] = tag\n",
    "                            spex[idx] = spec\n",
    "                        utterancespace[idx] = uvector\n",
    "                        kk += 1\n",
    "                    i += 1\n",
    "                    idx += 1\n",
    "            try:\n",
    "                quline = infile.readline() \n",
    "            except UnicodeDecodeError:\n",
    "                logger(\"read error: \"+quline,True)\n",
    "                quline = infile.readline() \n",
    "def testem():\n",
    "    for uu in utterancespace:\n",
    "        tt = txts[uu]\n",
    "        aa = dependencyanalysisstore[uu]\n",
    "        nn = utterancevector(aa)\n",
    "        ss = utterancespace[uu]\n",
    "        print(uu,tt)\n",
    "        print(uu,sparsecosine(nn,ss),tags[uu],spex[uu],sep=\"\\t\")\n",
    "        for vv in utterancespace:\n",
    "            mm = utterancespace[vv]\n",
    "            print(\"\\t\",vv,sparsecosine(nn,mm),sparsecosine(ss,mm),sep=\"\\t\",end=\"\\t\")\n",
    "            try:\n",
    "                print(tags[vv],spex[vv],sep=\"\\t\")\n",
    "            except KeyError:\n",
    "                print(\"NONE\",\"none\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvector(analysis,sentence,semroles=False,selective=False,train=True):\n",
    "    uvector = {} # vector for test item\n",
    "    if semroles: \n",
    "        wds = list(analysis.values())\n",
    "        chkwordspace(wds) # make sure no KeyErrors will occur, add all words to lexicon\n",
    "        for role in analysis:\n",
    "            item = analysis[role]\n",
    "            if role not in permutationcollection:\n",
    "                permutationcollection[role] = createpermutation(dimensionality)\n",
    "            vector = sparseadd(uvector,\n",
    "                               permute(normalise(indexspace[item]),\n",
    "                                       permutationcollection[role]))\n",
    "    elif selective: # only lexical items with roles\n",
    "        wds = list(analysis.values())\n",
    "        chkwordspace(wds) # make sure no KeyErrors will occur, add all words to lexicon\n",
    "        for item in wds:\n",
    "            uvector = sparseadd(uvector,normalise(indexspace[item]))\n",
    "    else: # straight lexical \n",
    "        wds = word_tokenize(sentence)\n",
    "        chkwordspace(wds) # make sure no KeyErrors will occur, add all words to lexicon\n",
    "        for item in wds:\n",
    "            uvector = sparseadd(uvector,normalise(indexspace[item]))\n",
    "    return uvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateTREC(debug=False,moredebug=False,semroles=False,selective=False):\n",
    "    global permutationcollection\n",
    "    confusionmatrix = {}\n",
    "    antal = 0\n",
    "    treffar = 0\n",
    "    treff01 = 0\n",
    "    treff05 = 0\n",
    "    treff10 = 0\n",
    "    ticker = 0\n",
    "    streffar = 0\n",
    "    streff01 = 0\n",
    "    streff05 = 0\n",
    "    streff10 = 0\n",
    "    sticker = 0\n",
    "    batch = 100\n",
    "    debugprintthreshold = 10\n",
    "    qupattern = re.compile(r'(\\w+):(\\w+)\\s+(.*)$')\n",
    "    with codecs.open(testfile,\"r\", encoding='utf-8') as infile:        \n",
    "        quline = infile.readline().rstrip()\n",
    "        idx = 0\n",
    "        while quline:\n",
    "            m = qupattern.match(quline)\n",
    "            if m:\n",
    "                text = m.groups()[2]\n",
    "                tag = m.groups()[0]\n",
    "                spec = m.groups()[1]\n",
    "                sents = sent_tokenize(text)\n",
    "                for sentence in sents:\n",
    "                    key = tag+\":\"+spec\n",
    "                    logger(key+\"\\t\"+sentence,debug)\n",
    "                    analyses = qudepparse(sentence)\n",
    "                    for analysis in analyses:  # almost certainly only one analysis\n",
    "                        antal += 1\n",
    "                        ticker += 1\n",
    "                        uvector = getvector(analysis,sentence,semroles,selective)\n",
    "                        # =========================================================================\n",
    "                        # compare with tag context vectors\n",
    "                        tagneighbours = {}\n",
    "                        for kk in taglist:\n",
    "                            for ll in spectag[kk]:\n",
    "                                cosinedistance = sparsecosine(sparseadd(normalise(ctx[kk]),normalise(ctx[ll])),uvector)\n",
    "                                tagneighbours[kk+\":\"+ll] = cosinedistance\n",
    "                                sortedtagneighbours = sorted(tagneighbours.items(), key=lambda neighbour:neighbour[1], reverse=True) #[0:10]\n",
    "                        addconfusion(key,sortedtagneighbours[0][0])  # compare top prediction with gold standard\n",
    "                        rankofsortedtagneighbour = 0\n",
    "                        for onesortedtagneighbour in sortedtagneighbours:\n",
    "                            rankofsortedtagneighbour += 1\n",
    "                            result = \"\"\n",
    "                            if onesortedtagneighbour[0] == key:  # it's a hit!\n",
    "                                if debug:\n",
    "                                    result = \"***\"\n",
    "                                treffar += rankofsortedtagneighbour\n",
    "                                if rankofsortedtagneighbour == 1:\n",
    "                                    treff01 += 1\n",
    "                                if rankofsortedtagneighbour <= 5:\n",
    "                                    treff05 += 1\n",
    "                                if rankofsortedtagneighbour <= 10:\n",
    "                                    treff10 += 1\n",
    "                            if rankofsortedtagneighbour <= debugprintthreshold:\n",
    "                                logger(onesortedtagneighbour[0]+\"\\t\"+str(onesortedtagneighbour[1])+\"\\t\"+str(result),debug)\n",
    "                            if batch > 0 and ticker >= batch:\n",
    "                                average = 0 \n",
    "                                if antal > 0:\n",
    "                                    average = treffar / antal\n",
    "                                print(treff01,treff05,treff10,antal,average,sep=\"\\t\")\n",
    "                                ticker = 0\n",
    "                        # =========================================================================\n",
    "                        # compare with train sentence vectors\n",
    "                        sentenceneighbours = {}\n",
    "                        for uu in utterancespace:\n",
    "                            cosinedistance = sparsecosine(utterancespace[uu],uvector)\n",
    "                            mt = 0\n",
    "                            kk = \"TBD\"\n",
    "                            for kt in taglist:\n",
    "                                if sparsecosine(indexspace[kt],utterancespace[uu]) > mt:\n",
    "                                    kk = kt\n",
    "                            ms = 0\n",
    "                            ll = \"tbd\"\n",
    "                            try:\n",
    "                                for st in spectag[kt]:\n",
    "                                    if sparsecosine(indexspace[st],utterancespace[uu]) > ms:\n",
    "                                        ll = st\n",
    "                            except:\n",
    "                                ll = \"error\"\n",
    "                            sentenceneighbours[kk+\":\"+ll] = cosinedistance\n",
    "                            sortedsentenceneighbours = sorted(sentenceneighbours.items(), key=lambda neighbour:neighbour[1], reverse=True) #[0:10]\n",
    "                        addconfusion(key,sortedsentenceneighbours[0][0])  # compare top prediction with gold standard\n",
    "                        rankofsortedsentenceneighbour = 0\n",
    "                        for onesortedneighbour in sortedsentenceneighbours:\n",
    "                            rankofsortedsentenceneighbour += 1\n",
    "                            result = \"\"\n",
    "                            if onesortedneighbour[0] == key:  # it's a hit!\n",
    "                                if debug:\n",
    "                                    result = \"***\"\n",
    "                                streffar += rankofsortedsentenceneighbour\n",
    "                                if rankofsortedsentenceneighbour == 1:\n",
    "                                    streff01 += 1\n",
    "                                if rankofsortedsentenceneighbour <= 5:\n",
    "                                    streff05 += 1\n",
    "                                if rankofsortedsentenceneighbour <= 10:\n",
    "                                    streff10 += 1\n",
    "                            if rankofsortedsentenceneighbour <= debugprintthreshold:\n",
    "                                logger(onesortedneighbour[0]+\"\\t\"+str(onesortedneighbour[1])+\"\\t\"+str(result),debug)\n",
    "                            if batch > 0 and sticker >= batch:\n",
    "                                average = 0 \n",
    "                                if antal > 0:\n",
    "                                    average = streffar / antal\n",
    "                                print(streff01,streff05,streff10,antal,average,sep=\"\\t\")\n",
    "                                sticker = 0\n",
    "            try:\n",
    "                quline = infile.readline() \n",
    "            except UnicodeDecodeError:\n",
    "                logger(\"read error: \"+quline,True)\n",
    "                quline = infile.readline() \n",
    "    average = 0\n",
    "    if antal > 0:\n",
    "        average = treffar / antal\n",
    "    print(treff01,treff05,treff10,antal,average,sep=\"\\t\")\n",
    "    if antal > 0:\n",
    "        average = streffar / antal\n",
    "    print(streff01,streff05,streff10,antal,average,sep=\"\\t\")\n",
    "    return confusionmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training False False Wed Feb  7 13:39:22 2018\n",
      "testing False False Wed Feb  7 13:40:46 2018\n",
      "52\t68\t72\t100\t4.17\n",
      "97\t131\t143\t200\t4.96\n",
      "145\t201\t219\t300\t4.703333333333333\n",
      "195\t265\t289\t400\t4.9025\n",
      "249\t333\t364\t500\t4.882\n",
      "250\t335\t366\t502\t4.870517928286852\n",
      "0\t2\t5\t502\t1.051792828685259\n",
      "training False True Wed Feb  7 16:57:49 2018\n",
      "testing False True Wed Feb  7 16:59:26 2018\n",
      "sparsecosine(): errors at position  22\n",
      "sparsecosine(): errors at position  890\n",
      "sparsecosine(): errors at position  1389\n",
      "sparsecosine(): errors at position  1327\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-761-8742ff88c704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtestquestions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoredebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msemantics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mcm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluateTREC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoredebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselective\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mselective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0msemantics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-760-644bebd5d716>\u001b[0m in \u001b[0;36mevaluateTREC\u001b[0;34m(debug, moredebug, semroles, selective)\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0msentenceneighbours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0muu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mutterancespace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                             \u001b[0mcosinedistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparsecosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutterancespace\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                             \u001b[0mmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                             \u001b[0mkk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TBD\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-719-56208048cc52>\u001b[0m in \u001b[0;36msparsecosine\u001b[0;34m(xvec, yvec, rounding, decimals)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxvec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mx2\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mxvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxvec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparsecosine(): error at position \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "questionfile=\"/home/jussi/data/questions/train_3000.label\"\n",
    "testfile=\"/home/jussi/data/questions/TREC_10.label\"\n",
    "initconstantsandpermutations()\n",
    "tags = {}\n",
    "spex = {}\n",
    "txts = {}\n",
    "ctx = {}\n",
    "indexspace = {}\n",
    "chkwordspace([\"epsilon\"])\n",
    "denseness = 0.2\n",
    "permutationcollection = {}\n",
    "debug = False\n",
    "moredebug = False\n",
    "semantics = False\n",
    "selective = False\n",
    "utterancespace = {}\n",
    "ctx = {}\n",
    "print(\"training\",semantics,selective,time.ctime())\n",
    "testquestions(debug, moredebug, semantics, selective)\n",
    "print(\"testing\",semantics,selective,time.ctime())\n",
    "cm1 = evaluateTREC(debug, moredebug, semantics, selective)\n",
    "selective = True\n",
    "utterancespace = {}\n",
    "ctx = {}\n",
    "print(\"training\",semantics,selective,time.ctime())\n",
    "testquestions(debug, moredebug, semantics, selective)\n",
    "print(\"testing\",semantics,selective,time.ctime())\n",
    "cm2 = evaluateTREC(debug, moredebug, semantics, selective)\n",
    "selective = False\n",
    "semantics = True\n",
    "utterancespace = {}\n",
    "ctx = {}\n",
    "print(\"training\",semantics,selective,time.ctime())\n",
    "testquestions(debug, moredebug, semantics, selective)\n",
    "print(\"testing\",semantics,selective,time.ctime())\n",
    "cm3 = evaluateTREC(debug, moredebug, semantics, selective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ABBR': ['exp', 'abb'],\n",
       " 'DESC': ['manner', 'def', 'reason', 'desc'],\n",
       " 'ENTY': ['cremat',\n",
       "  'animal',\n",
       "  'event',\n",
       "  'other',\n",
       "  'letter',\n",
       "  'religion',\n",
       "  'food',\n",
       "  'color',\n",
       "  'termeq',\n",
       "  'body',\n",
       "  'dismed',\n",
       "  'product',\n",
       "  'substance',\n",
       "  'sport',\n",
       "  'plant',\n",
       "  'techmeth',\n",
       "  'instru',\n",
       "  'word',\n",
       "  'lang',\n",
       "  'symbol',\n",
       "  'veh',\n",
       "  'currency'],\n",
       " 'HUM': ['ind', 'gr', 'title'],\n",
       " 'LOC': ['state', 'country', 'city', 'mount'],\n",
       " 'NUM': ['date',\n",
       "  'count',\n",
       "  'money',\n",
       "  'period',\n",
       "  'volsize',\n",
       "  'speed',\n",
       "  'perc',\n",
       "  'code',\n",
       "  'dist',\n",
       "  'temp',\n",
       "  'ord',\n",
       "  'weight']}"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\t65\t67\t100\t7.11\n",
      "94\t118\t127\t200\t7.875\n",
      "141\t182\t196\t300\t7.266666666666667\n",
      "192\t243\t260\t400\t7.2025\n",
      "239\t303\t325\t500\t7.374\n",
      "240\t304\t327\t502\t7.3585657370517925\n"
     ]
    }
   ],
   "source": [
    "evaluateTREC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': -1,\n",
       " '10': -1,\n",
       " '1022': -1,\n",
       " '1025': -1,\n",
       " '1029': -1,\n",
       " '1038': 1,\n",
       " '1039': 1,\n",
       " '1045': -1,\n",
       " '1050': -1,\n",
       " '1052': 1,\n",
       " '1053': 1,\n",
       " '1058': -1,\n",
       " '1067': 1,\n",
       " '1069': 1,\n",
       " '1075': 1,\n",
       " '1076': 1,\n",
       " '1080': -1,\n",
       " '1084': -1,\n",
       " '1087': 1,\n",
       " '1089': 1,\n",
       " '1092': 1,\n",
       " '1094': 1,\n",
       " '1095': -1,\n",
       " '1097': -1,\n",
       " '1100': 1,\n",
       " '1101': -1,\n",
       " '1102': -1,\n",
       " '1103': -1,\n",
       " '1112': -1,\n",
       " '1128': 1,\n",
       " '1129': -1,\n",
       " '1130': -1,\n",
       " '1132': 1,\n",
       " '1138': -1,\n",
       " '1143': 1,\n",
       " '1152': 1,\n",
       " '1153': -1,\n",
       " '1158': -1,\n",
       " '1163': 1,\n",
       " '1182': 1,\n",
       " '1186': 1,\n",
       " '1187': -1,\n",
       " '1191': -1,\n",
       " '1193': 1,\n",
       " '12': -1,\n",
       " '120': 1,\n",
       " '1204': -1,\n",
       " '1221': 1,\n",
       " '1243': -1,\n",
       " '1245': 1,\n",
       " '1246': 1,\n",
       " '1254': -1,\n",
       " '1257': 1,\n",
       " '1259': -1,\n",
       " '1263': -1,\n",
       " '1275': 1,\n",
       " '1276': -1,\n",
       " '1278': -1,\n",
       " '1279': -1,\n",
       " '128': 1,\n",
       " '1282': -1,\n",
       " '1285': 1,\n",
       " '1286': 1,\n",
       " '1297': 1,\n",
       " '13': -1,\n",
       " '1301': 1,\n",
       " '1302': 1,\n",
       " '1303': -1,\n",
       " '1307': 1,\n",
       " '131': -1,\n",
       " '1323': -1,\n",
       " '1324': 1,\n",
       " '133': 1,\n",
       " '1330': 1,\n",
       " '1335': 1,\n",
       " '1336': 1,\n",
       " '1337': -1,\n",
       " '1340': 1,\n",
       " '1367': -1,\n",
       " '1369': -1,\n",
       " '1372': -1,\n",
       " '1374': -1,\n",
       " '1375': -1,\n",
       " '1376': 1,\n",
       " '1377': -1,\n",
       " '1380': 1,\n",
       " '1381': -1,\n",
       " '1382': -1,\n",
       " '1383': -1,\n",
       " '1388': -1,\n",
       " '1395': 1,\n",
       " '1397': -1,\n",
       " '1400': -1,\n",
       " '1413': 1,\n",
       " '1424': 1,\n",
       " '1426': 1,\n",
       " '1431': -1,\n",
       " '1439': 1,\n",
       " '1445': -1,\n",
       " '1447': -1,\n",
       " '1453': -1,\n",
       " '1468': 1,\n",
       " '1469': -1,\n",
       " '147': 1,\n",
       " '1471': -1,\n",
       " '1480': 1,\n",
       " '1482': -1,\n",
       " '1495': -1,\n",
       " '1497': 1,\n",
       " '1519': -1,\n",
       " '1523': 1,\n",
       " '1525': -1,\n",
       " '1527': 1,\n",
       " '153': 1,\n",
       " '1536': 1,\n",
       " '1539': 1,\n",
       " '1549': 1,\n",
       " '1552': 1,\n",
       " '1555': 1,\n",
       " '1558': 1,\n",
       " '1567': -1,\n",
       " '157': 1,\n",
       " '1578': 1,\n",
       " '158': -1,\n",
       " '1584': 1,\n",
       " '1590': 1,\n",
       " '1591': -1,\n",
       " '1593': -1,\n",
       " '1595': 1,\n",
       " '1600': -1,\n",
       " '1602': -1,\n",
       " '1605': -1,\n",
       " '1615': 1,\n",
       " '1616': -1,\n",
       " '1619': -1,\n",
       " '1622': 1,\n",
       " '1625': 1,\n",
       " '1628': 1,\n",
       " '1630': 1,\n",
       " '1632': 1,\n",
       " '1637': 1,\n",
       " '1638': -1,\n",
       " '1649': 1,\n",
       " '1651': -1,\n",
       " '1652': -1,\n",
       " '1655': 1,\n",
       " '1658': -1,\n",
       " '1659': -1,\n",
       " '166': 1,\n",
       " '1661': -1,\n",
       " '1662': 1,\n",
       " '1663': 1,\n",
       " '167': 1,\n",
       " '1683': 1,\n",
       " '1688': 1,\n",
       " '1692': 1,\n",
       " '1698': 1,\n",
       " '1705': 1,\n",
       " '1706': -1,\n",
       " '1707': -1,\n",
       " '171': -1,\n",
       " '1715': -1,\n",
       " '1716': -1,\n",
       " '1734': -1,\n",
       " '1736': 1,\n",
       " '1748': -1,\n",
       " '1757': -1,\n",
       " '1762': 1,\n",
       " '1768': 1,\n",
       " '1769': -1,\n",
       " '1774': -1,\n",
       " '1778': -1,\n",
       " '1779': 1,\n",
       " '1786': 1,\n",
       " '179': 1,\n",
       " '1791': -1,\n",
       " '180': -1,\n",
       " '1800': 1,\n",
       " '1805': -1,\n",
       " '1807': 1,\n",
       " '181': -1,\n",
       " '1821': 1,\n",
       " '1828': -1,\n",
       " '1829': -1,\n",
       " '1833': -1,\n",
       " '1839': 1,\n",
       " '1858': 1,\n",
       " '1870': -1,\n",
       " '1874': -1,\n",
       " '1877': -1,\n",
       " '1878': 1,\n",
       " '1879': 1,\n",
       " '1882': -1,\n",
       " '1884': -1,\n",
       " '1888': -1,\n",
       " '1890': 1,\n",
       " '1897': 1,\n",
       " '19': 1,\n",
       " '1900': 1,\n",
       " '1902': 1,\n",
       " '1903': 1,\n",
       " '1907': -1,\n",
       " '1910': 1,\n",
       " '1911': 1,\n",
       " '1913': -1,\n",
       " '1914': -1,\n",
       " '1916': 1,\n",
       " '192': 1,\n",
       " '1935': -1,\n",
       " '1938': 1,\n",
       " '1945': -1,\n",
       " '195': 1,\n",
       " '1950': 1,\n",
       " '1958': -1,\n",
       " '1961': 1,\n",
       " '1969': -1,\n",
       " '1971': 1,\n",
       " '1972': -1,\n",
       " '198': 1,\n",
       " '1983': 1,\n",
       " '1984': -1,\n",
       " '1986': -1,\n",
       " '203': -1,\n",
       " '206': 1,\n",
       " '209': -1,\n",
       " '213': -1,\n",
       " '227': 1,\n",
       " '229': 1,\n",
       " '231': 1,\n",
       " '238': -1,\n",
       " '240': -1,\n",
       " '241': 1,\n",
       " '242': 1,\n",
       " '243': -1,\n",
       " '247': -1,\n",
       " '249': -1,\n",
       " '25': 1,\n",
       " '250': -1,\n",
       " '253': -1,\n",
       " '257': -1,\n",
       " '261': 1,\n",
       " '263': -1,\n",
       " '266': 1,\n",
       " '27': 1,\n",
       " '275': -1,\n",
       " '281': 1,\n",
       " '29': 1,\n",
       " '291': 1,\n",
       " '293': 1,\n",
       " '295': -1,\n",
       " '3': 1,\n",
       " '300': 1,\n",
       " '301': -1,\n",
       " '306': 1,\n",
       " '311': 1,\n",
       " '312': 1,\n",
       " '327': 1,\n",
       " '336': 1,\n",
       " '34': 1,\n",
       " '343': -1,\n",
       " '344': -1,\n",
       " '351': -1,\n",
       " '352': 1,\n",
       " '355': -1,\n",
       " '362': 1,\n",
       " '365': -1,\n",
       " '368': 1,\n",
       " '369': -1,\n",
       " '371': -1,\n",
       " '372': 1,\n",
       " '375': 1,\n",
       " '378': 1,\n",
       " '379': -1,\n",
       " '39': 1,\n",
       " '404': -1,\n",
       " '409': 1,\n",
       " '410': -1,\n",
       " '412': -1,\n",
       " '414': -1,\n",
       " '422': 1,\n",
       " '423': 1,\n",
       " '428': 1,\n",
       " '446': 1,\n",
       " '45': 1,\n",
       " '464': 1,\n",
       " '467': -1,\n",
       " '474': -1,\n",
       " '477': -1,\n",
       " '490': 1,\n",
       " '492': 1,\n",
       " '495': 1,\n",
       " '496': -1,\n",
       " '50': 1,\n",
       " '510': -1,\n",
       " '518': -1,\n",
       " '520': 1,\n",
       " '521': -1,\n",
       " '526': -1,\n",
       " '53': 1,\n",
       " '55': 1,\n",
       " '551': 1,\n",
       " '554': -1,\n",
       " '560': 1,\n",
       " '565': 1,\n",
       " '567': -1,\n",
       " '568': -1,\n",
       " '570': -1,\n",
       " '582': -1,\n",
       " '595': -1,\n",
       " '599': -1,\n",
       " '6': -1,\n",
       " '601': -1,\n",
       " '606': 1,\n",
       " '617': 1,\n",
       " '625': -1,\n",
       " '627': -1,\n",
       " '629': -1,\n",
       " '636': 1,\n",
       " '640': -1,\n",
       " '645': -1,\n",
       " '649': 1,\n",
       " '654': 1,\n",
       " '660': 1,\n",
       " '663': -1,\n",
       " '667': 1,\n",
       " '672': -1,\n",
       " '676': 1,\n",
       " '681': -1,\n",
       " '687': 1,\n",
       " '689': -1,\n",
       " '691': -1,\n",
       " '695': -1,\n",
       " '698': -1,\n",
       " '706': -1,\n",
       " '713': -1,\n",
       " '714': 1,\n",
       " '716': -1,\n",
       " '718': 1,\n",
       " '726': 1,\n",
       " '733': -1,\n",
       " '734': -1,\n",
       " '735': -1,\n",
       " '741': 1,\n",
       " '748': -1,\n",
       " '75': 1,\n",
       " '757': 1,\n",
       " '759': -1,\n",
       " '768': -1,\n",
       " '776': -1,\n",
       " '777': 1,\n",
       " '779': -1,\n",
       " '78': -1,\n",
       " '784': -1,\n",
       " '790': -1,\n",
       " '791': -1,\n",
       " '802': 1,\n",
       " '813': -1,\n",
       " '819': -1,\n",
       " '82': 1,\n",
       " '823': 1,\n",
       " '825': 1,\n",
       " '83': -1,\n",
       " '837': -1,\n",
       " '845': -1,\n",
       " '851': 1,\n",
       " '858': -1,\n",
       " '862': -1,\n",
       " '864': 1,\n",
       " '865': 1,\n",
       " '873': -1,\n",
       " '874': 1,\n",
       " '88': 1,\n",
       " '889': 1,\n",
       " '892': 1,\n",
       " '893': 1,\n",
       " '897': -1,\n",
       " '902': -1,\n",
       " '903': 1,\n",
       " '915': -1,\n",
       " '917': 1,\n",
       " '924': 1,\n",
       " '927': 1,\n",
       " '936': 1,\n",
       " '938': 1,\n",
       " '948': 1,\n",
       " '950': 1,\n",
       " '953': -1,\n",
       " '958': -1,\n",
       " '959': -1,\n",
       " '961': 1,\n",
       " '963': -1,\n",
       " '971': 1,\n",
       " '978': -1,\n",
       " '985': -1,\n",
       " '988': -1,\n",
       " '989': -1,\n",
       " '992': 1,\n",
       " '995': -1,\n",
       " '996': 1,\n",
       " '999': 1}"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexspace[\"DESC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in taglist:\n",
    "    print(t,sparselength(ctx[t]))\n",
    "    for w in indexspace:\n",
    "        c = sparsecosine(ctx[t],indexspace[w])\n",
    "        if w and c > 0.1:\n",
    "            print(\"\\t\",w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusionmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESC : manner \t 0.1483\n",
      "DESC : desc \t 0.1013\n",
      "ENTY : animal \t 0.1359\n",
      "ENTY : other \t 0.1163\n",
      "ENTY : letter \t 0.1161\n",
      "ENTY : religion \t 0.1204\n",
      "ENTY : color \t 0.1782\n",
      "ENTY : dismed \t 0.1041\n",
      "ENTY : product \t 0.1091\n",
      "ENTY : sport \t 0.1516\n",
      "ENTY : plant \t 0.1107\n",
      "ENTY : instru \t 0.1029\n",
      "ENTY : symbol \t 0.1535\n",
      "ENTY : currency \t 0.101\n",
      "HUM : gr \t 0.1175\n",
      "HUM : title \t 0.1165\n",
      "NUM : date \t 0.15\n",
      "NUM : count \t 0.202\n",
      "NUM : money \t 0.1764\n",
      "NUM : period \t 0.277\n",
      "NUM : volsize \t 0.1942\n",
      "NUM : speed \t 0.1876\n",
      "NUM : perc \t 0.1725\n",
      "NUM : code \t 0.149\n",
      "NUM : dist \t 0.2467\n",
      "NUM : temp \t 0.1729\n",
      "NUM : ord \t 0.1178\n",
      "NUM : weight \t 0.1548\n",
      "LOC : state \t 0.162\n",
      "LOC : country \t 0.177\n",
      "LOC : city \t 0.1553\n",
      "LOC : mount \t 0.1299\n"
     ]
    }
   ],
   "source": [
    "uu = {}\n",
    "for ws in [\"how\",\"long\",\"be\",\"the\",\"river\"]:\n",
    "    uu = sparseadd(normalise(uu),normalise(indexspace[ws]))\n",
    "for kk in taglist:\n",
    "    for ll in spectag[kk]:\n",
    "        c = sparsecosine(sparseadd(normalise(ctx[kk]),normalise(ctx[ll])),uu)\n",
    "        if c > 0.1:\n",
    "            print(kk,\":\",ll,\"\\t\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: [1]\n",
      "1   who   WP\n",
      "2   be   VBD\n",
      "3   Peugeot   NNP\n",
      "4   ?   .\n",
      "1 who - cop -> 2 be\n",
      "1 who - nsubj -> 3 Peugeot\n",
      "1 who - punct -> 4 ?\n",
      "1 who - cop -> 2 be\n",
      "1 who - nsubj -> 3 Peugeot\n",
      "1 who - punct -> 4 ?\n",
      "who   WP\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<__main__.clause at 0x7f10a1d28ba8>]"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semanticdepparse(\"Who was Peugeot?\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nu 1.0\n",
      "8u -0.0082\n",
      "nu 0.7115\n",
      "8u 0.074\n",
      "nu 0.5774\n",
      "8u 0.0662\n",
      "nu 0.4979\n",
      "8u 0.0675\n",
      "nu 0.4543\n",
      "8u 0.0655\n",
      "n8 -0.0082\n",
      "M\n",
      "nu 0.3106\n",
      "8u 0.063\n",
      "S\n",
      "nu 0.2392\n",
      "8u 0.0578\n",
      "nu2 0.0394\n",
      "8u2 0.0212\n",
      "C\n",
      "nu 0.1535\n",
      "hu 0.1818\n",
      "tu 0.17\n",
      "8u 0.0324\n"
     ]
    }
   ],
   "source": [
    "uvv={}\n",
    "uvv = sparseadd(indexspace['name'],uvv)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "uvv = sparseadd(indexspace['he'],uvv)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "uvv = sparseadd(indexspace['10'],uvv)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "uvv = sparseadd(indexspace['HUM'],uvv)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "uvv = sparseadd(indexspace['ind'],uvv)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "print(\"n8\",sparsecosine(indexspace['name'],utterancespace[8]))\n",
    "utterance=dependencyanalysisstore[8]\n",
    "print(\"M\")\n",
    "morphvec={}\n",
    "morphvec=sparseadd(morphvec,permute(indexspace[utterance.agent.surfacestring],definitepermutation))\n",
    "morphvec=sparseadd(morphvec,permute(indexspace[utterance.patient.surfacestring],definitepermutation))\n",
    "morphvec=sparseadd(morphvec,permute(indexspace[utterance.event.surfacestring],pasttensepermutation))\n",
    "uvv=sparseadd(normalise(uvv),normalise(morphvec))\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "print(\"S\")\n",
    "semroles={}\n",
    "semroles=sparseadd(semroles,permute(indexspace[utterance.patient.surfacestring],patientpermutation))\n",
    "semroles=sparseadd(semroles,permute(indexspace[utterance.event.surfacestring],eventpermutation))\n",
    "semroles=sparseadd(semroles,permute(indexspace[utterance.event.adverbial],adverbialpermutation))\n",
    "semroles=sparseadd(semroles,permute(indexspace[utterance.agent.surfacestring],agentpermutation))\n",
    "uvv=sparseadd(normalise(uvv),normalise(semroles))\n",
    "uvv2=sparseadd(uvv,semroles)\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n",
    "print(\"nu2\",sparsecosine(indexspace['name'],uvv2))\n",
    "print(\"8u2\",sparsecosine(utterancespace[8],uvv2))\n",
    "print(\"C\")\n",
    "constrvec={}\n",
    "constrvec=sparseadd(constrvec,permute(agentabstract,definitepermutation))\n",
    "constrvec=sparseadd(constrvec,permute(patientabstract,definitepermutation))\n",
    "constrvec=sparseadd(constrvec,permute(pastabstract,tensepermutation))\n",
    "uvv=sparseadd(normalise(uvv),normalise(constrvec))\n",
    "print(\"nu\",sparsecosine(indexspace['name'],uvv))\n",
    "print(\"hu\",sparsecosine(indexspace['he'],uvv))\n",
    "print(\"tu\",sparsecosine(indexspace['10'],uvv))\n",
    "print(\"8u\",sparsecosine(utterancespace[8],uvv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rununittest():\n",
    "    for v in utterancespace:\n",
    "        print(\"-------------\")\n",
    "        print(v)\n",
    "        neighbours = {}\n",
    "        for w in utterancespace:\n",
    "            print(w)\n",
    "            if w == v:\n",
    "                continue\n",
    "            neighbours[w] = sparsecosine(utterancespace[v],utterancespace[w])\n",
    "        ns = sorted(neighbours.items(), key=lambda neighbour:neighbour[1], reverse=True)[0:10]\n",
    "        for ww in ns:\n",
    "            print(\"-\",ww,tags[idx],spex[idx],txts[idx],sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "qu0\n",
      "qu0\n"
     ]
    }
   ],
   "source": [
    "rununittest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def command_loop(debug=False,moredebug=False):\n",
    "    instring = ''\n",
    "    prev=newrandomvector(dimensionality,denseness)\n",
    "    prevlex=newrandomvector(dimensionality,denseness)\n",
    "    prevsem=newrandomvector(dimensionality,denseness)\n",
    "    prevmor=newrandomvector(dimensionality,denseness)\n",
    "    prevmgn=newrandomvector(dimensionality,denseness)\n",
    "    instring = input('> ')\n",
    "    while instring != 'quit':\n",
    "        try:\n",
    "            s = instring.rstrip()\n",
    "            wds=nltk.word_tokenize(s.lower())\n",
    "            chkwordspace(wds,debug)\n",
    "            try:\n",
    "                cs = semanticdepparse(s.lower(),debug)\n",
    "                for c in cs:\n",
    "                    u = utterancevector(c,moredebug, True, False, False,False)\n",
    "                    print(\" lexical     \",sparsecosine(prev,u),end=\"\\t(\")\n",
    "                    print(sparsecosine(prevlex,u),\")\")\n",
    "                    prevlex=u\n",
    "                    u = utterancevector(c,moredebug, False, False, True, False)\n",
    "                    print(\" semrole     \",sparsecosine(prev,u),end=\"\\t(\")\n",
    "                    print(sparsecosine(prevsem,u),\")\")\n",
    "                    prevsem=u\n",
    "                    u = utterancevector(c,moredebug,  False, True, False, False)\n",
    "                    print(\" morph     \",sparsecosine(prev,u),end=\"\\t(\")\n",
    "                    print(sparsecosine(prevmor,u),\")\")\n",
    "                    prevmor=u\n",
    "                    u = utterancevector(c,moredebug, False, False, False, True)\n",
    "                    print(\" construction  \",sparsecosine(prev,u),end=\"\\t(\")\n",
    "                    print(sparsecosine(prevmgn,u),\")\")\n",
    "                    prevmgn=u\n",
    "                    u = utterancevector(c,moredebug)\n",
    "                    print(\" in toto     \",sparsecosine(prev,u))\n",
    "                    prev = u\n",
    "            except:\n",
    "                print(\"****\")\n",
    "            instring = input('> ')\n",
    "        except:\n",
    "            instring = 'quit'\n",
    "    print(\"hey!\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "command_loop(False,False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lexicon: what concepts are mentioned (context vectors)\n",
    "f1 = False\n",
    "\n",
    "# morphology: what morphological form is represented (index vectors permuted with morph permutations)\n",
    "f2 = True\n",
    "\n",
    "# semantic roles: e.g. is the agent definite? (index vectors, deep case permutations)\n",
    "f3 = True\n",
    "\n",
    "# constructions: (abstract random feature vectors)\n",
    "f4 = True\n",
    "\n",
    "#debug output (no good at present)\n",
    "d = False\n",
    "a = \"The cat eats the cream.\"\n",
    "b = \"The fish ate the worm.\"\n",
    "c = \"she winked.\"\n",
    "target=permute(pastabstract,tensepermutation)\n",
    "ap = semanticdepparse(a.lower())[0]\n",
    "bp = semanticdepparse(b.lower())[0]\n",
    "cp = semanticdepparse(c.lower())[0]\n",
    "av = utterancevector(ap,d,f1,f2,f3,f4)\n",
    "bv = utterancevector(bp,d,f1,f2,f3,f4)\n",
    "cv = utterancevector(cp,d,f1,f2,f3,f4)\n",
    "\n",
    "print(\"a vs b\\t\",sparsecosine(av,bv))\n",
    "print(\"a vs c\\t\",sparsecosine(av,cv))\n",
    "print(\"b vs c\\t\",sparsecosine(bv,cv))\n",
    "\n",
    "print(\"a vs tgt\\t\",sparsecosine(av,target))\n",
    "print(\"b vs tgt\\t\",sparsecosine(bv,target))\n",
    "print(\"c vs tgt\\t\",sparsecosine(cv,target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initconstantsandpermutations(dimensionality,0.2)\n",
    "indexspace={}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definite:\t 0.4053\n",
      "definite:\t 0.3847\n",
      "negation:\t 0.702\n",
      "non-neg vs negation:\t -0.0143\n",
      "present:\t 0.5998\n",
      "past vs present:\t 0.0143\n",
      "neg vs non-neg:\t 0.4521\n",
      "similar lex:\t 1.0\n",
      "similar lex:\t 0.647\n"
     ]
    }
   ],
   "source": [
    "#for denseness in [0.05]: #[0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.01]:\n",
    "\n",
    "\n",
    "#for k in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]:\n",
    "#    negationabstract=newrandomvector(dimensionality,k)\n",
    "#    presentabstract=newrandomvector(dimensionality,k)\n",
    "\n",
    "def uv(words):\n",
    "    utterance=semanticdepparse(words.lower())[0]\n",
    "    return utterancevector(utterance,False, False, True, True, True)\n",
    "\n",
    "\n",
    "print(\"definite:\\t\",sparsecosine(uv(\"The cat does not eat the fish.\"),permute(agentabstract,definitepermutation)))\n",
    "print(\"definite:\\t\",sparsecosine(uv(\"The cat does not eat the fish.\"),permute(patientabstract,definitepermutation)))\n",
    "print(\"negation:\\t\",sparsecosine(uv(\"The cat does not eat the fish.\"),permute(negationabstract,negationpermutation)))\n",
    "print(\"non-neg vs negation:\\t\",sparsecosine(uv(\"The cat eats the fish.\"),permute(negationabstract,negationpermutation)))\n",
    "print(\"present:\\t\",sparsecosine(uv(\"The cat eats the fish.\"),permute(presentabstract,tensepermutation)))\n",
    "print(\"past vs present:\\t\",sparsecosine(uv(\"The cat has not eaten the fish.\"),permute(presentabstract,tensepermutation)))\n",
    "print(\"neg vs non-neg:\\t\",sparsecosine(uv(\"The cat eats the fish.\"),uv(\"The cat did not eat the fish.\")))\n",
    "print(\"similar lex:\\t\",sparsecosine(uv(\"The cat eats the fish.\"),uv(\"The dog eats the fish.\")))\n",
    "print(\"similar lex:\\t\",sparsecosine(uv(\"The cat eats the fish.\"),uv(\"The fish ate the worm.\")))\n",
    "\n",
    "#initconstantsandpermutations(dimensionality,0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one vector per main verb\n",
    "- identify interesting use case: authorship? gender? \n",
    "- add features cpw wps complexity p1 p2 p3 here and now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can we find indefinite agents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4053\t0.702\t-0.0431\t0.6244\tit wouldn't take long for her to place who he was.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the almost asked but he didn't want her to look at him with that skeletal face and gap-tooth smirk.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tjoel had not pressured her.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tthey had never been camping.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tshe almost told dad that she wasn't feeling well and that she'd rather just go home and curl up in bed with a book.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthere were no cars near his.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\ti never heard of him until that day when arlon, my boss, called and said some wackjob shot up the place.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthey probably didn't have a car.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\teven after a month of similar exchanges, he denied her the full pleasure of his sex because he had found someone new and he didn't want to take advantage of her.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the had pressed his luck, no doubt about that, and if he pressed it any further he would be, as his father used to say, s.o.l.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tunlike hugo, however, victor was not about to kill in one grotesque orgy and then blow his own face off.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tit was the how of this situation he had never fully calculated.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tnot very impressive for someone with a bachelor's in literary theory with a minor in english literature.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tshe was supposed to be a woman, a college-educated woman, not some teen fretting over boy issues.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the'd recently added free wi-fi and was building a coffee bar with a few plush chairs for costumers who didn't want to buy books but liked scrolling the internet in such ambiance.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tdad's smile now was no less sweet but carried a weight of desperation.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tthe store was never very busy except around christmas, but pete made enough to keep the business going and pay mercy twelve dollars an hour.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tshe couldn't tell him how she felt about mom, about her desperation to have sex with joel, her overwhelming feeling of failure about her music and her fear that her life was a pointless string of disappointments.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tinstead, she shrugged and said she hadn't been working that day, but it was a horrible, horrible tragedy.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tthe girl was no beast.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tyou don't have to keep things bottled, dad said.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthen the rats had no chance.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tno pity on your old man.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tthat was easy to do when his mind wasn't flooded with images of the girl on her knees before him, mouth wide.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\twhen she looked at her father, he had that sweet face he always got when things didn't turn out right for her.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tit didn't matter who put it there, hugo or someone before him.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\ti shouldn't have pushed this whole camping thing on you.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tit won't be as bad as you think, dad said.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tit's not like we're heading out unprepared.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tvictor did not let his glance linger over her smooth flesh or soft red hair.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tlesson is, girls, the teacher said, don't go camping.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthe times did not print the letter.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\twhen he didn't relieve himself for a while, he could get a little crazy.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tcan't be prude about it, he said to his friend.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tvictor didn't want to be s.o.l.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\the was the kind of father who never understood what fatherhood was really about.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\ti'm glad i wasn't here, his friend said.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tit's not going to rain.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthe girl had laughed at something her friend said before saying that maybe it wouldn't be so bad, maybe they would run into a cute hiker.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tno doubt, victor said.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tnot to mention the hiking.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\twhy didn't anyone try to stop him?\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tnot that she knew anything about him, but he hadn't exactly been as covert as the situation required.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tshe'd begged to have sex with him, told him she was a virgin and that she really wanted him, even though she wasn't sure that was the case but she was almost out of college and nobody graduated college still a virgin.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the didn't belief in tender consolation.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the had never fired one.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\ti didn't know what to expect.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tshe lived at home and didn't really care about the money.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tcan't trust everything they say, dad.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\ti know you don't want to do this, dad said.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the never seemed to find anything to buy.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the would leave five dollars on the counter and a full cup of coffee mercy higgins did not want to climb some ugly mountain with her father when she could be at home reading a book or working on one of her short stories.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tdad never showed any interest and she certainly had no desire to sleep in a tent on the ground.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthen those two boys started talking about the killing and she was remembering joel for some reason and how he said she smelled stale and had clammy hands and wouldn't have sex with her and then she had graduated a virgin.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tnot threatening, exactly, but certainly strange.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tvictor held the girl's gaze for no more than a second or two.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the hoped the rifle would not be necessary.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\twatch the way the girl, not a girl but a young woman, chewed her food.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tit's not that.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\the couldn't simply speak to her.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tno dinner tonight, son.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tshe could not explain everything to dad.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\twhen he had paid a woman to suck him off, she made no comment about his size.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tthey weren't prissy people; they just liked their quiet time at home.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tthey wanted to have enough energy to make it to a late lunch if not dinner.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\the never returned her hellos.\n",
      "0.4053\t0.702\t-0.0431\t0.6244\tvictor's father hadn't been a bad man or even a bad father.\n",
      "0.3943\t0.6997\t0.3965\t0.8745\tshe did not look up.\n"
     ]
    }
   ],
   "source": [
    "for i in utterancespace:\n",
    "    c = sparsecosine(utterancespace[i],permute(agentabstract,definitepermutation)) #permute(negationabstract,negationpermutation))  #sparseadd(,normalise(permute(negationabstract,negationpermutation)))) #sparseadd(permute(negationabstract,negationpermutation),permute(agentabstract,definitepermutation)))) #,permute(negationabstract,negationpermutation)))\n",
    "    d = sparsecosine(utterancespace[i],permute(negationabstract,negationpermutation))\n",
    "    f = sparsecosine(utterancespace[i],permute(pastabstract,tensepermutation))\n",
    "    e = sparsecosine(utterancespace[i],sparseadd(permute(pastabstract,tensepermutation),sparseadd(permute(agentabstract,definitepermutation),permute(negationabstract,negationpermutation))))\n",
    "    if (d > 0.1):\n",
    "        print (c, d, f, e, i, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weighting\n",
    "\n",
    "now an exponential, math.exp(200*math.pi*bign/globalfrequency(wd)) which defaults to 0.5 if unknown word, i'd prefer arc tan tho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparserecoverabilityexperiment():\n",
    "    step = 0.1  #density\n",
    "    start = 0.1 #density\n",
    "    stop = 1 #density\n",
    "    for dimension in 10,100,500,1000,2000:\n",
    "        print(\"=============================================\")\n",
    "        print(dimension)\n",
    "        for cellsize in [10,100,250]:\n",
    "            print(dimension,\"=============================================\")\n",
    "            print(\"number of features aggregated: \",cellsize)\n",
    "            print(\"analytic optimum: \",(2*cellsize*dimension)**(-1/3)*dimension)\n",
    "            d = start\n",
    "            featspace = {} # a hash of cellsize random vectors of denseness d\n",
    "            featvec = {} # an addition of cellsize random vectors of denseness d into one vector\n",
    "            bigs = 0\n",
    "            bigh = 0\n",
    "            ii = 0\n",
    "            while (d <= stop):\n",
    "                i = 0\n",
    "                featspace[d] = [None]*cellsize\n",
    "                featvec[d] = {}\n",
    "                while (i < cellsize): \n",
    "                    featspace[d][i] = newrandomvector(dimension,d)\n",
    "                    prev = featvec[d]\n",
    "                    featvec[d] = sparseadd(featvec[d],featspace[d][i])\n",
    "                    i += 1\n",
    "                i = 0\n",
    "                lils = 0\n",
    "                lilh = 0\n",
    "                while (i < cellsize):\n",
    "                    h = sparsecosine(featvec[d],newrandomvector(dimension,d))\n",
    "                    c = sparsecosine(featvec[d],featspace[d][i])\n",
    "                    lils += c\n",
    "                    lilh += abs(h)\n",
    "                    i += 1\n",
    "                avs = lils / cellsize\n",
    "                avh = lilh / cellsize\n",
    "                try: \n",
    "                    sn = avs / avh\n",
    "                except ZeroDivisionError:\n",
    "                    sn = 0\n",
    "                print(dimension,\"{0:.2f}\".format(d),\"signal:\",\"{0:.3f}\".format(avs),\"noise: \",\"{0:.3f}\".format(avh),\"signal/noise: \",\"{0:.3f}\".format(sn), sep=\"\\t\",flush=True)\n",
    "                ii += i\n",
    "                bigs += lils\n",
    "                bigh += lilh\n",
    "                d += step\n",
    "            avs = bigs / ii\n",
    "            avh = bigh / ii\n",
    "            try: \n",
    "                sn = avs / avh\n",
    "            except ZeroDivisionError:\n",
    "                sn = 0\n",
    "            print(\"average signal: \",\"{0:.4f}\".format(avs),sep=\"\\t\")    \n",
    "            print(\"average noise: \",\"{0:.4f}\".format(avh),sep=\"\\t\")  \n",
    "            print(\"signal / noise: \",\"{0:.4f}\".format(sn),sep=\"\\t\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "10\n",
      "10 =============================================\n",
      "number of features aggregated:  10\n",
      "analytic optimum:  1.7099759466766973\n",
      "10\t0.10\tsignal:\t0.332\tnoise: \t0.256\tsignal/noise: \t1.294\n",
      "10\t0.20\tsignal:\t0.374\tnoise: \t0.187\tsignal/noise: \t2.000\n",
      "10\t0.30\tsignal:\t0.394\tnoise: \t0.330\tsignal/noise: \t1.192\n",
      "10\t0.40\tsignal:\t0.354\tnoise: \t0.297\tsignal/noise: \t1.191\n",
      "10\t0.50\tsignal:\t0.416\tnoise: \t0.260\tsignal/noise: \t1.600\n",
      "10\t0.60\tsignal:\t0.216\tnoise: \t0.262\tsignal/noise: \t0.824\n",
      "10\t0.70\tsignal:\t0.269\tnoise: \t0.320\tsignal/noise: \t0.841\n",
      "10\t0.80\tsignal:\t0.269\tnoise: \t0.227\tsignal/noise: \t1.184\n",
      "10\t0.90\tsignal:\t0.155\tnoise: \t0.207\tsignal/noise: \t0.750\n",
      "10\t1.00\tsignal:\t0.390\tnoise: \t0.277\tsignal/noise: \t1.407\n",
      "average signal: \t0.3169\n",
      "average noise: \t0.2624\n",
      "signal / noise: \t1.2074\n",
      "10 =============================================\n",
      "number of features aggregated:  100\n",
      "analytic optimum:  0.7937005259840999\n",
      "10\t0.10\tsignal:\t0.082\tnoise: \t0.272\tsignal/noise: \t0.304\n",
      "10\t0.20\tsignal:\t0.136\tnoise: \t0.264\tsignal/noise: \t0.516\n",
      "10\t0.30\tsignal:\t0.057\tnoise: \t0.270\tsignal/noise: \t0.210\n",
      "10\t0.40\tsignal:\t0.099\tnoise: \t0.274\tsignal/noise: \t0.360\n",
      "10\t0.50\tsignal:\t0.140\tnoise: \t0.295\tsignal/noise: \t0.476\n",
      "10\t0.60\tsignal:\t0.071\tnoise: \t0.284\tsignal/noise: \t0.249\n",
      "10\t0.70\tsignal:\t0.073\tnoise: \t0.236\tsignal/noise: \t0.310\n",
      "10\t0.80\tsignal:\t0.090\tnoise: \t0.267\tsignal/noise: \t0.335\n",
      "10\t0.90\tsignal:\t0.064\tnoise: \t0.303\tsignal/noise: \t0.211\n",
      "10\t1.00\tsignal:\t0.128\tnoise: \t0.261\tsignal/noise: \t0.489\n",
      "average signal: \t0.0940\n",
      "average noise: \t0.2727\n",
      "signal / noise: \t0.3445\n",
      "10 =============================================\n",
      "number of features aggregated:  250\n",
      "analytic optimum:  0.5848035476425733\n",
      "10\t0.10\tsignal:\t0.069\tnoise: \t0.269\tsignal/noise: \t0.255\n",
      "10\t0.20\tsignal:\t0.066\tnoise: \t0.278\tsignal/noise: \t0.239\n",
      "10\t0.30\tsignal:\t0.062\tnoise: \t0.279\tsignal/noise: \t0.222\n",
      "10\t0.40\tsignal:\t0.052\tnoise: \t0.264\tsignal/noise: \t0.197\n",
      "10\t0.50\tsignal:\t0.066\tnoise: \t0.264\tsignal/noise: \t0.252\n",
      "10\t0.60\tsignal:\t0.038\tnoise: \t0.247\tsignal/noise: \t0.155\n",
      "10\t0.70\tsignal:\t0.074\tnoise: \t0.290\tsignal/noise: \t0.257\n",
      "10\t0.80\tsignal:\t0.077\tnoise: \t0.265\tsignal/noise: \t0.290\n",
      "10\t0.90\tsignal:\t0.050\tnoise: \t0.271\tsignal/noise: \t0.184\n",
      "10\t1.00\tsignal:\t0.056\tnoise: \t0.277\tsignal/noise: \t0.204\n",
      "average signal: \t0.0611\n",
      "average noise: \t0.2702\n",
      "signal / noise: \t0.2262\n",
      "=============================================\n",
      "100\n",
      "100 =============================================\n",
      "number of features aggregated:  10\n",
      "analytic optimum:  7.937005259840999\n",
      "100\t0.10\tsignal:\t0.316\tnoise: \t0.089\tsignal/noise: \t3.572\n",
      "100\t0.20\tsignal:\t0.346\tnoise: \t0.052\tsignal/noise: \t6.668\n",
      "100\t0.30\tsignal:\t0.348\tnoise: \t0.092\tsignal/noise: \t3.792\n",
      "100\t0.40\tsignal:\t0.291\tnoise: \t0.082\tsignal/noise: \t3.558\n",
      "100\t0.50\tsignal:\t0.339\tnoise: \t0.081\tsignal/noise: \t4.160\n",
      "100\t0.60\tsignal:\t0.270\tnoise: \t0.091\tsignal/noise: \t2.980\n",
      "100\t0.70\tsignal:\t0.348\tnoise: \t0.074\tsignal/noise: \t4.711\n",
      "100\t0.80\tsignal:\t0.334\tnoise: \t0.062\tsignal/noise: \t5.406\n",
      "100\t0.90\tsignal:\t0.286\tnoise: \t0.068\tsignal/noise: \t4.181\n",
      "100\t1.00\tsignal:\t0.331\tnoise: \t0.051\tsignal/noise: \t6.523\n",
      "average signal: \t0.3210\n",
      "average noise: \t0.0741\n",
      "signal / noise: \t4.3317\n",
      "100 =============================================\n",
      "number of features aggregated:  100\n",
      "analytic optimum:  3.6840314986403873\n",
      "100\t0.10\tsignal:\t0.097\tnoise: \t0.068\tsignal/noise: \t1.424\n",
      "100\t0.20\tsignal:\t0.097\tnoise: \t0.086\tsignal/noise: \t1.133\n",
      "100\t0.30\tsignal:\t0.100\tnoise: \t0.074\tsignal/noise: \t1.355\n",
      "100\t0.40\tsignal:\t0.107\tnoise: \t0.080\tsignal/noise: \t1.350\n",
      "100\t0.50\tsignal:\t0.093\tnoise: \t0.084\tsignal/noise: \t1.110\n",
      "100\t0.60\tsignal:\t0.102\tnoise: \t0.087\tsignal/noise: \t1.168\n",
      "100\t0.70\tsignal:\t0.098\tnoise: \t0.079\tsignal/noise: \t1.251\n",
      "100\t0.80\tsignal:\t0.094\tnoise: \t0.086\tsignal/noise: \t1.099\n",
      "100\t0.90\tsignal:\t0.096\tnoise: \t0.088\tsignal/noise: \t1.100\n",
      "100\t1.00\tsignal:\t0.106\tnoise: \t0.065\tsignal/noise: \t1.618\n",
      "average signal: \t0.0992\n",
      "average noise: \t0.0796\n",
      "signal / noise: \t1.2459\n",
      "100 =============================================\n",
      "number of features aggregated:  250\n",
      "analytic optimum:  2.7144176165949068\n",
      "100\t0.10\tsignal:\t0.068\tnoise: \t0.077\tsignal/noise: \t0.877\n",
      "100\t0.20\tsignal:\t0.066\tnoise: \t0.086\tsignal/noise: \t0.771\n",
      "100\t0.30\tsignal:\t0.057\tnoise: \t0.078\tsignal/noise: \t0.725\n",
      "100\t0.40\tsignal:\t0.057\tnoise: \t0.081\tsignal/noise: \t0.703\n",
      "100\t0.50\tsignal:\t0.067\tnoise: \t0.082\tsignal/noise: \t0.822\n",
      "100\t0.60\tsignal:\t0.063\tnoise: \t0.074\tsignal/noise: \t0.854\n",
      "100\t0.70\tsignal:\t0.067\tnoise: \t0.077\tsignal/noise: \t0.873\n",
      "100\t0.80\tsignal:\t0.059\tnoise: \t0.081\tsignal/noise: \t0.733\n",
      "100\t0.90\tsignal:\t0.059\tnoise: \t0.085\tsignal/noise: \t0.693\n",
      "100\t1.00\tsignal:\t0.071\tnoise: \t0.086\tsignal/noise: \t0.827\n",
      "average signal: \t0.0634\n",
      "average noise: \t0.0806\n",
      "signal / noise: \t0.7864\n",
      "=============================================\n",
      "500\n",
      "500 =============================================\n",
      "number of features aggregated:  10\n",
      "analytic optimum:  23.207944168063896\n",
      "500\t0.10\tsignal:\t0.318\tnoise: \t0.032\tsignal/noise: \t9.882\n",
      "500\t0.20\tsignal:\t0.321\tnoise: \t0.034\tsignal/noise: \t9.345\n",
      "500\t0.30\tsignal:\t0.320\tnoise: \t0.053\tsignal/noise: \t6.031\n",
      "500\t0.40\tsignal:\t0.317\tnoise: \t0.032\tsignal/noise: \t9.832\n",
      "500\t0.50\tsignal:\t0.308\tnoise: \t0.038\tsignal/noise: \t8.031\n",
      "500\t0.60\tsignal:\t0.320\tnoise: \t0.029\tsignal/noise: \t10.892\n",
      "500\t0.70\tsignal:\t0.299\tnoise: \t0.032\tsignal/noise: \t9.434\n",
      "500\t0.80\tsignal:\t0.316\tnoise: \t0.041\tsignal/noise: \t7.787\n",
      "500\t0.90\tsignal:\t0.334\tnoise: \t0.037\tsignal/noise: \t9.093\n",
      "500\t1.00\tsignal:\t0.302\tnoise: \t0.031\tsignal/noise: \t9.833\n",
      "average signal: \t0.3153\n",
      "average noise: \t0.0359\n",
      "signal / noise: \t8.7815\n",
      "500 =============================================\n",
      "number of features aggregated:  100\n",
      "analytic optimum:  10.772173450159421\n",
      "500\t0.10\tsignal:\t0.101\tnoise: \t0.036\tsignal/noise: \t2.815\n",
      "500\t0.20\tsignal:\t0.106\tnoise: \t0.037\tsignal/noise: \t2.868\n",
      "500\t0.30\tsignal:\t0.104\tnoise: \t0.035\tsignal/noise: \t3.002\n",
      "500\t0.40\tsignal:\t0.095\tnoise: \t0.038\tsignal/noise: \t2.481\n",
      "500\t0.50\tsignal:\t0.106\tnoise: \t0.041\tsignal/noise: \t2.585\n",
      "500\t0.60\tsignal:\t0.102\tnoise: \t0.036\tsignal/noise: \t2.850\n",
      "500\t0.70\tsignal:\t0.099\tnoise: \t0.036\tsignal/noise: \t2.730\n",
      "500\t0.80\tsignal:\t0.098\tnoise: \t0.037\tsignal/noise: \t2.628\n",
      "500\t0.90\tsignal:\t0.092\tnoise: \t0.037\tsignal/noise: \t2.465\n",
      "500\t1.00\tsignal:\t0.101\tnoise: \t0.036\tsignal/noise: \t2.808\n",
      "average signal: \t0.1002\n",
      "average noise: \t0.0369\n",
      "signal / noise: \t2.7178\n",
      "500 =============================================\n",
      "number of features aggregated:  250\n",
      "analytic optimum:  7.9370052598409995\n",
      "500\t0.10\tsignal:\t0.062\tnoise: \t0.035\tsignal/noise: \t1.785\n",
      "500\t0.20\tsignal:\t0.063\tnoise: \t0.033\tsignal/noise: \t1.923\n",
      "500\t0.30\tsignal:\t0.066\tnoise: \t0.035\tsignal/noise: \t1.886\n",
      "500\t0.40\tsignal:\t0.063\tnoise: \t0.038\tsignal/noise: \t1.674\n",
      "500\t0.50\tsignal:\t0.064\tnoise: \t0.033\tsignal/noise: \t1.933\n",
      "500\t0.60\tsignal:\t0.063\tnoise: \t0.036\tsignal/noise: \t1.723\n",
      "500\t0.70\tsignal:\t0.062\tnoise: \t0.034\tsignal/noise: \t1.841\n",
      "500\t0.80\tsignal:\t0.061\tnoise: \t0.038\tsignal/noise: \t1.627\n",
      "500\t0.90\tsignal:\t0.061\tnoise: \t0.036\tsignal/noise: \t1.690\n",
      "500\t1.00\tsignal:\t0.061\tnoise: \t0.036\tsignal/noise: \t1.723\n",
      "average signal: \t0.0628\n",
      "average noise: \t0.0353\n",
      "signal / noise: \t1.7760\n",
      "=============================================\n",
      "1000\n",
      "1000 =============================================\n",
      "number of features aggregated:  10\n",
      "analytic optimum:  36.840314986403875\n",
      "1000\t0.10\tsignal:\t0.320\tnoise: \t0.022\tsignal/noise: \t14.814\n",
      "1000\t0.20\tsignal:\t0.317\tnoise: \t0.043\tsignal/noise: \t7.419\n",
      "1000\t0.30\tsignal:\t0.329\tnoise: \t0.026\tsignal/noise: \t12.424\n",
      "1000\t0.40\tsignal:\t0.312\tnoise: \t0.035\tsignal/noise: \t9.015\n",
      "1000\t0.50\tsignal:\t0.318\tnoise: \t0.021\tsignal/noise: \t15.378\n",
      "1000\t0.60\tsignal:\t0.313\tnoise: \t0.030\tsignal/noise: \t10.279\n",
      "1000\t0.70\tsignal:\t0.321\tnoise: \t0.026\tsignal/noise: \t12.250\n",
      "1000\t0.80\tsignal:\t0.321\tnoise: \t0.027\tsignal/noise: \t12.021\n",
      "1000\t0.90\tsignal:\t0.316\tnoise: \t0.026\tsignal/noise: \t12.335\n",
      "1000\t1.00\tsignal:\t0.326\tnoise: \t0.030\tsignal/noise: \t10.742\n",
      "average signal: \t0.3193\n",
      "average noise: \t0.0285\n",
      "signal / noise: \t11.1870\n",
      "1000 =============================================\n",
      "number of features aggregated:  100\n",
      "analytic optimum:  17.099759466766976\n",
      "1000\t0.10\tsignal:\t0.101\tnoise: \t0.028\tsignal/noise: \t3.657\n",
      "1000\t0.20\tsignal:\t0.101\tnoise: \t0.025\tsignal/noise: \t4.051\n",
      "1000\t0.30\tsignal:\t0.096\tnoise: \t0.026\tsignal/noise: \t3.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\t0.40\tsignal:\t0.104\tnoise: \t0.023\tsignal/noise: \t4.558\n",
      "1000\t0.50\tsignal:\t0.099\tnoise: \t0.029\tsignal/noise: \t3.393\n",
      "1000\t0.60\tsignal:\t0.099\tnoise: \t0.028\tsignal/noise: \t3.542\n",
      "1000\t0.70\tsignal:\t0.103\tnoise: \t0.023\tsignal/noise: \t4.529\n",
      "1000\t0.80\tsignal:\t0.098\tnoise: \t0.027\tsignal/noise: \t3.680\n",
      "1000\t0.90\tsignal:\t0.098\tnoise: \t0.026\tsignal/noise: \t3.851\n",
      "1000\t1.00\tsignal:\t0.098\tnoise: \t0.028\tsignal/noise: \t3.508\n",
      "average signal: \t0.0998\n",
      "average noise: \t0.0261\n",
      "signal / noise: \t3.8180\n",
      "1000 =============================================\n",
      "number of features aggregated:  250\n",
      "analytic optimum:  12.599210498948734\n",
      "1000\t0.10\tsignal:\t0.061\tnoise: \t0.025\tsignal/noise: \t2.416\n",
      "1000\t0.20\tsignal:\t0.063\tnoise: \t0.025\tsignal/noise: \t2.488\n",
      "1000\t0.30\tsignal:\t0.064\tnoise: \t0.024\tsignal/noise: \t2.647\n",
      "1000\t0.40\tsignal:\t0.063\tnoise: \t0.022\tsignal/noise: \t2.828\n",
      "1000\t0.50\tsignal:\t0.065\tnoise: \t0.024\tsignal/noise: \t2.741\n",
      "1000\t0.60\tsignal:\t0.063\tnoise: \t0.027\tsignal/noise: \t2.347\n",
      "1000\t0.70\tsignal:\t0.062\tnoise: \t0.024\tsignal/noise: \t2.559\n",
      "1000\t0.80\tsignal:\t0.062\tnoise: \t0.025\tsignal/noise: \t2.512\n",
      "1000\t0.90\tsignal:\t0.064\tnoise: \t0.025\tsignal/noise: \t2.563\n",
      "1000\t1.00\tsignal:\t0.063\tnoise: \t0.023\tsignal/noise: \t2.690\n",
      "average signal: \t0.0630\n",
      "average noise: \t0.0245\n",
      "signal / noise: \t2.5723\n",
      "=============================================\n",
      "2000\n",
      "2000 =============================================\n",
      "number of features aggregated:  10\n",
      "analytic optimum:  58.48035476425733\n",
      "2000\t0.10\tsignal:\t0.315\tnoise: \t0.017\tsignal/noise: \t18.194\n",
      "2000\t0.20\tsignal:\t0.324\tnoise: \t0.017\tsignal/noise: \t18.850\n",
      "2000\t0.30\tsignal:\t0.324\tnoise: \t0.022\tsignal/noise: \t14.996\n",
      "2000\t0.40\tsignal:\t0.319\tnoise: \t0.023\tsignal/noise: \t13.994\n",
      "2000\t0.50\tsignal:\t0.317\tnoise: \t0.017\tsignal/noise: \t18.837\n",
      "2000\t0.60\tsignal:\t0.315\tnoise: \t0.020\tsignal/noise: \t15.953\n",
      "2000\t0.70\tsignal:\t0.321\tnoise: \t0.024\tsignal/noise: \t13.434\n",
      "2000\t0.80\tsignal:\t0.317\tnoise: \t0.022\tsignal/noise: \t14.117\n",
      "2000\t0.90\tsignal:\t0.320\tnoise: \t0.025\tsignal/noise: \t12.686\n",
      "2000\t1.00\tsignal:\t0.319\tnoise: \t0.020\tsignal/noise: \t15.657\n",
      "average signal: \t0.3191\n",
      "average noise: \t0.0207\n",
      "signal / noise: \t15.3843\n",
      "2000 =============================================\n",
      "number of features aggregated:  100\n",
      "analytic optimum:  27.144176165949073\n",
      "2000\t0.10\tsignal:\t0.099\tnoise: \t0.017\tsignal/noise: \t5.911\n",
      "2000\t0.20\tsignal:\t0.103\tnoise: \t0.016\tsignal/noise: \t6.589\n",
      "2000\t0.30\tsignal:\t0.100\tnoise: \t0.017\tsignal/noise: \t5.866\n",
      "2000\t0.40\tsignal:\t0.100\tnoise: \t0.017\tsignal/noise: \t5.722\n",
      "2000\t0.50\tsignal:\t0.100\tnoise: \t0.017\tsignal/noise: \t5.864\n",
      "2000\t0.60\tsignal:\t0.102\tnoise: \t0.021\tsignal/noise: \t4.913\n",
      "2000\t0.70\tsignal:\t0.102\tnoise: \t0.017\tsignal/noise: \t5.877\n",
      "2000\t0.80\tsignal:\t0.102\tnoise: \t0.017\tsignal/noise: \t5.968\n",
      "2000\t0.90\tsignal:\t0.101\tnoise: \t0.019\tsignal/noise: \t5.431\n",
      "2000\t1.00\tsignal:\t0.099\tnoise: \t0.019\tsignal/noise: \t5.083\n",
      "average signal: \t0.1008\n",
      "average noise: \t0.0177\n",
      "signal / noise: \t5.6868\n",
      "2000 =============================================\n",
      "number of features aggregated:  250\n",
      "analytic optimum:  20.000000000000004\n",
      "2000\t0.10\tsignal:\t0.064\tnoise: \t0.017\tsignal/noise: \t3.775\n",
      "2000\t0.20\tsignal:\t0.063\tnoise: \t0.017\tsignal/noise: \t3.771\n",
      "2000\t0.30\tsignal:\t0.064\tnoise: \t0.017\tsignal/noise: \t3.711\n",
      "2000\t0.40\tsignal:\t0.064\tnoise: \t0.018\tsignal/noise: \t3.604\n",
      "2000\t0.50\tsignal:\t0.064\tnoise: \t0.018\tsignal/noise: \t3.638\n",
      "2000\t0.60\tsignal:\t0.064\tnoise: \t0.018\tsignal/noise: \t3.596\n",
      "2000\t0.70\tsignal:\t0.065\tnoise: \t0.018\tsignal/noise: \t3.674\n",
      "2000\t0.80\tsignal:\t0.064\tnoise: \t0.017\tsignal/noise: \t3.758\n",
      "2000\t0.90\tsignal:\t0.063\tnoise: \t0.018\tsignal/noise: \t3.566\n",
      "2000\t1.00\tsignal:\t0.063\tnoise: \t0.016\tsignal/noise: \t3.843\n",
      "average signal: \t0.0638\n",
      "average noise: \t0.0173\n",
      "signal / noise: \t3.6915\n"
     ]
    }
   ],
   "source": [
    "sparserecoverabilityexperiment()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def genplot():\n",
    "    step = 0.01  #density\n",
    "    start = 0.01 #density\n",
    "    stop = 0.02 #density\n",
    "    w = 5 # how many random noise vectors\n",
    "    for dimension in 10,100,500,1000,2000:\n",
    "        print(\"=============================================\")\n",
    "        for cellsize in [20]:\n",
    "            print(\"dimensionality \",dimension)\n",
    "            print(\"number of features aggregated: \",cellsize)\n",
    " #           print(\"analytic optimum: \",(2*cellsize*dimension)**(-1/3)*dimension)\n",
    "            d = start\n",
    "            featspace = {} # a hash of cellsize random vectors of denseness d\n",
    "            featvec = {} # an addition of cellsize random vectors of denseness d into one vector\n",
    "            bigs = 0\n",
    "            bigh = 0\n",
    "            ii = 0\n",
    "            jj = 0\n",
    "            kk = 0\n",
    "            hh = {}\n",
    "            cc = {}\n",
    "            while (d <= stop):\n",
    "                i = 0\n",
    "                featspace[d] = [None]*cellsize\n",
    "                featvec[d] = {}\n",
    "                while (i < cellsize): \n",
    "                    featspace[d][i] = newrandomvector(dimension,d)\n",
    "                    prev = featvec[d]\n",
    "                    featvec[d] = sparseadd(featvec[d],featspace[d][i])\n",
    "                    i += 1\n",
    "                i = 0\n",
    "                while (i < cellsize):\n",
    "                    r = 0\n",
    "                    while (r < w):\n",
    "                        h = sparsecosine(featvec[d],newrandomvector(dimension,d),False)\n",
    "                        jj += 1\n",
    "                        bigh += abs(h)\n",
    "                        hh[kk] = h\n",
    "                        r += 1\n",
    "                        kk += 1\n",
    "                    c = sparsecosine(featvec[d],featspace[d][i],False)\n",
    "                    bigs += c\n",
    "                    i += 1\n",
    "                    cc[kk] = c\n",
    "                    kk += 1\n",
    "                    r = 0\n",
    "                    while (r < w):\n",
    "                        h = sparsecosine(featvec[d],newrandomvector(dimension,d),False)\n",
    "                        jj += 1\n",
    "                        bigh += abs(h)\n",
    "                        hh[kk] = h\n",
    "                        r += 1\n",
    "                        kk += 1\n",
    "                ii += i\n",
    "                d += step\n",
    "            avs = bigs / ii\n",
    "            avh = bigh / jj\n",
    "            for hi in hh:\n",
    "                print(\"(\",hi,\",\",hh[hi],\")\",end=\" \")\n",
    "            print()\n",
    "            for ci in cc:\n",
    "                print(\"(\",ci,\",\",cc[ci],\")\",end=\" \")\n",
    "            print()                 \n",
    "            print(\"average signal: \",\"{0:.4f}\".format(avs),sep=\"\\t\")    \n",
    "            print(\"average noise: \",\"{0:.4f}\".format(avh),sep=\"\\t\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "dimensionality  10\n",
      "number of features aggregated:  20\n",
      "( 0 , 0 ) ( 1 , 0 ) ( 2 , 0 ) ( 3 , 0 ) ( 4 , 0 ) ( 6 , 0 ) ( 7 , 0 ) ( 8 , 0 ) ( 9 , 0 ) ( 10 , 0 ) ( 11 , 0 ) ( 12 , 0 ) ( 13 , 0 ) ( 14 , 0 ) ( 15 , 0 ) ( 17 , 0 ) ( 18 , 0 ) ( 19 , 0 ) ( 20 , 0 ) ( 21 , 0 ) ( 22 , 0 ) ( 23 , 0 ) ( 24 , 0 ) ( 25 , 0 ) ( 26 , 0 ) ( 28 , 0 ) ( 29 , 0 ) ( 30 , 0 ) ( 31 , 0 ) ( 32 , 0 ) ( 33 , 0 ) ( 34 , 0 ) ( 35 , 0 ) ( 36 , 0 ) ( 37 , 0 ) ( 39 , 0 ) ( 40 , 0 ) ( 41 , 0 ) ( 42 , 0 ) ( 43 , 0 ) ( 44 , 0 ) ( 45 , 0 ) ( 46 , 0 ) ( 47 , 0 ) ( 48 , 0 ) ( 50 , 0 ) ( 51 , 0 ) ( 52 , 0 ) ( 53 , 0 ) ( 54 , 0 ) ( 55 , 0 ) ( 56 , 0 ) ( 57 , 0 ) ( 58 , 0 ) ( 59 , 0 ) ( 61 , 0 ) ( 62 , 0 ) ( 63 , 0 ) ( 64 , 0 ) ( 65 , 0 ) ( 66 , 0 ) ( 67 , 0 ) ( 68 , 0 ) ( 69 , 0 ) ( 70 , 0 ) ( 72 , 0 ) ( 73 , 0 ) ( 74 , 0 ) ( 75 , 0 ) ( 76 , 0 ) ( 77 , 0 ) ( 78 , 0 ) ( 79 , 0 ) ( 80 , 0 ) ( 81 , 0 ) ( 83 , 0 ) ( 84 , 0 ) ( 85 , 0 ) ( 86 , 0 ) ( 87 , 0 ) ( 88 , 0 ) ( 89 , 0 ) ( 90 , 0 ) ( 91 , 0 ) ( 92 , 0 ) ( 94 , 0 ) ( 95 , 0 ) ( 96 , 0 ) ( 97 , 0 ) ( 98 , 0 ) ( 99 , 0 ) ( 100 , 0 ) ( 101 , 0 ) ( 102 , 0 ) ( 103 , 0 ) ( 105 , 0 ) ( 106 , 0 ) ( 107 , 0 ) ( 108 , 0 ) ( 109 , 0 ) ( 110 , 0 ) ( 111 , 0 ) ( 112 , 0 ) ( 113 , 0 ) ( 114 , 0 ) ( 116 , 0 ) ( 117 , 0 ) ( 118 , 0 ) ( 119 , 0 ) ( 120 , 0 ) ( 121 , 0 ) ( 122 , 0 ) ( 123 , 0 ) ( 124 , 0 ) ( 125 , 0 ) ( 127 , 0 ) ( 128 , 0 ) ( 129 , 0 ) ( 130 , 0 ) ( 131 , 0 ) ( 132 , 0 ) ( 133 , 0 ) ( 134 , 0 ) ( 135 , 0 ) ( 136 , 0 ) ( 138 , 0 ) ( 139 , 0 ) ( 140 , 0 ) ( 141 , 0 ) ( 142 , 0 ) ( 143 , 0 ) ( 144 , 0 ) ( 145 , 0 ) ( 146 , 0 ) ( 147 , 0 ) ( 149 , 0 ) ( 150 , 0 ) ( 151 , 0 ) ( 152 , 0 ) ( 153 , 0 ) ( 154 , 0 ) ( 155 , 0 ) ( 156 , 0 ) ( 157 , 0 ) ( 158 , 0 ) ( 160 , 0 ) ( 161 , 0 ) ( 162 , 0 ) ( 163 , 0 ) ( 164 , 0 ) ( 165 , 0 ) ( 166 , 0 ) ( 167 , 0 ) ( 168 , 0 ) ( 169 , 0 ) ( 171 , 0 ) ( 172 , 0 ) ( 173 , 0 ) ( 174 , 0 ) ( 175 , 0 ) ( 176 , 0 ) ( 177 , 0 ) ( 178 , 0 ) ( 179 , 0 ) ( 180 , 0 ) ( 182 , 0 ) ( 183 , 0 ) ( 184 , 0 ) ( 185 , 0 ) ( 186 , 0 ) ( 187 , 0 ) ( 188 , 0 ) ( 189 , 0 ) ( 190 , 0 ) ( 191 , 0 ) ( 193 , 0 ) ( 194 , 0 ) ( 195 , 0 ) ( 196 , 0 ) ( 197 , 0 ) ( 198 , 0 ) ( 199 , 0 ) ( 200 , 0 ) ( 201 , 0 ) ( 202 , 0 ) ( 204 , 0 ) ( 205 , 0 ) ( 206 , 0 ) ( 207 , 0 ) ( 208 , 0 ) ( 209 , 0 ) ( 210 , 0 ) ( 211 , 0 ) ( 212 , 0 ) ( 213 , 0 ) ( 215 , 0 ) ( 216 , 0 ) ( 217 , 0 ) ( 218 , 0 ) ( 219 , 0 ) ( 220 , 0 ) ( 221 , 0 ) ( 222 , 0 ) ( 223 , 0 ) ( 224 , 0 ) ( 226 , 0 ) ( 227 , 0 ) ( 228 , 0 ) ( 229 , 0 ) ( 230 , 0 ) ( 231 , 0 ) ( 232 , 0 ) ( 233 , 0 ) ( 234 , 0 ) ( 235 , 0 ) ( 237 , 0 ) ( 238 , 0 ) ( 239 , 0 ) ( 240 , 0 ) ( 241 , 0 ) ( 242 , 0 ) ( 243 , 0 ) ( 244 , 0 ) ( 245 , 0 ) ( 246 , 0 ) ( 248 , 0 ) ( 249 , 0 ) ( 250 , 0 ) ( 251 , 0 ) ( 252 , 0 ) ( 253 , 0 ) ( 254 , 0 ) ( 255 , 0 ) ( 256 , 0 ) ( 257 , 0 ) ( 259 , 0 ) ( 260 , 0 ) ( 261 , 0 ) ( 262 , 0 ) ( 263 , 0 ) ( 264 , 0 ) ( 265 , 0 ) ( 266 , 0 ) ( 267 , 0 ) ( 268 , 0 ) ( 270 , 0 ) ( 271 , 0 ) ( 272 , 0 ) ( 273 , 0 ) ( 274 , 0 ) ( 275 , 0 ) ( 276 , 0 ) ( 277 , 0 ) ( 278 , 0 ) ( 279 , 0 ) ( 281 , 0 ) ( 282 , 0 ) ( 283 , 0 ) ( 284 , 0 ) ( 285 , 0 ) ( 286 , 0 ) ( 287 , 0 ) ( 288 , 0 ) ( 289 , 0 ) ( 290 , 0 ) ( 292 , 0 ) ( 293 , 0 ) ( 294 , 0 ) ( 295 , 0 ) ( 296 , 0 ) ( 297 , 0 ) ( 298 , 0 ) ( 299 , 0 ) ( 300 , 0 ) ( 301 , 0 ) ( 303 , 0 ) ( 304 , 0 ) ( 305 , 0 ) ( 306 , 0 ) ( 307 , 0 ) ( 308 , 0 ) ( 309 , 0 ) ( 310 , 0 ) ( 311 , 0 ) ( 312 , 0 ) ( 314 , 0 ) ( 315 , 0 ) ( 316 , 0 ) ( 317 , 0 ) ( 318 , 0 ) ( 319 , 0 ) ( 320 , 0 ) ( 321 , 0 ) ( 322 , 0 ) ( 323 , 0 ) ( 325 , 0 ) ( 326 , 0 ) ( 327 , 0 ) ( 328 , 0 ) ( 329 , 0 ) ( 330 , 0 ) ( 331 , 0 ) ( 332 , 0 ) ( 333 , 0 ) ( 334 , 0 ) ( 336 , 0 ) ( 337 , 0 ) ( 338 , 0 ) ( 339 , 0 ) ( 340 , 0 ) ( 341 , 0 ) ( 342 , 0 ) ( 343 , 0 ) ( 344 , 0 ) ( 345 , 0 ) ( 347 , 0 ) ( 348 , 0 ) ( 349 , 0 ) ( 350 , 0 ) ( 351 , 0 ) ( 352 , 0 ) ( 353 , 0 ) ( 354 , 0 ) ( 355 , 0 ) ( 356 , 0 ) ( 358 , 0 ) ( 359 , 0 ) ( 360 , 0 ) ( 361 , 0 ) ( 362 , 0 ) ( 363 , 0 ) ( 364 , 0 ) ( 365 , 0 ) ( 366 , 0 ) ( 367 , 0 ) ( 369 , 0 ) ( 370 , 0 ) ( 371 , 0 ) ( 372 , 0 ) ( 373 , 0 ) ( 374 , 0 ) ( 375 , 0 ) ( 376 , 0 ) ( 377 , 0 ) ( 378 , 0 ) ( 380 , 0 ) ( 381 , 0 ) ( 382 , 0 ) ( 383 , 0 ) ( 384 , 0 ) ( 385 , 0 ) ( 386 , 0 ) ( 387 , 0 ) ( 388 , 0 ) ( 389 , 0 ) ( 391 , 0 ) ( 392 , 0 ) ( 393 , 0 ) ( 394 , 0 ) ( 395 , 0 ) ( 396 , 0 ) ( 397 , 0 ) ( 398 , 0 ) ( 399 , 0 ) ( 400 , 0 ) ( 402 , 0 ) ( 403 , 0 ) ( 404 , 0 ) ( 405 , 0 ) ( 406 , 0 ) ( 407 , 0 ) ( 408 , 0 ) ( 409 , 0 ) ( 410 , 0 ) ( 411 , 0 ) ( 413 , 0 ) ( 414 , 0 ) ( 415 , 0 ) ( 416 , 0 ) ( 417 , 0 ) ( 418 , 0 ) ( 419 , 0 ) ( 420 , 0 ) ( 421 , 0 ) ( 422 , 0 ) ( 424 , 0 ) ( 425 , 0 ) ( 426 , 0 ) ( 427 , 0 ) ( 428 , 0 ) ( 429 , 0 ) ( 430 , 0 ) ( 431 , 0 ) ( 432 , 0 ) ( 433 , 0 ) ( 435 , 0 ) ( 436 , 0 ) ( 437 , 0 ) ( 438 , 0 ) ( 439 , 0 ) \n",
      "( 192 , 0 ) ( 258 , 0 ) ( 324 , 0 ) ( 5 , 0 ) ( 390 , 0 ) ( 71 , 0 ) ( 137 , 0 ) ( 203 , 0 ) ( 269 , 0 ) ( 335 , 0 ) ( 16 , 0 ) ( 401 , 0 ) ( 82 , 0 ) ( 148 , 0 ) ( 214 , 0 ) ( 280 , 0 ) ( 346 , 0 ) ( 27 , 0 ) ( 412 , 0 ) ( 93 , 0 ) ( 159 , 0 ) ( 225 , 0 ) ( 291 , 0 ) ( 357 , 0 ) ( 38 , 0 ) ( 423 , 0 ) ( 104 , 0 ) ( 170 , 0 ) ( 236 , 0 ) ( 302 , 0 ) ( 368 , 0 ) ( 49 , 0 ) ( 434 , 0 ) ( 115 , 0 ) ( 181 , 0 ) ( 247 , 0 ) ( 313 , 0 ) ( 379 , 0 ) ( 60 , 0 ) ( 126 , 0 ) \n",
      "average signal: \t0.0000\n",
      "average noise: \t0.0000\n",
      "=============================================\n",
      "dimensionality  100\n",
      "number of features aggregated:  20\n",
      "( 0 , 0.11470786693528087 ) ( 1 , 0.11470786693528087 ) ( 2 , -0.11470786693528087 ) ( 3 , 0.0 ) ( 4 , -0.11470786693528087 ) ( 6 , -0.11470786693528087 ) ( 7 , 0.0 ) ( 8 , -0.11470786693528087 ) ( 9 , 0.0 ) ( 10 , -0.11470786693528087 ) ( 11 , 0.0 ) ( 12 , 0.11470786693528087 ) ( 13 , 0.0 ) ( 14 , 0.0 ) ( 15 , 0.0 ) ( 17 , 0.11470786693528087 ) ( 18 , -0.11470786693528087 ) ( 19 , 0.0 ) ( 20 , 0.0 ) ( 21 , -0.11470786693528087 ) ( 22 , 0.11470786693528087 ) ( 23 , 0.0 ) ( 24 , 0.0 ) ( 25 , 0.0 ) ( 26 , -0.11470786693528087 ) ( 28 , 0.11470786693528087 ) ( 29 , 0.0 ) ( 30 , 0.22941573387056174 ) ( 31 , 0.0 ) ( 32 , -0.22941573387056174 ) ( 33 , 0.0 ) ( 34 , -0.11470786693528087 ) ( 35 , 0.11470786693528087 ) ( 36 , 0.0 ) ( 37 , 0.0 ) ( 39 , 0.0 ) ( 40 , 0.11470786693528087 ) ( 41 , 0.0 ) ( 42 , 0.11470786693528087 ) ( 43 , 0.11470786693528087 ) ( 44 , -0.11470786693528087 ) ( 45 , -0.22941573387056174 ) ( 46 , -0.11470786693528087 ) ( 47 , 0.11470786693528087 ) ( 48 , -0.11470786693528087 ) ( 50 , 0.0 ) ( 51 , 0.0 ) ( 52 , -0.11470786693528087 ) ( 53 , -0.11470786693528087 ) ( 54 , 0.11470786693528087 ) ( 55 , 0.22941573387056174 ) ( 56 , 0.0 ) ( 57 , 0.0 ) ( 58 , 0.11470786693528087 ) ( 59 , 0.11470786693528087 ) ( 61 , 0.0 ) ( 62 , 0.0 ) ( 63 , -0.11470786693528087 ) ( 64 , 0.11470786693528087 ) ( 65 , 0.0 ) ( 66 , -0.11470786693528087 ) ( 67 , 0.0 ) ( 68 , 0.11470786693528087 ) ( 69 , 0.22941573387056174 ) ( 70 , 0.0 ) ( 72 , 0.11470786693528087 ) ( 73 , 0.0 ) ( 74 , 0.0 ) ( 75 , -0.11470786693528087 ) ( 76 , 0.0 ) ( 77 , 0.0 ) ( 78 , 0.0 ) ( 79 , -0.11470786693528087 ) ( 80 , 0.0 ) ( 81 , 0.11470786693528087 ) ( 83 , -0.11470786693528087 ) ( 84 , 0.0 ) ( 85 , -0.11470786693528087 ) ( 86 , 0.0 ) ( 87 , 0.0 ) ( 88 , 0.0 ) ( 89 , 0.0 ) ( 90 , 0.11470786693528087 ) ( 91 , 0.0 ) ( 92 , 0.0 ) ( 94 , 0.11470786693528087 ) ( 95 , -0.11470786693528087 ) ( 96 , -0.11470786693528087 ) ( 97 , -0.11470786693528087 ) ( 98 , 0.0 ) ( 99 , 0.0 ) ( 100 , 0.11470786693528087 ) ( 101 , 0.11470786693528087 ) ( 102 , 0.0 ) ( 103 , 0.0 ) ( 105 , 0.0 ) ( 106 , 0.11470786693528087 ) ( 107 , 0.0 ) ( 108 , 0.22941573387056174 ) ( 109 , -0.11470786693528087 ) ( 110 , -0.11470786693528087 ) ( 111 , 0.0 ) ( 112 , -0.11470786693528087 ) ( 113 , 0.0 ) ( 114 , 0.11470786693528087 ) ( 116 , 0.11470786693528087 ) ( 117 , 0.0 ) ( 118 , 0.11470786693528087 ) ( 119 , 0.0 ) ( 120 , -0.11470786693528087 ) ( 121 , 0.0 ) ( 122 , -0.11470786693528087 ) ( 123 , 0.11470786693528087 ) ( 124 , 0.0 ) ( 125 , 0.0 ) ( 127 , 0.22941573387056174 ) ( 128 , 0.0 ) ( 129 , 0.11470786693528087 ) ( 130 , 0.0 ) ( 131 , 0.11470786693528087 ) ( 132 , 0.0 ) ( 133 , 0.0 ) ( 134 , 0.0 ) ( 135 , 0.0 ) ( 136 , 0.11470786693528087 ) ( 138 , 0.0 ) ( 139 , 0.0 ) ( 140 , 0.0 ) ( 141 , 0.0 ) ( 142 , 0.0 ) ( 143 , -0.11470786693528087 ) ( 144 , 0.0 ) ( 145 , 0.0 ) ( 146 , 0.0 ) ( 147 , -0.11470786693528087 ) ( 149 , 0.0 ) ( 150 , 0.0 ) ( 151 , 0.22941573387056174 ) ( 152 , -0.11470786693528087 ) ( 153 , 0.0 ) ( 154 , -0.11470786693528087 ) ( 155 , 0.0 ) ( 156 , 0.0 ) ( 157 , -0.11470786693528087 ) ( 158 , 0.0 ) ( 160 , 0.11470786693528087 ) ( 161 , 0.0 ) ( 162 , 0.0 ) ( 163 , -0.11470786693528087 ) ( 164 , 0.22941573387056174 ) ( 165 , 0.0 ) ( 166 , 0.0 ) ( 167 , -0.11470786693528087 ) ( 168 , 0.11470786693528087 ) ( 169 , 0.0 ) ( 171 , 0.0 ) ( 172 , 0.11470786693528087 ) ( 173 , 0.0 ) ( 174 , 0.0 ) ( 175 , 0.0 ) ( 176 , 0.11470786693528087 ) ( 177 , -0.11470786693528087 ) ( 178 , 0.0 ) ( 179 , 0.0 ) ( 180 , -0.11470786693528087 ) ( 182 , 0.0 ) ( 183 , 0.0 ) ( 184 , 0.0 ) ( 185 , 0.0 ) ( 186 , 0.11470786693528087 ) ( 187 , -0.11470786693528087 ) ( 188 , 0.0 ) ( 189 , 0.11470786693528087 ) ( 190 , 0.0 ) ( 191 , 0.11470786693528087 ) ( 193 , -0.11470786693528087 ) ( 194 , 0.11470786693528087 ) ( 195 , -0.11470786693528087 ) ( 196 , 0.11470786693528087 ) ( 197 , -0.22941573387056174 ) ( 198 , 0.0 ) ( 199 , -0.11470786693528087 ) ( 200 , 0.0 ) ( 201 , 0.0 ) ( 202 , 0.11470786693528087 ) ( 204 , -0.11470786693528087 ) ( 205 , 0.0 ) ( 206 , 0.0 ) ( 207 , 0.22941573387056174 ) ( 208 , -0.11470786693528087 ) ( 209 , 0.11470786693528087 ) ( 210 , 0.11470786693528087 ) ( 211 , 0.0 ) ( 212 , 0.22941573387056174 ) ( 213 , 0.11470786693528087 ) ( 215 , 0.0 ) ( 216 , 0.22941573387056174 ) ( 217 , 0.0 ) ( 218 , 0.11470786693528087 ) ( 219 , 0.0 ) ( 220 , -0.09805806756909201 ) ( 221 , 0.09805806756909201 ) ( 222 , 0.09805806756909201 ) ( 223 , 0.09805806756909201 ) ( 224 , 0.0 ) ( 226 , 0.09805806756909201 ) ( 227 , -0.19611613513818402 ) ( 228 , -0.09805806756909201 ) ( 229 , 0.0 ) ( 230 , 0.0 ) ( 231 , 0.0 ) ( 232 , 0.0 ) ( 233 , -0.09805806756909201 ) ( 234 , 0.0 ) ( 235 , 0.0 ) ( 237 , 0.0 ) ( 238 , -0.09805806756909201 ) ( 239 , 0.0 ) ( 240 , 0.09805806756909201 ) ( 241 , 0.09805806756909201 ) ( 242 , 0.09805806756909201 ) ( 243 , 0.0 ) ( 244 , 0.09805806756909201 ) ( 245 , 0.0 ) ( 246 , 0.09805806756909201 ) ( 248 , -0.09805806756909201 ) ( 249 , -0.09805806756909201 ) ( 250 , 0.19611613513818402 ) ( 251 , 0.0 ) ( 252 , 0.0 ) ( 253 , -0.09805806756909201 ) ( 254 , -0.19611613513818402 ) ( 255 , 0.0 ) ( 256 , 0.0 ) ( 257 , 0.0 ) ( 259 , 0.0 ) ( 260 , -0.09805806756909201 ) ( 261 , 0.09805806756909201 ) ( 262 , 0.0 ) ( 263 , 0.0 ) ( 264 , 0.0 ) ( 265 , 0.0 ) ( 266 , 0.0 ) ( 267 , 0.0 ) ( 268 , 0.0 ) ( 270 , 0.19611613513818402 ) ( 271 , 0.09805806756909201 ) ( 272 , 0.0 ) ( 273 , 0.09805806756909201 ) ( 274 , 0.09805806756909201 ) ( 275 , 0.09805806756909201 ) ( 276 , -0.09805806756909201 ) ( 277 , 0.09805806756909201 ) ( 278 , 0.09805806756909201 ) ( 279 , 0.09805806756909201 ) ( 281 , 0.09805806756909201 ) ( 282 , 0.0 ) ( 283 , 0.0 ) ( 284 , 0.09805806756909201 ) ( 285 , -0.294174202707276 ) ( 286 , -0.09805806756909201 ) ( 287 , -0.09805806756909201 ) ( 288 , -0.09805806756909201 ) ( 289 , 0.0 ) ( 290 , 0.09805806756909201 ) ( 292 , 0.0 ) ( 293 , 0.09805806756909201 ) ( 294 , 0.0 ) ( 295 , 0.0 ) ( 296 , -0.09805806756909201 ) ( 297 , 0.0 ) ( 298 , 0.19611613513818402 ) ( 299 , 0.0 ) ( 300 , 0.09805806756909201 ) ( 301 , 0.0 ) ( 303 , 0.19611613513818402 ) ( 304 , 0.09805806756909201 ) ( 305 , -0.19611613513818402 ) ( 306 , 0.0 ) ( 307 , -0.19611613513818402 ) ( 308 , 0.0 ) ( 309 , -0.09805806756909201 ) ( 310 , -0.09805806756909201 ) ( 311 , 0.0 ) ( 312 , 0.0 ) ( 314 , 0.0 ) ( 315 , 0.0 ) ( 316 , 0.39223227027636803 ) ( 317 , -0.09805806756909201 ) ( 318 , 0.0 ) ( 319 , 0.09805806756909201 ) ( 320 , -0.09805806756909201 ) ( 321 , 0.0 ) ( 322 , 0.09805806756909201 ) ( 323 , 0.0 ) ( 325 , 0.0 ) ( 326 , 0.0 ) ( 327 , 0.0 ) ( 328 , 0.0 ) ( 329 , 0.0 ) ( 330 , 0.09805806756909201 ) ( 331 , 0.0 ) ( 332 , 0.0 ) ( 333 , 0.09805806756909201 ) ( 334 , 0.0 ) ( 336 , 0.0 ) ( 337 , 0.0 ) ( 338 , 0.09805806756909201 ) ( 339 , 0.0 ) ( 340 , 0.0 ) ( 341 , 0.0 ) ( 342 , 0.0 ) ( 343 , 0.0 ) ( 344 , 0.0 ) ( 345 , 0.0 ) ( 347 , 0.09805806756909201 ) ( 348 , 0.0 ) ( 349 , -0.09805806756909201 ) ( 350 , 0.0 ) ( 351 , 0.0 ) ( 352 , -0.09805806756909201 ) ( 353 , -0.09805806756909201 ) ( 354 , -0.19611613513818402 ) ( 355 , 0.0 ) ( 356 , 0.0 ) ( 358 , 0.0 ) ( 359 , 0.09805806756909201 ) ( 360 , 0.0 ) ( 361 , -0.09805806756909201 ) ( 362 , -0.09805806756909201 ) ( 363 , 0.0 ) ( 364 , 0.0 ) ( 365 , -0.19611613513818402 ) ( 366 , 0.09805806756909201 ) ( 367 , 0.0 ) ( 369 , 0.0 ) ( 370 , -0.294174202707276 ) ( 371 , 0.09805806756909201 ) ( 372 , 0.0 ) ( 373 , 0.0 ) ( 374 , 0.0 ) ( 375 , 0.0 ) ( 376 , 0.0 ) ( 377 , 0.0 ) ( 378 , 0.0 ) ( 380 , 0.0 ) ( 381 , -0.09805806756909201 ) ( 382 , -0.19611613513818402 ) ( 383 , 0.0 ) ( 384 , -0.09805806756909201 ) ( 385 , -0.19611613513818402 ) ( 386 , 0.0 ) ( 387 , 0.0 ) ( 388 , -0.09805806756909201 ) ( 389 , -0.09805806756909201 ) ( 391 , 0.0 ) ( 392 , 0.09805806756909201 ) ( 393 , 0.0 ) ( 394 , 0.09805806756909201 ) ( 395 , 0.0 ) ( 396 , 0.0 ) ( 397 , -0.09805806756909201 ) ( 398 , 0.0 ) ( 399 , 0.0 ) ( 400 , -0.09805806756909201 ) ( 402 , 0.0 ) ( 403 , -0.09805806756909201 ) ( 404 , 0.0 ) ( 405 , -0.19611613513818402 ) ( 406 , -0.19611613513818402 ) ( 407 , -0.09805806756909201 ) ( 408 , 0.09805806756909201 ) ( 409 , 0.0 ) ( 410 , 0.0 ) ( 411 , 0.0 ) ( 413 , 0.0 ) ( 414 , 0.09805806756909201 ) ( 415 , -0.19611613513818402 ) ( 416 , 0.0 ) ( 417 , 0.0 ) ( 418 , 0.0 ) ( 419 , 0.0 ) ( 420 , 0.0 ) ( 421 , 0.0 ) ( 422 , 0.0 ) ( 424 , 0.0 ) ( 425 , 0.0 ) ( 426 , 0.0 ) ( 427 , 0.0 ) ( 428 , 0.0 ) ( 429 , -0.09805806756909201 ) ( 430 , 0.09805806756909201 ) ( 431 , 0.0 ) ( 432 , -0.09805806756909201 ) ( 433 , 0.09805806756909201 ) ( 435 , 0.0 ) ( 436 , 0.19611613513818402 ) ( 437 , -0.39223227027636803 ) ( 438 , -0.09805806756909201 ) ( 439 , 0.0 ) \n",
      "( 192 , 0.11470786693528087 ) ( 258 , 0.19611613513818402 ) ( 324 , 0.09805806756909201 ) ( 5 , 0.3441236008058426 ) ( 390 , 0.294174202707276 ) ( 71 , 0.22941573387056174 ) ( 137 , 0.11470786693528087 ) ( 203 , 0.22941573387056174 ) ( 269 , 0.19611613513818402 ) ( 335 , 0.49029033784546 ) ( 16 , 0.3441236008058426 ) ( 401 , 0.294174202707276 ) ( 82 , 0.22941573387056174 ) ( 148 , 0.22941573387056174 ) ( 214 , 0.22941573387056174 ) ( 280 , 0.09805806756909201 ) ( 346 , 0.19611613513818402 ) ( 27 , 0.22941573387056174 ) ( 412 , 0.19611613513818402 ) ( 93 , 0.3441236008058426 ) ( 159 , 0.11470786693528087 ) ( 225 , 0.39223227027636803 ) ( 291 , 0.294174202707276 ) ( 357 , 0.39223227027636803 ) ( 38 , 0.11470786693528087 ) ( 423 , 0.294174202707276 ) ( 104 , 0.3441236008058426 ) ( 170 , 0.3441236008058426 ) ( 236 , 0.294174202707276 ) ( 302 , 0.09805806756909201 ) ( 368 , 0.39223227027636803 ) ( 49 , 0.3441236008058426 ) ( 434 , 0.19611613513818402 ) ( 115 , -0.11470786693528087 ) ( 181 , 0.22941573387056174 ) ( 247 , 0.09805806756909201 ) ( 313 , 0.294174202707276 ) ( 379 , 0.294174202707276 ) ( 60 , 0.22941573387056174 ) ( 126 , 0.11470786693528087 ) \n",
      "average signal: \t0.2364\n",
      "average noise: \t0.0611\n",
      "=============================================\n",
      "dimensionality  500\n",
      "number of features aggregated:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0 , 0.0 ) ( 1 , -0.037904902178945175 ) ( 2 , 0.0 ) ( 3 , 0.0 ) ( 4 , -0.11371470653683552 ) ( 6 , -0.037904902178945175 ) ( 7 , 0.037904902178945175 ) ( 8 , -0.037904902178945175 ) ( 9 , 0.0 ) ( 10 , 0.0 ) ( 11 , 0.0 ) ( 12 , -0.07580980435789035 ) ( 13 , 0.037904902178945175 ) ( 14 , 0.037904902178945175 ) ( 15 , 0.0 ) ( 17 , 0.037904902178945175 ) ( 18 , 0.0 ) ( 19 , 0.11371470653683552 ) ( 20 , -0.07580980435789035 ) ( 21 , -0.037904902178945175 ) ( 22 , 0.0 ) ( 23 , -0.037904902178945175 ) ( 24 , 0.0 ) ( 25 , -0.07580980435789035 ) ( 26 , 0.07580980435789035 ) ( 28 , 0.037904902178945175 ) ( 29 , 0.0 ) ( 30 , -0.037904902178945175 ) ( 31 , 0.0 ) ( 32 , 0.0 ) ( 33 , 0.037904902178945175 ) ( 34 , 0.037904902178945175 ) ( 35 , 0.0 ) ( 36 , 0.0 ) ( 37 , -0.037904902178945175 ) ( 39 , -0.07580980435789035 ) ( 40 , -0.037904902178945175 ) ( 41 , -0.07580980435789035 ) ( 42 , 0.18952451089472588 ) ( 43 , 0.0 ) ( 44 , -0.037904902178945175 ) ( 45 , 0.0 ) ( 46 , 0.037904902178945175 ) ( 47 , 0.0 ) ( 48 , -0.037904902178945175 ) ( 50 , -0.037904902178945175 ) ( 51 , 0.037904902178945175 ) ( 52 , 0.07580980435789035 ) ( 53 , -0.11371470653683552 ) ( 54 , 0.0 ) ( 55 , 0.0 ) ( 56 , 0.037904902178945175 ) ( 57 , 0.037904902178945175 ) ( 58 , 0.0 ) ( 59 , 0.0 ) ( 61 , 0.037904902178945175 ) ( 62 , -0.037904902178945175 ) ( 63 , 0.0 ) ( 64 , 0.0 ) ( 65 , 0.0 ) ( 66 , -0.037904902178945175 ) ( 67 , 0.0 ) ( 68 , -0.037904902178945175 ) ( 69 , 0.0 ) ( 70 , 0.0 ) ( 72 , 0.037904902178945175 ) ( 73 , -0.037904902178945175 ) ( 74 , -0.07580980435789035 ) ( 75 , 0.0 ) ( 76 , -0.037904902178945175 ) ( 77 , -0.037904902178945175 ) ( 78 , 0.037904902178945175 ) ( 79 , 0.0 ) ( 80 , 0.0 ) ( 81 , 0.037904902178945175 ) ( 83 , 0.0 ) ( 84 , 0.07580980435789035 ) ( 85 , 0.0 ) ( 86 , 0.0 ) ( 87 , 0.0 ) ( 88 , 0.0 ) ( 89 , -0.07580980435789035 ) ( 90 , 0.0 ) ( 91 , 0.11371470653683552 ) ( 92 , 0.037904902178945175 ) ( 94 , 0.0 ) ( 95 , -0.11371470653683552 ) ( 96 , 0.07580980435789035 ) ( 97 , 0.0 ) ( 98 , 0.037904902178945175 ) ( 99 , 0.037904902178945175 ) ( 100 , -0.07580980435789035 ) ( 101 , 0.0 ) ( 102 , -0.037904902178945175 ) ( 103 , -0.037904902178945175 ) ( 105 , 0.037904902178945175 ) ( 106 , -0.037904902178945175 ) ( 107 , 0.0 ) ( 108 , 0.0 ) ( 109 , 0.037904902178945175 ) ( 110 , 0.0 ) ( 111 , 0.037904902178945175 ) ( 112 , -0.037904902178945175 ) ( 113 , -0.037904902178945175 ) ( 114 , 0.0 ) ( 116 , -0.037904902178945175 ) ( 117 , 0.0 ) ( 118 , 0.037904902178945175 ) ( 119 , 0.037904902178945175 ) ( 120 , 0.0 ) ( 121 , 0.07580980435789035 ) ( 122 , -0.037904902178945175 ) ( 123 , 0.0 ) ( 124 , 0.0 ) ( 125 , -0.037904902178945175 ) ( 127 , -0.07580980435789035 ) ( 128 , 0.07580980435789035 ) ( 129 , -0.037904902178945175 ) ( 130 , 0.0 ) ( 131 , 0.037904902178945175 ) ( 132 , 0.0 ) ( 133 , 0.037904902178945175 ) ( 134 , -0.037904902178945175 ) ( 135 , 0.07580980435789035 ) ( 136 , -0.037904902178945175 ) ( 138 , 0.0 ) ( 139 , 0.037904902178945175 ) ( 140 , -0.07580980435789035 ) ( 141 , 0.037904902178945175 ) ( 142 , -0.07580980435789035 ) ( 143 , -0.037904902178945175 ) ( 144 , 0.0 ) ( 145 , 0.037904902178945175 ) ( 146 , 0.0 ) ( 147 , 0.0 ) ( 149 , 0.037904902178945175 ) ( 150 , 0.0 ) ( 151 , 0.0 ) ( 152 , 0.037904902178945175 ) ( 153 , 0.0 ) ( 154 , -0.037904902178945175 ) ( 155 , 0.0 ) ( 156 , -0.037904902178945175 ) ( 157 , 0.0 ) ( 158 , -0.037904902178945175 ) ( 160 , 0.0 ) ( 161 , -0.037904902178945175 ) ( 162 , 0.0 ) ( 163 , -0.037904902178945175 ) ( 164 , -0.037904902178945175 ) ( 165 , 0.0 ) ( 166 , 0.0 ) ( 167 , 0.07580980435789035 ) ( 168 , 0.037904902178945175 ) ( 169 , 0.037904902178945175 ) ( 171 , 0.0 ) ( 172 , -0.07580980435789035 ) ( 173 , -0.037904902178945175 ) ( 174 , 0.0 ) ( 175 , 0.07580980435789035 ) ( 176 , -0.07580980435789035 ) ( 177 , 0.11371470653683552 ) ( 178 , 0.037904902178945175 ) ( 179 , 0.037904902178945175 ) ( 180 , 0.037904902178945175 ) ( 182 , 0.0 ) ( 183 , 0.0 ) ( 184 , 0.037904902178945175 ) ( 185 , 0.037904902178945175 ) ( 186 , -0.037904902178945175 ) ( 187 , -0.1516196087157807 ) ( 188 , 0.07580980435789035 ) ( 189 , 0.07580980435789035 ) ( 190 , 0.11371470653683552 ) ( 191 , 0.0 ) ( 193 , 0.037904902178945175 ) ( 194 , 0.037904902178945175 ) ( 195 , 0.037904902178945175 ) ( 196 , 0.0 ) ( 197 , -0.07580980435789035 ) ( 198 , -0.07580980435789035 ) ( 199 , -0.037904902178945175 ) ( 200 , 0.0 ) ( 201 , -0.07580980435789035 ) ( 202 , 0.037904902178945175 ) ( 204 , 0.0 ) ( 205 , 0.0 ) ( 206 , 0.0 ) ( 207 , 0.0 ) ( 208 , 0.0 ) ( 209 , 0.0 ) ( 210 , 0.037904902178945175 ) ( 211 , -0.037904902178945175 ) ( 212 , -0.07580980435789035 ) ( 213 , 0.037904902178945175 ) ( 215 , -0.037904902178945175 ) ( 216 , 0.0 ) ( 217 , 0.037904902178945175 ) ( 218 , 0.0 ) ( 219 , 0.037904902178945175 ) ( 220 , 0.0 ) ( 221 , -0.04343722427630693 ) ( 222 , 0.021718612138153466 ) ( 223 , 0.0 ) ( 224 , -0.06515583641446039 ) ( 226 , 0.021718612138153466 ) ( 227 , -0.08687444855261386 ) ( 228 , -0.021718612138153466 ) ( 229 , 0.0 ) ( 230 , -0.021718612138153466 ) ( 231 , -0.021718612138153466 ) ( 232 , 0.10859306069076732 ) ( 233 , -0.021718612138153466 ) ( 234 , -0.04343722427630693 ) ( 235 , 0.0 ) ( 237 , 0.04343722427630693 ) ( 238 , 0.0 ) ( 239 , -0.08687444855261386 ) ( 240 , 0.021718612138153466 ) ( 241 , 0.0 ) ( 242 , -0.021718612138153466 ) ( 243 , -0.08687444855261386 ) ( 244 , 0.04343722427630693 ) ( 245 , 0.021718612138153466 ) ( 246 , -0.021718612138153466 ) ( 248 , -0.021718612138153466 ) ( 249 , 0.04343722427630693 ) ( 250 , -0.04343722427630693 ) ( 251 , -0.06515583641446039 ) ( 252 , 0.021718612138153466 ) ( 253 , -0.021718612138153466 ) ( 254 , 0.08687444855261386 ) ( 255 , 0.0 ) ( 256 , 0.0 ) ( 257 , 0.021718612138153466 ) ( 259 , 0.0 ) ( 260 , 0.021718612138153466 ) ( 261 , -0.04343722427630693 ) ( 262 , 0.021718612138153466 ) ( 263 , -0.06515583641446039 ) ( 264 , 0.04343722427630693 ) ( 265 , 0.0 ) ( 266 , -0.04343722427630693 ) ( 267 , -0.06515583641446039 ) ( 268 , -0.06515583641446039 ) ( 270 , -0.06515583641446039 ) ( 271 , 0.04343722427630693 ) ( 272 , 0.06515583641446039 ) ( 273 , 0.0 ) ( 274 , 0.0 ) ( 275 , -0.04343722427630693 ) ( 276 , -0.021718612138153466 ) ( 277 , -0.021718612138153466 ) ( 278 , 0.08687444855261386 ) ( 279 , 0.10859306069076732 ) ( 281 , -0.021718612138153466 ) ( 282 , -0.021718612138153466 ) ( 283 , -0.06515583641446039 ) ( 284 , -0.04343722427630693 ) ( 285 , 0.021718612138153466 ) ( 286 , 0.06515583641446039 ) ( 287 , 0.0 ) ( 288 , 0.021718612138153466 ) ( 289 , 0.0 ) ( 290 , -0.06515583641446039 ) ( 292 , -0.08687444855261386 ) ( 293 , -0.08687444855261386 ) ( 294 , -0.04343722427630693 ) ( 295 , 0.021718612138153466 ) ( 296 , -0.04343722427630693 ) ( 297 , 0.10859306069076732 ) ( 298 , 0.06515583641446039 ) ( 299 , 0.06515583641446039 ) ( 300 , 0.021718612138153466 ) ( 301 , 0.0 ) ( 303 , -0.021718612138153466 ) ( 304 , 0.021718612138153466 ) ( 305 , -0.021718612138153466 ) ( 306 , -0.06515583641446039 ) ( 307 , 0.021718612138153466 ) ( 308 , -0.021718612138153466 ) ( 309 , 0.021718612138153466 ) ( 310 , -0.04343722427630693 ) ( 311 , 0.021718612138153466 ) ( 312 , 0.0 ) ( 314 , 0.021718612138153466 ) ( 315 , 0.0 ) ( 316 , -0.021718612138153466 ) ( 317 , 0.021718612138153466 ) ( 318 , 0.04343722427630693 ) ( 319 , 0.021718612138153466 ) ( 320 , -0.021718612138153466 ) ( 321 , 0.04343722427630693 ) ( 322 , -0.06515583641446039 ) ( 323 , 0.0 ) ( 325 , 0.021718612138153466 ) ( 326 , 0.021718612138153466 ) ( 327 , -0.04343722427630693 ) ( 328 , -0.021718612138153466 ) ( 329 , 0.04343722427630693 ) ( 330 , -0.021718612138153466 ) ( 331 , -0.10859306069076732 ) ( 332 , 0.04343722427630693 ) ( 333 , -0.021718612138153466 ) ( 334 , -0.04343722427630693 ) ( 336 , 0.021718612138153466 ) ( 337 , -0.06515583641446039 ) ( 338 , -0.021718612138153466 ) ( 339 , 0.0 ) ( 340 , 0.04343722427630693 ) ( 341 , 0.08687444855261386 ) ( 342 , -0.04343722427630693 ) ( 343 , -0.04343722427630693 ) ( 344 , 0.021718612138153466 ) ( 345 , 0.06515583641446039 ) ( 347 , -0.04343722427630693 ) ( 348 , 0.0 ) ( 349 , 0.0 ) ( 350 , 0.0 ) ( 351 , -0.021718612138153466 ) ( 352 , 0.04343722427630693 ) ( 353 , 0.021718612138153466 ) ( 354 , 0.08687444855261386 ) ( 355 , -0.08687444855261386 ) ( 356 , 0.06515583641446039 ) ( 358 , 0.04343722427630693 ) ( 359 , -0.08687444855261386 ) ( 360 , -0.021718612138153466 ) ( 361 , -0.06515583641446039 ) ( 362 , -0.021718612138153466 ) ( 363 , -0.08687444855261386 ) ( 364 , 0.04343722427630693 ) ( 365 , 0.021718612138153466 ) ( 366 , 0.0 ) ( 367 , 0.04343722427630693 ) ( 369 , 0.04343722427630693 ) ( 370 , 0.021718612138153466 ) ( 371 , -0.04343722427630693 ) ( 372 , -0.04343722427630693 ) ( 373 , -0.021718612138153466 ) ( 374 , 0.04343722427630693 ) ( 375 , 0.0 ) ( 376 , 0.04343722427630693 ) ( 377 , -0.021718612138153466 ) ( 378 , 0.04343722427630693 ) ( 380 , -0.06515583641446039 ) ( 381 , -0.04343722427630693 ) ( 382 , 0.06515583641446039 ) ( 383 , -0.021718612138153466 ) ( 384 , 0.08687444855261386 ) ( 385 , -0.04343722427630693 ) ( 386 , 0.04343722427630693 ) ( 387 , 0.021718612138153466 ) ( 388 , 0.0 ) ( 389 , 0.0 ) ( 391 , 0.021718612138153466 ) ( 392 , -0.021718612138153466 ) ( 393 , 0.0 ) ( 394 , 0.0 ) ( 395 , 0.04343722427630693 ) ( 396 , -0.021718612138153466 ) ( 397 , 0.021718612138153466 ) ( 398 , -0.06515583641446039 ) ( 399 , 0.04343722427630693 ) ( 400 , 0.021718612138153466 ) ( 402 , 0.0 ) ( 403 , 0.021718612138153466 ) ( 404 , 0.10859306069076732 ) ( 405 , 0.021718612138153466 ) ( 406 , 0.0 ) ( 407 , 0.08687444855261386 ) ( 408 , 0.0 ) ( 409 , 0.13031167282892078 ) ( 410 , -0.08687444855261386 ) ( 411 , 0.04343722427630693 ) ( 413 , 0.021718612138153466 ) ( 414 , -0.06515583641446039 ) ( 415 , -0.04343722427630693 ) ( 416 , 0.0 ) ( 417 , 0.0 ) ( 418 , -0.08687444855261386 ) ( 419 , 0.0 ) ( 420 , -0.04343722427630693 ) ( 421 , 0.04343722427630693 ) ( 422 , 0.0 ) ( 424 , -0.021718612138153466 ) ( 425 , 0.021718612138153466 ) ( 426 , 0.021718612138153466 ) ( 427 , -0.04343722427630693 ) ( 428 , 0.0 ) ( 429 , 0.04343722427630693 ) ( 430 , -0.08687444855261386 ) ( 431 , 0.0 ) ( 432 , 0.021718612138153466 ) ( 433 , -0.021718612138153466 ) ( 435 , 0.021718612138153466 ) ( 436 , 0.06515583641446039 ) ( 437 , -0.021718612138153466 ) ( 438 , 0.021718612138153466 ) ( 439 , -0.021718612138153466 ) \n",
      "( 192 , 0.18952451089472588 ) ( 258 , 0.2823419577959951 ) ( 324 , 0.17374889710522773 ) ( 5 , 0.18952451089472588 ) ( 390 , 0.26062334565784157 ) ( 71 , 0.2653343152526162 ) ( 137 , 0.2653343152526162 ) ( 203 , 0.18952451089472588 ) ( 269 , 0.1954675092433812 ) ( 335 , 0.26062334565784157 ) ( 16 , 0.2653343152526162 ) ( 401 , 0.23890473351968813 ) ( 82 , 0.22742941307367104 ) ( 148 , 0.22742941307367104 ) ( 214 , 0.18952451089472588 ) ( 280 , 0.23890473351968813 ) ( 346 , 0.26062334565784157 ) ( 27 , 0.11371470653683552 ) ( 412 , 0.23890473351968813 ) ( 93 , 0.18952451089472588 ) ( 159 , 0.22742941307367104 ) ( 225 , 0.21718612138153465 ) ( 291 , 0.21718612138153465 ) ( 357 , 0.30406056993414854 ) ( 38 , 0.3032392174315614 ) ( 423 , 0.23890473351968813 ) ( 104 , 0.18952451089472588 ) ( 170 , 0.22742941307367104 ) ( 236 , 0.21718612138153465 ) ( 302 , 0.23890473351968813 ) ( 368 , 0.34749779421045546 ) ( 49 , 0.18952451089472588 ) ( 434 , 0.21718612138153465 ) ( 115 , 0.22742941307367104 ) ( 181 , 0.2653343152526162 ) ( 247 , 0.17374889710522773 ) ( 313 , 0.13031167282892078 ) ( 379 , 0.15203028496707427 ) ( 60 , 0.22742941307367104 ) ( 126 , 0.22742941307367104 ) \n",
      "average signal: \t0.2250\n",
      "average noise: \t0.0342\n",
      "=============================================\n",
      "dimensionality  1000\n",
      "number of features aggregated:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0 , 0.022473328748774737 ) ( 1 , 0.0 ) ( 2 , 0.0 ) ( 3 , 0.044946657497549475 ) ( 4 , 0.0 ) ( 6 , 0.0 ) ( 7 , 0.0 ) ( 8 , -0.022473328748774737 ) ( 9 , 0.022473328748774737 ) ( 10 , 0.022473328748774737 ) ( 11 , 0.0 ) ( 12 , 0.022473328748774737 ) ( 13 , 0.044946657497549475 ) ( 14 , 0.0 ) ( 15 , 0.0 ) ( 17 , 0.0 ) ( 18 , -0.022473328748774737 ) ( 19 , 0.0 ) ( 20 , -0.022473328748774737 ) ( 21 , 0.06741998624632421 ) ( 22 , -0.044946657497549475 ) ( 23 , -0.022473328748774737 ) ( 24 , -0.022473328748774737 ) ( 25 , 0.022473328748774737 ) ( 26 , 0.022473328748774737 ) ( 28 , -0.022473328748774737 ) ( 29 , 0.022473328748774737 ) ( 30 , 0.0 ) ( 31 , 0.022473328748774737 ) ( 32 , 0.0 ) ( 33 , -0.044946657497549475 ) ( 34 , -0.044946657497549475 ) ( 35 , 0.044946657497549475 ) ( 36 , -0.044946657497549475 ) ( 37 , 0.022473328748774737 ) ( 39 , -0.022473328748774737 ) ( 40 , -0.08989331499509895 ) ( 41 , 0.022473328748774737 ) ( 42 , 0.0 ) ( 43 , -0.044946657497549475 ) ( 44 , -0.022473328748774737 ) ( 45 , 0.11236664374387369 ) ( 46 , 0.022473328748774737 ) ( 47 , 0.0 ) ( 48 , -0.044946657497549475 ) ( 50 , -0.022473328748774737 ) ( 51 , -0.022473328748774737 ) ( 52 , 0.022473328748774737 ) ( 53 , 0.022473328748774737 ) ( 54 , -0.022473328748774737 ) ( 55 , 0.044946657497549475 ) ( 56 , 0.022473328748774737 ) ( 57 , -0.044946657497549475 ) ( 58 , 0.044946657497549475 ) ( 59 , -0.022473328748774737 ) ( 61 , 0.022473328748774737 ) ( 62 , 0.0 ) ( 63 , 0.0 ) ( 64 , -0.022473328748774737 ) ( 65 , -0.022473328748774737 ) ( 66 , -0.022473328748774737 ) ( 67 , 0.022473328748774737 ) ( 68 , 0.044946657497549475 ) ( 69 , -0.022473328748774737 ) ( 70 , 0.0 ) ( 72 , -0.044946657497549475 ) ( 73 , 0.0 ) ( 74 , 0.0 ) ( 75 , -0.06741998624632421 ) ( 76 , -0.022473328748774737 ) ( 77 , 0.0 ) ( 78 , 0.0 ) ( 79 , -0.022473328748774737 ) ( 80 , -0.044946657497549475 ) ( 81 , -0.022473328748774737 ) ( 83 , 0.0 ) ( 84 , 0.044946657497549475 ) ( 85 , 0.044946657497549475 ) ( 86 , 0.0 ) ( 87 , 0.022473328748774737 ) ( 88 , 0.044946657497549475 ) ( 89 , -0.044946657497549475 ) ( 90 , 0.0 ) ( 91 , 0.0 ) ( 92 , 0.044946657497549475 ) ( 94 , 0.0 ) ( 95 , -0.022473328748774737 ) ( 96 , -0.06741998624632421 ) ( 97 , -0.022473328748774737 ) ( 98 , 0.022473328748774737 ) ( 99 , 0.022473328748774737 ) ( 100 , 0.022473328748774737 ) ( 101 , 0.0 ) ( 102 , 0.0 ) ( 103 , 0.0 ) ( 105 , 0.0 ) ( 106 , 0.0 ) ( 107 , -0.022473328748774737 ) ( 108 , 0.06741998624632421 ) ( 109 , 0.022473328748774737 ) ( 110 , 0.0 ) ( 111 , 0.022473328748774737 ) ( 112 , -0.06741998624632421 ) ( 113 , -0.022473328748774737 ) ( 114 , -0.022473328748774737 ) ( 116 , -0.022473328748774737 ) ( 117 , -0.044946657497549475 ) ( 118 , 0.022473328748774737 ) ( 119 , 0.0 ) ( 120 , 0.044946657497549475 ) ( 121 , 0.0 ) ( 122 , 0.044946657497549475 ) ( 123 , 0.022473328748774737 ) ( 124 , 0.0 ) ( 125 , 0.06741998624632421 ) ( 127 , 0.022473328748774737 ) ( 128 , -0.044946657497549475 ) ( 129 , 0.0 ) ( 130 , 0.0 ) ( 131 , 0.0 ) ( 132 , 0.0 ) ( 133 , -0.044946657497549475 ) ( 134 , 0.044946657497549475 ) ( 135 , 0.022473328748774737 ) ( 136 , 0.022473328748774737 ) ( 138 , 0.044946657497549475 ) ( 139 , 0.044946657497549475 ) ( 140 , 0.0 ) ( 141 , 0.0 ) ( 142 , 0.0 ) ( 143 , -0.022473328748774737 ) ( 144 , 0.08989331499509895 ) ( 145 , -0.022473328748774737 ) ( 146 , 0.0 ) ( 147 , 0.044946657497549475 ) ( 149 , -0.022473328748774737 ) ( 150 , -0.06741998624632421 ) ( 151 , 0.0 ) ( 152 , 0.0 ) ( 153 , -0.022473328748774737 ) ( 154 , -0.022473328748774737 ) ( 155 , 0.0 ) ( 156 , 0.0 ) ( 157 , 0.022473328748774737 ) ( 158 , 0.022473328748774737 ) ( 160 , -0.022473328748774737 ) ( 161 , 0.022473328748774737 ) ( 162 , -0.08989331499509895 ) ( 163 , 0.044946657497549475 ) ( 164 , 0.022473328748774737 ) ( 165 , -0.06741998624632421 ) ( 166 , 0.0 ) ( 167 , 0.022473328748774737 ) ( 168 , -0.022473328748774737 ) ( 169 , 0.0 ) ( 171 , -0.044946657497549475 ) ( 172 , 0.022473328748774737 ) ( 173 , 0.022473328748774737 ) ( 174 , 0.0 ) ( 175 , 0.044946657497549475 ) ( 176 , 0.0 ) ( 177 , 0.022473328748774737 ) ( 178 , 0.0 ) ( 179 , 0.044946657497549475 ) ( 180 , 0.044946657497549475 ) ( 182 , -0.044946657497549475 ) ( 183 , -0.022473328748774737 ) ( 184 , -0.022473328748774737 ) ( 185 , -0.022473328748774737 ) ( 186 , -0.08989331499509895 ) ( 187 , 0.022473328748774737 ) ( 188 , 0.06741998624632421 ) ( 189 , -0.022473328748774737 ) ( 190 , -0.044946657497549475 ) ( 191 , 0.022473328748774737 ) ( 193 , 0.044946657497549475 ) ( 194 , 0.0 ) ( 195 , 0.022473328748774737 ) ( 196 , 0.0 ) ( 197 , 0.022473328748774737 ) ( 198 , 0.022473328748774737 ) ( 199 , -0.06741998624632421 ) ( 200 , -0.022473328748774737 ) ( 201 , -0.022473328748774737 ) ( 202 , -0.022473328748774737 ) ( 204 , 0.022473328748774737 ) ( 205 , 0.022473328748774737 ) ( 206 , -0.022473328748774737 ) ( 207 , 0.0 ) ( 208 , 0.0 ) ( 209 , 0.022473328748774737 ) ( 210 , -0.022473328748774737 ) ( 211 , 0.0 ) ( 212 , 0.044946657497549475 ) ( 213 , 0.0 ) ( 215 , 0.044946657497549475 ) ( 216 , 0.0 ) ( 217 , -0.044946657497549475 ) ( 218 , -0.06741998624632421 ) ( 219 , 0.022473328748774737 ) ( 220 , 0.04529108136578383 ) ( 221 , 0.03396831102433787 ) ( 222 , 0.022645540682891915 ) ( 223 , 0.022645540682891915 ) ( 224 , -0.011322770341445958 ) ( 226 , -0.03396831102433787 ) ( 227 , 0.0 ) ( 228 , -0.011322770341445958 ) ( 229 , 0.022645540682891915 ) ( 230 , -0.022645540682891915 ) ( 231 , -0.04529108136578383 ) ( 232 , 0.03396831102433787 ) ( 233 , 0.011322770341445958 ) ( 234 , -0.06793662204867575 ) ( 235 , 0.011322770341445958 ) ( 237 , 0.022645540682891915 ) ( 238 , -0.011322770341445958 ) ( 239 , -0.022645540682891915 ) ( 240 , 0.0 ) ( 241 , -0.011322770341445958 ) ( 242 , 0.011322770341445958 ) ( 243 , 0.022645540682891915 ) ( 244 , -0.03396831102433787 ) ( 245 , 0.011322770341445958 ) ( 246 , 0.10190493307301363 ) ( 248 , 0.03396831102433787 ) ( 249 , 0.04529108136578383 ) ( 250 , -0.07925939239012171 ) ( 251 , 0.03396831102433787 ) ( 252 , 0.03396831102433787 ) ( 253 , 0.03396831102433787 ) ( 254 , 0.022645540682891915 ) ( 255 , 0.011322770341445958 ) ( 256 , 0.011322770341445958 ) ( 257 , 0.0 ) ( 259 , 0.0 ) ( 260 , -0.09058216273156766 ) ( 261 , -0.022645540682891915 ) ( 262 , -0.04529108136578383 ) ( 263 , -0.05661385170722979 ) ( 264 , 0.011322770341445958 ) ( 265 , -0.03396831102433787 ) ( 266 , 0.011322770341445958 ) ( 267 , 0.011322770341445958 ) ( 268 , -0.011322770341445958 ) ( 270 , 0.0 ) ( 271 , -0.022645540682891915 ) ( 272 , 0.011322770341445958 ) ( 273 , 0.011322770341445958 ) ( 274 , -0.05661385170722979 ) ( 275 , -0.011322770341445958 ) ( 276 , -0.011322770341445958 ) ( 277 , -0.022645540682891915 ) ( 278 , 0.03396831102433787 ) ( 279 , 0.022645540682891915 ) ( 281 , 0.011322770341445958 ) ( 282 , 0.011322770341445958 ) ( 283 , 0.022645540682891915 ) ( 284 , -0.011322770341445958 ) ( 285 , -0.022645540682891915 ) ( 286 , -0.07925939239012171 ) ( 287 , -0.04529108136578383 ) ( 288 , 0.011322770341445958 ) ( 289 , -0.022645540682891915 ) ( 290 , 0.0 ) ( 292 , 0.022645540682891915 ) ( 293 , -0.03396831102433787 ) ( 294 , -0.022645540682891915 ) ( 295 , 0.011322770341445958 ) ( 296 , 0.0 ) ( 297 , -0.011322770341445958 ) ( 298 , -0.011322770341445958 ) ( 299 , 0.011322770341445958 ) ( 300 , 0.011322770341445958 ) ( 301 , -0.022645540682891915 ) ( 303 , 0.0 ) ( 304 , -0.03396831102433787 ) ( 305 , 0.011322770341445958 ) ( 306 , 0.011322770341445958 ) ( 307 , -0.05661385170722979 ) ( 308 , 0.05661385170722979 ) ( 309 , -0.03396831102433787 ) ( 310 , -0.022645540682891915 ) ( 311 , -0.022645540682891915 ) ( 312 , -0.011322770341445958 ) ( 314 , -0.03396831102433787 ) ( 315 , 0.0 ) ( 316 , 0.07925939239012171 ) ( 317 , 0.0 ) ( 318 , 0.011322770341445958 ) ( 319 , 0.04529108136578383 ) ( 320 , -0.011322770341445958 ) ( 321 , 0.022645540682891915 ) ( 322 , -0.04529108136578383 ) ( 323 , -0.04529108136578383 ) ( 325 , 0.011322770341445958 ) ( 326 , -0.05661385170722979 ) ( 327 , -0.06793662204867575 ) ( 328 , -0.04529108136578383 ) ( 329 , 0.022645540682891915 ) ( 330 , 0.022645540682891915 ) ( 331 , -0.05661385170722979 ) ( 332 , 0.011322770341445958 ) ( 333 , 0.022645540682891915 ) ( 334 , -0.03396831102433787 ) ( 336 , 0.05661385170722979 ) ( 337 , 0.022645540682891915 ) ( 338 , 0.03396831102433787 ) ( 339 , 0.0 ) ( 340 , 0.03396831102433787 ) ( 341 , 0.0 ) ( 342 , 0.022645540682891915 ) ( 343 , -0.011322770341445958 ) ( 344 , -0.03396831102433787 ) ( 345 , 0.022645540682891915 ) ( 347 , 0.05661385170722979 ) ( 348 , -0.04529108136578383 ) ( 349 , 0.011322770341445958 ) ( 350 , -0.03396831102433787 ) ( 351 , -0.03396831102433787 ) ( 352 , 0.0 ) ( 353 , 0.04529108136578383 ) ( 354 , -0.022645540682891915 ) ( 355 , 0.011322770341445958 ) ( 356 , -0.022645540682891915 ) ( 358 , 0.011322770341445958 ) ( 359 , 0.0 ) ( 360 , 0.011322770341445958 ) ( 361 , 0.022645540682891915 ) ( 362 , 0.011322770341445958 ) ( 363 , 0.0 ) ( 364 , 0.0 ) ( 365 , 0.0 ) ( 366 , -0.011322770341445958 ) ( 367 , 0.04529108136578383 ) ( 369 , 0.03396831102433787 ) ( 370 , 0.022645540682891915 ) ( 371 , 0.03396831102433787 ) ( 372 , 0.03396831102433787 ) ( 373 , 0.011322770341445958 ) ( 374 , -0.011322770341445958 ) ( 375 , -0.022645540682891915 ) ( 376 , 0.022645540682891915 ) ( 377 , 0.0 ) ( 378 , 0.022645540682891915 ) ( 380 , 0.011322770341445958 ) ( 381 , -0.022645540682891915 ) ( 382 , -0.04529108136578383 ) ( 383 , 0.0 ) ( 384 , -0.022645540682891915 ) ( 385 , -0.05661385170722979 ) ( 386 , 0.0 ) ( 387 , 0.0 ) ( 388 , 0.022645540682891915 ) ( 389 , -0.022645540682891915 ) ( 391 , -0.03396831102433787 ) ( 392 , -0.011322770341445958 ) ( 393 , -0.011322770341445958 ) ( 394 , 0.04529108136578383 ) ( 395 , 0.022645540682891915 ) ( 396 , 0.04529108136578383 ) ( 397 , 0.011322770341445958 ) ( 398 , -0.011322770341445958 ) ( 399 , 0.0 ) ( 400 , 0.0 ) ( 402 , 0.0 ) ( 403 , -0.011322770341445958 ) ( 404 , -0.011322770341445958 ) ( 405 , 0.03396831102433787 ) ( 406 , -0.03396831102433787 ) ( 407 , 0.022645540682891915 ) ( 408 , -0.04529108136578383 ) ( 409 , 0.04529108136578383 ) ( 410 , -0.011322770341445958 ) ( 411 , -0.03396831102433787 ) ( 413 , 0.022645540682891915 ) ( 414 , 0.022645540682891915 ) ( 415 , 0.011322770341445958 ) ( 416 , 0.07925939239012171 ) ( 417 , 0.04529108136578383 ) ( 418 , -0.011322770341445958 ) ( 419 , -0.03396831102433787 ) ( 420 , 0.04529108136578383 ) ( 421 , -0.04529108136578383 ) ( 422 , -0.04529108136578383 ) ( 424 , 0.011322770341445958 ) ( 425 , -0.022645540682891915 ) ( 426 , 0.0 ) ( 427 , 0.022645540682891915 ) ( 428 , 0.0 ) ( 429 , 0.022645540682891915 ) ( 430 , -0.022645540682891915 ) ( 431 , -0.03396831102433787 ) ( 432 , 0.05661385170722979 ) ( 433 , 0.0 ) ( 435 , -0.011322770341445958 ) ( 436 , -0.03396831102433787 ) ( 437 , -0.03396831102433787 ) ( 438 , -0.03396831102433787 ) ( 439 , -0.03396831102433787 ) \n",
      "( 192 , 0.22473328748774737 ) ( 258 , 0.260423717853257 ) ( 324 , 0.20380986614602725 ) ( 5 , 0.13483997249264842 ) ( 390 , 0.24910094751181108 ) ( 71 , 0.2472066162365221 ) ( 137 , 0.26967994498529685 ) ( 203 , 0.2472066162365221 ) ( 269 , 0.20380986614602725 ) ( 335 , 0.23777817717036512 ) ( 16 , 0.2472066162365221 ) ( 401 , 0.22645540682891915 ) ( 82 , 0.20225995873897262 ) ( 148 , 0.15731330124142315 ) ( 214 , 0.20225995873897262 ) ( 280 , 0.20380986614602725 ) ( 346 , 0.21513263648747322 ) ( 27 , 0.20225995873897262 ) ( 412 , 0.24910094751181108 ) ( 93 , 0.22473328748774737 ) ( 159 , 0.2472066162365221 ) ( 225 , 0.20380986614602725 ) ( 291 , 0.22645540682891915 ) ( 357 , 0.21513263648747322 ) ( 38 , 0.22473328748774737 ) ( 423 , 0.1924870958045813 ) ( 104 , 0.2472066162365221 ) ( 170 , 0.2472066162365221 ) ( 236 , 0.22645540682891915 ) ( 302 , 0.1924870958045813 ) ( 368 , 0.21513263648747322 ) ( 49 , 0.22473328748774737 ) ( 434 , 0.22645540682891915 ) ( 115 , 0.20225995873897262 ) ( 181 , 0.2472066162365221 ) ( 247 , 0.23777817717036512 ) ( 313 , 0.23777817717036512 ) ( 379 , 0.1924870958045813 ) ( 60 , 0.20225995873897262 ) ( 126 , 0.2472066162365221 ) \n",
      "average signal: \t0.2216\n",
      "average noise: \t0.0247\n",
      "=============================================\n",
      "dimensionality  2000\n",
      "number of features aggregated:  20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0 , -0.02224970797449924 ) ( 1 , 0.01112485398724962 ) ( 2 , 0.01112485398724962 ) ( 3 , -0.01112485398724962 ) ( 4 , 0.01112485398724962 ) ( 6 , 0.02224970797449924 ) ( 7 , 0.02224970797449924 ) ( 8 , 0.01112485398724962 ) ( 9 , 0.02224970797449924 ) ( 10 , 0.02224970797449924 ) ( 11 , 0.02224970797449924 ) ( 12 , 0.03337456196174886 ) ( 13 , 0.0 ) ( 14 , -0.01112485398724962 ) ( 15 , 0.055624269936248104 ) ( 17 , 0.01112485398724962 ) ( 18 , 0.06674912392349772 ) ( 19 , 0.0 ) ( 20 , 0.01112485398724962 ) ( 21 , -0.01112485398724962 ) ( 22 , 0.01112485398724962 ) ( 23 , -0.03337456196174886 ) ( 24 , 0.03337456196174886 ) ( 25 , 0.0 ) ( 26 , 0.02224970797449924 ) ( 28 , 0.04449941594899848 ) ( 29 , -0.02224970797449924 ) ( 30 , 0.01112485398724962 ) ( 31 , 0.01112485398724962 ) ( 32 , 0.03337456196174886 ) ( 33 , -0.02224970797449924 ) ( 34 , 0.055624269936248104 ) ( 35 , -0.04449941594899848 ) ( 36 , -0.04449941594899848 ) ( 37 , 0.01112485398724962 ) ( 39 , -0.03337456196174886 ) ( 40 , 0.01112485398724962 ) ( 41 , -0.02224970797449924 ) ( 42 , 0.01112485398724962 ) ( 43 , 0.03337456196174886 ) ( 44 , 0.0 ) ( 45 , -0.01112485398724962 ) ( 46 , -0.01112485398724962 ) ( 47 , 0.0 ) ( 48 , 0.0 ) ( 50 , 0.055624269936248104 ) ( 51 , 0.02224970797449924 ) ( 52 , 0.03337456196174886 ) ( 53 , 0.02224970797449924 ) ( 54 , 0.02224970797449924 ) ( 55 , 0.01112485398724962 ) ( 56 , 0.03337456196174886 ) ( 57 , 0.02224970797449924 ) ( 58 , 0.01112485398724962 ) ( 59 , -0.02224970797449924 ) ( 61 , 0.0 ) ( 62 , -0.03337456196174886 ) ( 63 , 0.03337456196174886 ) ( 64 , 0.0 ) ( 65 , 0.01112485398724962 ) ( 66 , -0.03337456196174886 ) ( 67 , 0.01112485398724962 ) ( 68 , -0.02224970797449924 ) ( 69 , 0.01112485398724962 ) ( 70 , -0.01112485398724962 ) ( 72 , -0.01112485398724962 ) ( 73 , 0.0 ) ( 74 , 0.02224970797449924 ) ( 75 , 0.01112485398724962 ) ( 76 , 0.01112485398724962 ) ( 77 , 0.01112485398724962 ) ( 78 , 0.01112485398724962 ) ( 79 , -0.04449941594899848 ) ( 80 , 0.04449941594899848 ) ( 81 , -0.01112485398724962 ) ( 83 , 0.01112485398724962 ) ( 84 , 0.03337456196174886 ) ( 85 , 0.01112485398724962 ) ( 86 , -0.01112485398724962 ) ( 87 , 0.0 ) ( 88 , -0.01112485398724962 ) ( 89 , 0.01112485398724962 ) ( 90 , 0.055624269936248104 ) ( 91 , -0.01112485398724962 ) ( 92 , 0.01112485398724962 ) ( 94 , 0.01112485398724962 ) ( 95 , -0.02224970797449924 ) ( 96 , 0.03337456196174886 ) ( 97 , -0.01112485398724962 ) ( 98 , 0.04449941594899848 ) ( 99 , -0.03337456196174886 ) ( 100 , 0.0 ) ( 101 , 0.0 ) ( 102 , 0.01112485398724962 ) ( 103 , -0.01112485398724962 ) ( 105 , 0.02224970797449924 ) ( 106 , -0.03337456196174886 ) ( 107 , 0.04449941594899848 ) ( 108 , -0.02224970797449924 ) ( 109 , 0.01112485398724962 ) ( 110 , -0.02224970797449924 ) ( 111 , 0.04449941594899848 ) ( 112 , 0.055624269936248104 ) ( 113 , 0.01112485398724962 ) ( 114 , 0.02224970797449924 ) ( 116 , -0.03337456196174886 ) ( 117 , 0.01112485398724962 ) ( 118 , -0.01112485398724962 ) ( 119 , -0.02224970797449924 ) ( 120 , 0.01112485398724962 ) ( 121 , -0.01112485398724962 ) ( 122 , -0.01112485398724962 ) ( 123 , 0.01112485398724962 ) ( 124 , 0.0 ) ( 125 , 0.03337456196174886 ) ( 127 , -0.01112485398724962 ) ( 128 , -0.03337456196174886 ) ( 129 , 0.0 ) ( 130 , -0.01112485398724962 ) ( 131 , -0.02224970797449924 ) ( 132 , 0.0 ) ( 133 , 0.04449941594899848 ) ( 134 , -0.01112485398724962 ) ( 135 , 0.0 ) ( 136 , 0.01112485398724962 ) ( 138 , 0.0 ) ( 139 , 0.01112485398724962 ) ( 140 , 0.03337456196174886 ) ( 141 , 0.0 ) ( 142 , 0.01112485398724962 ) ( 143 , 0.01112485398724962 ) ( 144 , -0.02224970797449924 ) ( 145 , -0.02224970797449924 ) ( 146 , 0.0 ) ( 147 , -0.01112485398724962 ) ( 149 , 0.0 ) ( 150 , -0.01112485398724962 ) ( 151 , 0.0 ) ( 152 , 0.01112485398724962 ) ( 153 , -0.01112485398724962 ) ( 154 , 0.0 ) ( 155 , -0.01112485398724962 ) ( 156 , 0.02224970797449924 ) ( 157 , 0.02224970797449924 ) ( 158 , 0.04449941594899848 ) ( 160 , 0.01112485398724962 ) ( 161 , 0.02224970797449924 ) ( 162 , 0.02224970797449924 ) ( 163 , 0.03337456196174886 ) ( 164 , 0.0 ) ( 165 , 0.02224970797449924 ) ( 166 , 0.02224970797449924 ) ( 167 , 0.02224970797449924 ) ( 168 , 0.0 ) ( 169 , 0.0 ) ( 171 , 0.02224970797449924 ) ( 172 , -0.055624269936248104 ) ( 173 , -0.01112485398724962 ) ( 174 , 0.0 ) ( 175 , -0.01112485398724962 ) ( 176 , -0.01112485398724962 ) ( 177 , 0.01112485398724962 ) ( 178 , -0.02224970797449924 ) ( 179 , -0.01112485398724962 ) ( 180 , -0.02224970797449924 ) ( 182 , 0.01112485398724962 ) ( 183 , 0.01112485398724962 ) ( 184 , 0.02224970797449924 ) ( 185 , 0.0 ) ( 186 , 0.01112485398724962 ) ( 187 , 0.0 ) ( 188 , 0.01112485398724962 ) ( 189 , 0.01112485398724962 ) ( 190 , -0.02224970797449924 ) ( 191 , -0.04449941594899848 ) ( 193 , -0.01112485398724962 ) ( 194 , 0.0 ) ( 195 , 0.01112485398724962 ) ( 196 , 0.0 ) ( 197 , -0.01112485398724962 ) ( 198 , 0.01112485398724962 ) ( 199 , -0.06674912392349772 ) ( 200 , -0.04449941594899848 ) ( 201 , 0.03337456196174886 ) ( 202 , 0.04449941594899848 ) ( 204 , 0.01112485398724962 ) ( 205 , 0.02224970797449924 ) ( 206 , 0.01112485398724962 ) ( 207 , -0.01112485398724962 ) ( 208 , -0.02224970797449924 ) ( 209 , 0.0 ) ( 210 , -0.02224970797449924 ) ( 211 , -0.01112485398724962 ) ( 212 , 0.03337456196174886 ) ( 213 , 0.0 ) ( 215 , -0.03337456196174886 ) ( 216 , 0.01112485398724962 ) ( 217 , 0.0 ) ( 218 , 0.01112485398724962 ) ( 219 , -0.01112485398724962 ) ( 220 , 0.005423261445466404 ) ( 221 , -0.016269784336399213 ) ( 222 , 0.005423261445466404 ) ( 223 , 0.005423261445466404 ) ( 224 , 0.03796283011826483 ) ( 226 , 0.048809353009197635 ) ( 227 , 0.010846522890932807 ) ( 228 , -0.010846522890932807 ) ( 229 , -0.021693045781865615 ) ( 230 , 0.005423261445466404 ) ( 231 , 0.02711630722733202 ) ( 232 , 0.02711630722733202 ) ( 233 , -0.016269784336399213 ) ( 234 , -0.005423261445466404 ) ( 235 , 0.016269784336399213 ) ( 237 , 0.0 ) ( 238 , -0.016269784336399213 ) ( 239 , 0.016269784336399213 ) ( 240 , 0.016269784336399213 ) ( 241 , -0.048809353009197635 ) ( 242 , 0.0 ) ( 243 , -0.021693045781865615 ) ( 244 , -0.010846522890932807 ) ( 245 , 0.02711630722733202 ) ( 246 , 0.02711630722733202 ) ( 248 , -0.016269784336399213 ) ( 249 , -0.05423261445466404 ) ( 250 , 0.0 ) ( 251 , 0.021693045781865615 ) ( 252 , -0.032539568672798426 ) ( 253 , 0.0 ) ( 254 , -0.005423261445466404 ) ( 255 , 0.010846522890932807 ) ( 256 , -0.032539568672798426 ) ( 257 , -0.04338609156373123 ) ( 259 , 0.010846522890932807 ) ( 260 , -0.02711630722733202 ) ( 261 , -0.016269784336399213 ) ( 262 , -0.016269784336399213 ) ( 263 , 0.048809353009197635 ) ( 264 , -0.010846522890932807 ) ( 265 , -0.010846522890932807 ) ( 266 , -0.021693045781865615 ) ( 267 , 0.02711630722733202 ) ( 268 , 0.0 ) ( 270 , 0.016269784336399213 ) ( 271 , -0.02711630722733202 ) ( 272 , -0.005423261445466404 ) ( 273 , 0.0 ) ( 274 , 0.016269784336399213 ) ( 275 , -0.010846522890932807 ) ( 276 , 0.0 ) ( 277 , -0.07050239879106325 ) ( 278 , 0.005423261445466404 ) ( 279 , -0.03796283011826483 ) ( 281 , 0.010846522890932807 ) ( 282 , 0.010846522890932807 ) ( 283 , -0.021693045781865615 ) ( 284 , 0.06507913734559685 ) ( 285 , -0.02711630722733202 ) ( 286 , 0.010846522890932807 ) ( 287 , 0.02711630722733202 ) ( 288 , 0.021693045781865615 ) ( 289 , -0.010846522890932807 ) ( 290 , 0.0 ) ( 292 , 0.010846522890932807 ) ( 293 , 0.005423261445466404 ) ( 294 , -0.010846522890932807 ) ( 295 , 0.0 ) ( 296 , 0.016269784336399213 ) ( 297 , -0.02711630722733202 ) ( 298 , 0.016269784336399213 ) ( 299 , 0.016269784336399213 ) ( 300 , -0.016269784336399213 ) ( 301 , -0.005423261445466404 ) ( 303 , 0.032539568672798426 ) ( 304 , -0.010846522890932807 ) ( 305 , -0.02711630722733202 ) ( 306 , -0.016269784336399213 ) ( 307 , -0.021693045781865615 ) ( 308 , 0.010846522890932807 ) ( 309 , 0.032539568672798426 ) ( 310 , -0.005423261445466404 ) ( 311 , 0.005423261445466404 ) ( 312 , -0.005423261445466404 ) ( 314 , -0.005423261445466404 ) ( 315 , 0.0 ) ( 316 , 0.016269784336399213 ) ( 317 , 0.021693045781865615 ) ( 318 , 0.032539568672798426 ) ( 319 , 0.010846522890932807 ) ( 320 , -0.010846522890932807 ) ( 321 , -0.02711630722733202 ) ( 322 , 0.005423261445466404 ) ( 323 , 0.016269784336399213 ) ( 325 , 0.016269784336399213 ) ( 326 , -0.016269784336399213 ) ( 327 , -0.03796283011826483 ) ( 328 , -0.016269784336399213 ) ( 329 , 0.005423261445466404 ) ( 330 , -0.005423261445466404 ) ( 331 , -0.010846522890932807 ) ( 332 , 0.005423261445466404 ) ( 333 , -0.016269784336399213 ) ( 334 , -0.032539568672798426 ) ( 336 , -0.010846522890932807 ) ( 337 , -0.02711630722733202 ) ( 338 , -0.005423261445466404 ) ( 339 , -0.005423261445466404 ) ( 340 , 0.032539568672798426 ) ( 341 , -0.02711630722733202 ) ( 342 , -0.010846522890932807 ) ( 343 , 0.0 ) ( 344 , 0.021693045781865615 ) ( 345 , -0.021693045781865615 ) ( 347 , -0.005423261445466404 ) ( 348 , -0.032539568672798426 ) ( 349 , -0.005423261445466404 ) ( 350 , 0.010846522890932807 ) ( 351 , -0.016269784336399213 ) ( 352 , 0.0 ) ( 353 , 0.021693045781865615 ) ( 354 , 0.05423261445466404 ) ( 355 , 0.016269784336399213 ) ( 356 , 0.0 ) ( 358 , -0.05423261445466404 ) ( 359 , -0.02711630722733202 ) ( 360 , 0.016269784336399213 ) ( 361 , 0.016269784336399213 ) ( 362 , 0.03796283011826483 ) ( 363 , 0.016269784336399213 ) ( 364 , 0.010846522890932807 ) ( 365 , 0.032539568672798426 ) ( 366 , 0.02711630722733202 ) ( 367 , 0.010846522890932807 ) ( 369 , -0.04338609156373123 ) ( 370 , 0.02711630722733202 ) ( 371 , -0.005423261445466404 ) ( 372 , 0.021693045781865615 ) ( 373 , -0.02711630722733202 ) ( 374 , 0.02711630722733202 ) ( 375 , -0.016269784336399213 ) ( 376 , -0.032539568672798426 ) ( 377 , 0.021693045781865615 ) ( 378 , 0.016269784336399213 ) ( 380 , 0.010846522890932807 ) ( 381 , -0.005423261445466404 ) ( 382 , -0.010846522890932807 ) ( 383 , -0.048809353009197635 ) ( 384 , 0.0 ) ( 385 , 0.032539568672798426 ) ( 386 , -0.005423261445466404 ) ( 387 , 0.021693045781865615 ) ( 388 , 0.021693045781865615 ) ( 389 , -0.005423261445466404 ) ( 391 , -0.016269784336399213 ) ( 392 , -0.02711630722733202 ) ( 393 , -0.021693045781865615 ) ( 394 , 0.0 ) ( 395 , -0.010846522890932807 ) ( 396 , -0.021693045781865615 ) ( 397 , 0.032539568672798426 ) ( 398 , -0.05423261445466404 ) ( 399 , 0.032539568672798426 ) ( 400 , 0.0 ) ( 402 , -0.016269784336399213 ) ( 403 , 0.04338609156373123 ) ( 404 , 0.005423261445466404 ) ( 405 , -0.010846522890932807 ) ( 406 , 0.010846522890932807 ) ( 407 , 0.005423261445466404 ) ( 408 , 0.04338609156373123 ) ( 409 , -0.03796283011826483 ) ( 410 , 0.0 ) ( 411 , -0.048809353009197635 ) ( 413 , 0.010846522890932807 ) ( 414 , 0.005423261445466404 ) ( 415 , -0.021693045781865615 ) ( 416 , 0.005423261445466404 ) ( 417 , 0.010846522890932807 ) ( 418 , 0.010846522890932807 ) ( 419 , -0.03796283011826483 ) ( 420 , 0.016269784336399213 ) ( 421 , -0.010846522890932807 ) ( 422 , 0.005423261445466404 ) ( 424 , 0.010846522890932807 ) ( 425 , -0.05423261445466404 ) ( 426 , 0.03796283011826483 ) ( 427 , -0.021693045781865615 ) ( 428 , -0.010846522890932807 ) ( 429 , 0.0 ) ( 430 , 0.059655875900130446 ) ( 431 , -0.059655875900130446 ) ( 432 , 0.04338609156373123 ) ( 433 , -0.02711630722733202 ) ( 435 , 0.03796283011826483 ) ( 436 , 0.010846522890932807 ) ( 437 , 0.021693045781865615 ) ( 438 , 0.032539568672798426 ) ( 439 , 0.032539568672798426 ) \n",
      "( 192 , 0.23362193373224202 ) ( 258 , 0.21150719637318977 ) ( 324 , 0.23320024215505536 ) ( 5 , 0.21137222575774278 ) ( 390 , 0.21150719637318977 ) ( 71 , 0.23362193373224202 ) ( 137 , 0.23362193373224202 ) ( 203 , 0.26699649569399087 ) ( 269 , 0.24947002649145458 ) ( 335 , 0.18981415059132414 ) ( 16 , 0.23362193373224202 ) ( 401 , 0.23320024215505536 ) ( 82 , 0.22249707974499242 ) ( 148 , 0.22249707974499242 ) ( 214 , 0.18912251778324354 ) ( 280 , 0.20608393492772334 ) ( 346 , 0.23862350360052179 ) ( 27 , 0.23362193373224202 ) ( 412 , 0.23320024215505536 ) ( 93 , 0.20024737177049315 ) ( 159 , 0.1779976637959939 ) ( 225 , 0.23320024215505536 ) ( 291 , 0.2711630722733202 ) ( 357 , 0.22777698070958896 ) ( 38 , 0.1779976637959939 ) ( 423 , 0.21150719637318977 ) ( 104 , 0.23362193373224202 ) ( 170 , 0.22249707974499242 ) ( 236 , 0.2603165493823874 ) ( 302 , 0.21693045781865616 ) ( 368 , 0.21693045781865616 ) ( 49 , 0.22249707974499242 ) ( 434 , 0.23320024215505536 ) ( 115 , 0.24474678771949165 ) ( 181 , 0.23362193373224202 ) ( 247 , 0.24404676504598818 ) ( 313 , 0.2603165493823874 ) ( 379 , 0.22777698070958896 ) ( 60 , 0.23362193373224202 ) ( 126 , 0.26699649569399087 ) \n",
      "average signal: \t0.2276\n",
      "average noise: \t0.0187\n"
     ]
    }
   ],
   "source": [
    "genplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
