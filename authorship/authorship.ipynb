{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import corenlp\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import codecs\n",
    "\n",
    "import inspect  # for logger\n",
    "\n",
    "dimensionality = 2000\n",
    "denseness = 10 // dimensionality\n",
    "indexspace = {}\n",
    "globalfrequency = {}\n",
    "bign = 0\n",
    "\n",
    "parser_client = corenlp.CoreNLPClient(\n",
    "    annotators=\"tokenize ssplit pos natlog lemma depparse\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(msg, level=False, endspace=\"\\n\"):\n",
    "    if level:\n",
    "        print(time.ctime(),\" \",inspect.stack()[1][3],\"(): \", msg, sep=\"\", end=endspace)\n",
    "        \n",
    "\n",
    "def sparseadd(onevec, othvec, weight=1, normalised=False):\n",
    "    if normalised:\n",
    "        onevec = normalise(onevec)\n",
    "        othvec = normalise(othvec)\n",
    "    result = {}\n",
    "    try:\n",
    "        for l in onevec:\n",
    "            result[l] = onevec[l]\n",
    "        for k in othvec:\n",
    "            if k in result:\n",
    "                result[k] = result[k] + othvec[k] * float(weight)\n",
    "            else:\n",
    "                result[k] = othvec[k] * float(weight)\n",
    "    except KeyError:\n",
    "        print(\"sparseadd(): error\")\n",
    "        raise\n",
    "    return result\n",
    "\n",
    "\n",
    "def sparsemultiply(onevec, othvec, weight=1):\n",
    "    result = {}\n",
    "    try:\n",
    "        for l in onevec:\n",
    "            if l in othvec:\n",
    "                result[l] = onevec[l] * othvec[l] * float(weight)\n",
    "    except KeyError:\n",
    "        print(\"sparsemultiply(): error \")\n",
    "    return result\n",
    "\n",
    "\n",
    "def sparsexor(onevec, othvec):\n",
    "    result = {}\n",
    "    try:\n",
    "        for l in range(len(onevec)):\n",
    "            if ((l in onevec) and not (l in othvec)):\n",
    "                result[l] = 1\n",
    "            if (not (l in onevec) and (l in othvec)):\n",
    "                result[l] = 1\n",
    "    except KeyError:\n",
    "        print(\"sparsexor(): error \")\n",
    "    return result\n",
    "\n",
    "\n",
    "def newrandomvector(n, denseness):\n",
    "    vec = {}\n",
    "    k = int(n * denseness)\n",
    "    if k % 2 != 0:\n",
    "        k += 1\n",
    "    if (k > 0):  # no need to be careful about this, right? and k % 2 == 0):\n",
    "        nonzeros = random.sample(list(range(n)), k)\n",
    "        negatives = random.sample(nonzeros, k // 2)\n",
    "        for i in nonzeros:\n",
    "            vec[str(i)] = 1\n",
    "        for i in negatives:\n",
    "            vec[str(i)] = -1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def newoperator(n, k=0.1):\n",
    "    return newrandomvector(n, k)\n",
    "\n",
    "\n",
    "def sparsecosine(xvec, yvec, rounding=True, decimals=4):\n",
    "    x2 = 0\n",
    "    y2 = 0\n",
    "    xy = 0\n",
    "    try:\n",
    "        for i in xvec:\n",
    "            x2 += xvec[i] * xvec[i]\n",
    "    except KeyError:\n",
    "        print(\"sparsecosine(): error at position \", i)\n",
    "    try:\n",
    "        for j in yvec:\n",
    "            y2 += yvec[j] * yvec[j]\n",
    "            if j in xvec:\n",
    "                xy += xvec[j] * yvec[j]\n",
    "    except KeyError:\n",
    "        print(\"sparsecosine(): errors at position \", j)\n",
    "    if (x2 * y2 == 0):\n",
    "        cos = 0\n",
    "    else:\n",
    "        cos = xy / (math.sqrt(x2) * math.sqrt(y2))\n",
    "    if (rounding):\n",
    "        cos = round(cos, decimals)\n",
    "    return cos\n",
    "\n",
    "\n",
    "def sparselength(vec, rounding=True):\n",
    "    x2 = 0\n",
    "    length = 0\n",
    "    try:\n",
    "        for i in vec:\n",
    "            x2 += vec[i] * vec[i]\n",
    "    except KeyError:\n",
    "        print(\"sparselength(): error at position \", i)\n",
    "    if (x2 > 0):\n",
    "        length = math.sqrt(x2)\n",
    "    if (rounding):\n",
    "        length = round(length, 4)\n",
    "    return length\n",
    "\n",
    "\n",
    "def comb(vec, k=0.1, dim=dimensionality):\n",
    "    newvector = {}\n",
    "    n = int(k * dim / 2)\n",
    "    sorted_items = sorted(vec.items(),  key=lambda x: x[1])\n",
    "    bot = sorted_items[:n]\n",
    "    top = sorted_items[-n:]\n",
    "    for l in bot:\n",
    "        newvector[l[0]] = l[1]\n",
    "    for l in top:\n",
    "        newvector[l[0]] = l[1]\n",
    "    return newvector\n",
    "\n",
    "\n",
    "def sparsesum(vec):\n",
    "    s = 0\n",
    "    for i in vec:\n",
    "        s += float(vec[i])\n",
    "    return s\n",
    "\n",
    "\n",
    "def normalise(vec):\n",
    "    newvector = {}\n",
    "    vlen = sparselength(vec, False)\n",
    "    if (vlen > 0):\n",
    "        for i in vec:\n",
    "            newvector[i] = vec[i] / math.sqrt(vlen * vlen)\n",
    "    else:\n",
    "        newvector = vec\n",
    "    return newvector\n",
    "\n",
    "\n",
    "def modify(vec, factor):\n",
    "    newvector = {}\n",
    "    for i in vec:\n",
    "        if (random.random() > factor):\n",
    "            newvector[i] = vec[i]\n",
    "        else:\n",
    "            newvector[i] = float(vec[i]) * (0.5 - random.random()) * 2.0\n",
    "    return newvector\n",
    "\n",
    "\n",
    "def createpermutation(k):\n",
    "    permutation = random.sample(range(k),  k)\n",
    "    return permutation\n",
    "\n",
    "\n",
    "def permute(vector, permutation):\n",
    "    newvector = {}\n",
    "    try:\n",
    "        for i in range(len(permutation)):\n",
    "            if str(i) in vector:\n",
    "                newvector[str(permutation[i])] = vector[str(i)]\n",
    "    except KeyError:\n",
    "        newvector = vector\n",
    "        print(\"permute(): no permutation done, something wrong\")\n",
    "    return newvector\n",
    "\n",
    "\n",
    "def vectorsaturation(vector):\n",
    "    d = 0\n",
    "    for c in vector:\n",
    "        d += 1\n",
    "    return d\n",
    "\n",
    "\n",
    "def frequencyweight(word):\n",
    "    try:\n",
    "        w = math.exp(-300 * math.pi * globalfrequency[word] / bign)\n",
    "    except KeyError:\n",
    "        w = 0.5\n",
    "    return w\n",
    "\n",
    "\n",
    "def chkwordspace(words, debug=False):\n",
    "    global globalfrequency\n",
    "    global indexspace\n",
    "    global bign\n",
    "    for w in words:\n",
    "        bign += 1\n",
    "        if w in indexspace:\n",
    "            globalfrequency[w] += 1\n",
    "        else:\n",
    "            indexspace[w] = newrandomvector(dimensionality, denseness)\n",
    "            logger(str(w) + \" is new and now hallucinated.\", debug)\n",
    "        globalfrequency[w] = 1\n",
    "\n",
    "\n",
    "def semanticdepparse(string, debug=False, verbose=False):\n",
    "    depgraph = parser_client.annotate(string)\n",
    "    utterances = []\n",
    "    for ss in depgraph.sentence:\n",
    "        for w in ss.token:\n",
    "            if w.lemma not in indexspace:\n",
    "                chkwordspace([w.lemma])\n",
    "        utterances.append(depparseprocess(string, ss, debug))\n",
    "    return utterances\n",
    "\n",
    "\n",
    "def depparseprocess(string, ss, debug=False):\n",
    "    negated = False\n",
    "    target = \"epsilon\"\n",
    "    adverbial = \"epsilon\"\n",
    "    subject = \"epsilon\"\n",
    "    verb = \"epsilon\"\n",
    "    qu = \"epsilon\"\n",
    "    scratch = {}\n",
    "    question = {}\n",
    "    logger(\"root: \"+str(ss.basicDependencies.root), debug)\n",
    "    i = 0\n",
    "    for w in ss.token:\n",
    "        logger(str(i)+\" \"+w.lemma+\" \"+w.pos, debug)\n",
    "        i += 1\n",
    "    for e in ss.basicDependencies.edge:\n",
    "        logger(str(e.source) + ss.token[e.source - 1].lemma + \"-\" + e.dep + \"->\" +\n",
    "               str(e.target) + ss.token[e.target - 1].lemma, debug)\n",
    "    sentenceitems = {}\n",
    "    sentenceitems[\"epsilon\"] = None\n",
    "    sentencepos = {}\n",
    "    root = ss.basicDependencies.root[0]  # only one root for now fix this!\n",
    "    qu = root\n",
    "    target = root\n",
    "    verb = root\n",
    "    i = 1\n",
    "    for w in ss.token:\n",
    "        sentenceitems[i] = w.lemma\n",
    "        sentencepos[i] = w.pos\n",
    "        scratch[i] = False\n",
    "        if w.pos == \"WP\":\n",
    "            qu = i\n",
    "        if w.pos == \"WRB\":\n",
    "            qu = i\n",
    "        i += 1\n",
    "    tense = \"PRESENT\"\n",
    "    if sentencepos[root] == \"VBD\":\n",
    "        tense = \"PAST\"\n",
    "    if sentencepos[root] == \"VBN\":\n",
    "        tense = \"PAST\"\n",
    "\n",
    "    for edge in ss.basicDependencies.edge:\n",
    "        logger(str(edge.source) + \" \" + sentenceitems[edge.source] +\n",
    "               \" \" + \"-\" + \" \" + edge.dep + \" \" + \"->\" + \" \" +\n",
    "               str(edge.target) + \" \" + sentenceitems[edge.target], debug)\n",
    "        if edge.dep == 'nsubj':\n",
    "            subject = edge.target\n",
    "        elif edge.dep == 'neg':\n",
    "            negated = True\n",
    "        elif edge.dep == 'advmod':\n",
    "            if edge.target == qu:\n",
    "                if edge.source == root:\n",
    "                    target = \"epsilon\"\n",
    "                else:\n",
    "                    target = edge.source\n",
    "            else:\n",
    "                adverbial = edge.target\n",
    "        elif edge.dep == 'cop':\n",
    "            if edge.target == qu:\n",
    "                target = edge.source\n",
    "            else:\n",
    "                adverbial = edge.target\n",
    "        elif edge.dep == 'aux':\n",
    "            if (sentenceitems[edge.target] == \"have\"):\n",
    "                scratch['aux'] = \"have\"\n",
    "            if (sentenceitems[edge.target] == \"do\"):\n",
    "                scratch['aux'] = \"do\"\n",
    "            if (sentencepos[edge.target] == \"VBD\"):\n",
    "                tense = \"PAST\"\n",
    "            if (sentenceitems[edge.target] == \"will\"):\n",
    "                scratch['aux'] = \"will\"\n",
    "            if (sentenceitems[edge.target] == \"shall\"):\n",
    "                scratch['aux'] = \"shall\"\n",
    "    if target == \"epsilon\":\n",
    "        if subject != \"epsilon\":\n",
    "            target = subject\n",
    "    try:\n",
    "        logger(sentenceitems[root] + \" \" + sentencepos[root], debug)\n",
    "        if sentencepos[root] == \"VB\":\n",
    "            if 'aux' in scratch:\n",
    "                if (scratch['aux'] == \"will\" or scratch['aux'] == \"shall\"):\n",
    "                    tense = \"FUTURE\"\n",
    "    except KeyError:\n",
    "        logger(\"tense situation in \" + string, True)\n",
    "    question[\"question\"] = sentenceitems[qu]\n",
    "    question[\"target\"] = sentenceitems[target]\n",
    "    question[\"verb\"] = sentenceitems[verb]\n",
    "    question[\"adverbial\"] = sentenceitems[adverbial]\n",
    "    question[\"subject\"] = sentenceitems[subject]\n",
    "    question[\"tense\"] = tense\n",
    "    question[\"negated\"] = negated\n",
    "#    logger(question[\"question\"] + \" \" + question[\"target\"] + \" \" +\n",
    "#           question[\"verb\"] + \" \" + question[\"adverbial\"] + \" \" +\n",
    "# question[\"subject\"] + \" \" + question[\"tense\"] + \" \" +\n",
    "# question[\"negated\"] + \" \" + sep=\"\\t\",debug)\n",
    "    return question\n",
    "def evaluateConfusionMatrix(confusionmatrix):\n",
    "    for gold in sorted(confusionmatrix):\n",
    "        print(\"---\")\n",
    "        carat = 0\n",
    "        maximum = 0\n",
    "        hitn = 0\n",
    "        sortedglitter = sorted(\n",
    "                                confusionmatrix[gold].items(),\n",
    "                                key=lambda glitter: glitter[1],\n",
    "                                reverse=True)\n",
    "        for glitter in sortedglitter:\n",
    "            hit = \"\"\n",
    "            carat += glitter[1]\n",
    "            if glitter[0] == gold:\n",
    "                hit = \"***\"\n",
    "                hitn = glitter[1]\n",
    "            print(gold,glitter[0],glitter[1],hit,sep=\"\\t\")\n",
    "        print(gold,\"sum\",hitn,carat,hitn / carat,sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "def addconfusion(facit, predicted):\n",
    "    global confusionmatrix\n",
    "    if facit in confusionmatrix:\n",
    "        if predicted in confusionmatrix[facit]:\n",
    "            confusionmatrix[facit][predicted] += 1\n",
    "        else:\n",
    "            confusionmatrix[facit][predicted] = 1\n",
    "    else:\n",
    "        confusionmatrix[facit] = {}\n",
    "        confusionmatrix[facit][predicted] = 1\n",
    "\n",
    "\n",
    "def weightfunction(word):\n",
    "    if word in globalfrequency:\n",
    "        return globalfrequency[word]\n",
    "    elif word == \"be\":\n",
    "        return 0.1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def readthefile(trainfile, category, debug=False, train=True, lexical=True, roles=True):\n",
    "    global textspace\n",
    "    global utterancespace\n",
    "    global idx\n",
    "    ticker = 0\n",
    "    batch = 100\n",
    "    textvector = {}\n",
    "    with codecs.open(trainfile,\"r\", encoding='utf-8') as infile:\n",
    "        try:\n",
    "            rawtext = infile.read().lower()\n",
    "        except UnicodeDecodeError:\n",
    "            rawtext = \"\"\n",
    "            logger(\"File Read Problem: \"+trainfile,error)\n",
    "        rawtext = re.sub('\\n', ' ', rawtext)\n",
    "        rawtext = re.sub('\\\"', ' ', rawtext)\n",
    "        rawtext = re.sub('\\s+', ' ', rawtext)\n",
    "        sents = sent_tokenize(rawtext)\n",
    "        for sentence in sents:\n",
    "            idx += 1\n",
    "            ticker += 1\n",
    "            if ticker >= batch and monitor:\n",
    "                print(\".\",end=\"\")\n",
    "                ticker = 0\n",
    "            logger(sentence, debug)\n",
    "            wds=nltk.word_tokenize(sentence)\n",
    "            chkwordspace(wds, debug)\n",
    "            analyses = []\n",
    "            try:\n",
    "                analyses = semanticdepparse(sentence.lower(), debug)\n",
    "            except:\n",
    "                logger(\"PARSE ERROR \"+str(idx)+\"\\t\"+sentence,error)\n",
    "            kk = 0\n",
    "            for analysis in analyses:\n",
    "                wds = analysis.values()\n",
    "                chkwordspace(wds,debug)\n",
    "                for role in analysis:\n",
    "                    if role not in permutationcollection:\n",
    "                        permutationcollection[role] = createpermutation(dimensionality)\n",
    "                u = getvector(analysis, sentence, debug, lexical, roles, train)\n",
    "                if (kk > 0):\n",
    "                    idx += 1\n",
    "                utterancespace[idx] = u\n",
    "                textvector = sparseadd(textvector, u, 1)\n",
    "                kk += 1\n",
    "    return textvector\n",
    "\n",
    "\n",
    "def getvector(roleworddict, sentencestring, debug=False, lexical=True, roles=True, character=True, train=True, style=False):\n",
    "    uvector = {}  # vector for test item\n",
    "    if roles:\n",
    "        for role in roleworddict:\n",
    "            item = roleworddict[role]\n",
    "            factor = 1\n",
    "            if item in indexspace:\n",
    "                factor = specificity(item)\n",
    "            elif not train:\n",
    "                factor = 0\n",
    "            uvector = sparseadd(uvector,\n",
    "                        permute(normalise(indexspace[item]),\n",
    "                                permutationcollection[role]), factor)\n",
    "    if lexical:  \n",
    "        wds = word_tokenize(sentencestring)\n",
    "        for item in wds:\n",
    "            factor = 1\n",
    "            if item in indexspace:\n",
    "                factor = specificity(item)\n",
    "            elif not train:\n",
    "                factor = 0\n",
    "            uvector = sparseadd(uvector, normalise(indexspace[item]), factor)\n",
    "    if character: \n",
    "        for n in 3,4,5:\n",
    "            ngrams = [sentencestring[i:i+n] for i in range(len(sentencestring)-n+1)]\n",
    "            for ngram in ngrams:\n",
    "                pass\n",
    "    if style:\n",
    "        wds = word_tokenize(sentencestring)\n",
    "        cpw = len(sentencestring)/len(wds)\n",
    "        wps = len(wds)\n",
    "    # seq nchargrams\n",
    "    # seq newordgrams\n",
    "    # adverbials\n",
    "    # hedges and amps\n",
    "    # verb classes use wordspace!\n",
    "    # sent sequences\n",
    "    return uvector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "organise weighting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 22 12:35:20 2018 <module>(): \n",
      "author A at Thu Feb 22 12:35:20 2018\n",
      "Thu Feb 22 12:35:21 2018 <module>(): \n",
      "author B at Thu Feb 22 12:35:21 2018\n",
      "Thu Feb 22 12:35:21 2018 <module>(): \n",
      "author C at Thu Feb 22 12:35:21 2018\n"
     ]
    }
   ],
   "source": [
    "def idftable(trainfile, category):\n",
    "    global df\n",
    "    global dfn\n",
    "    global cft\n",
    "    global categories\n",
    "    global sf\n",
    "    global sn\n",
    "    try:\n",
    "        sn\n",
    "    except NameError:\n",
    "        sn = 0\n",
    "    try:\n",
    "        categories\n",
    "    except NameError:\n",
    "        categories = set()\n",
    "    categories.update(category)\n",
    "    try:\n",
    "        dfn\n",
    "    except NameError:\n",
    "        dfn = 0\n",
    "    with codecs.open(trainfile,\"r\", encoding='utf-8') as infile:\n",
    "        dfn += 1\n",
    "        try:\n",
    "            rawtext = infile.read().lower()\n",
    "        except UnicodeDecodeError:\n",
    "            rawtext = \"\"\n",
    "            logger(\"File Read Problem: \"+trainfile, error)\n",
    "        wds=nltk.word_tokenize(rawtext)\n",
    "        for wd in set(wds):\n",
    "            if wd in df:\n",
    "                df[wd] += 1\n",
    "            else:\n",
    "                df[wd] = 1\n",
    "            if wd not in cft:\n",
    "                cft[wd] = [category]\n",
    "            else:\n",
    "                if category not in cft[wd]:\n",
    "                    cft[wd].append(category)\n",
    "        sents=nltk.sent_tokenize(rawtext)\n",
    "        for sentence in sents:\n",
    "            sn += 1\n",
    "            wds = nltk.word_tokenize(sentence)\n",
    "            for wd in set(wds):\n",
    "                if wd in sf:\n",
    "                    sf[wd] += 1\n",
    "                else:\n",
    "                    sf[wd] = 1                \n",
    "\n",
    "                    \n",
    "def x2table(trainfile, category):\n",
    "    global globalfrequency\n",
    "    global termfrequency\n",
    "    global categorysize\n",
    "    global bign\n",
    "    with codecs.open(trainfile,\"r\", encoding='utf-8') as infile:\n",
    "        try:\n",
    "            rawtext = infile.read().lower()\n",
    "        except UnicodeDecodeError:\n",
    "            rawtext = \"\"\n",
    "            logger(\"File Read Problem: \"+trainfile, error)\n",
    "        wds=nltk.word_tokenize(rawtext)\n",
    "        uniquewds = set(wds)\n",
    "        for wd in wds:\n",
    "            bign += 1\n",
    "            if wd not in termfrequency:\n",
    "                termfrequency[wd] = {}\n",
    "            if category not in termfrequency[wd]:\n",
    "                termfrequency[wd][category] = 1\n",
    "            else:\n",
    "                termfrequency[wd][category] += 1\n",
    "            if wd in globalfrequency:\n",
    "                globalfrequency[wd] += 1\n",
    "            else:\n",
    "                globalfrequency[wd] = 1\n",
    "            if category in categorysize:\n",
    "                categorysize[category] += 1\n",
    "            else:\n",
    "                categorysize[category] = 1\n",
    "\n",
    "\n",
    "traindir = \"/home/jussi/data/pan/pan12A/\"\n",
    "debug = False  # logger level\n",
    "error = True   # logger level\n",
    "monitor = True # logger level\n",
    "globalfrequency = {}\n",
    "termfrequency = {}\n",
    "categorysize = {}\n",
    "bign = 0\n",
    "for task in \"A\":\n",
    "    for author in \"A\", \"B\", \"C\":\n",
    "        logger(\"\\nauthor \"+author+\" at \"+time.ctime(),monitor)\n",
    "        av = {}\n",
    "        for item in \"1\", \"2\":\n",
    "            trainfile = traindir+\"clean.12\"+task+\"train\"+author+item+\".txt\"\n",
    "            x2table(trainfile,author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(wd):\n",
    "    if wd in cft:\n",
    "        c = math.log(len(categories)/(1+len(cft[wd]))) # (len(categories)/(len(categories)+1+len(cft[wd])))**4\n",
    "    else:\n",
    "        c = 0.1\n",
    "    if wd in df:\n",
    "        d = math.log(dfn/(1+df[wd])) # (dfn/(dfn+df[wd]+1))**4\n",
    "    else:\n",
    "        d = 0.1\n",
    "    if wd in sf:\n",
    "        s = math.log(sn/(sf[wd]+1)) # (sn/(sf[wd]+1+sn))**4\n",
    "    else:\n",
    "        s = 0.1\n",
    "    if wd in globalfrequency:\n",
    "        x = 0\n",
    "        for category in categorysize:\n",
    "            try:\n",
    "                e = globalfrequency[wd]*categorysize[category]/bign\n",
    "                x += (e-termfrequency[wd][category])**2/e\n",
    "            except KeyError:\n",
    "                pass\n",
    "    else:\n",
    "        x = 0.1\n",
    "    return (dx*x+dc*c+dd*d+ds*s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3722392740051326"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = 0\n",
    "dc = 0\n",
    "dd = 0\n",
    "dx = 1\n",
    "specificity(\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.52008462096288"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity(\"victor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370079\n",
      "7627\n",
      "\t B 12840\n",
      "\t A 10410\n",
      "\t C 7008\n",
      "30258\n",
      "30258\n",
      "B 12840\n",
      "A 10410\n",
      "C 7008\n",
      "30258\n"
     ]
    }
   ],
   "source": [
    "print(bign)\n",
    "s=0\n",
    "for i in globalfrequency:\n",
    "    s += globalfrequency[i]\n",
    "print(s)\n",
    "d = {}\n",
    "ss=0\n",
    "for c in categorysize:\n",
    "    d[c] = 0\n",
    "    ss += categorysize[c]\n",
    "    print(\"\\t\",c,categorysize[c])\n",
    "print(ss)\n",
    "sss = 0\n",
    "for i in termfrequency:\n",
    "    for k in termfrequency[i]:\n",
    "        d[k] += termfrequency[i][k]\n",
    "        sss +=  termfrequency[i][k]\n",
    "print(sss)\n",
    "ssss = 0\n",
    "for c in categorysize:\n",
    "    print(c,d[c])\n",
    "    ssss += d[c]\n",
    "print(ssss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30286"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "Thu Feb 22 12:37:08 2018 <module>(): \n",
      "author A at Thu Feb 22 12:37:08 2018\n",
      "......Thu Feb 22 12:37:34 2018 <module>(): \n",
      "author B at Thu Feb 22 12:37:34 2018\n",
      ".....Thu Feb 22 12:38:07 2018 <module>(): \n",
      "author C at Thu Feb 22 12:38:07 2018\n",
      "..."
     ]
    }
   ],
   "source": [
    "traindir = \"/home/jussi/data/pan/pan12A/\"\n",
    "denseness = 0.2\n",
    "chkwordspace([\"epsilon\"])\n",
    "debug = False  # logger level\n",
    "error = True   # logger level\n",
    "monitor = True # logger level\n",
    "\n",
    "idx = 0\n",
    "dc = 0\n",
    "dd = 0\n",
    "ds = 0\n",
    "dx = 1\n",
    "print(\"X\")\n",
    "trainfiles = {}\n",
    "authorspace = {}\n",
    "indexspace = {}\n",
    "utterancespace = {}\n",
    "permutationcollection = {}\n",
    "for task in \"A\":\n",
    "    for author in \"A\", \"B\", \"C\":\n",
    "        logger(\"\\nauthor \"+author+\" at \"+time.ctime(),monitor)\n",
    "        av = {}\n",
    "        for item in \"1\", \"2\":\n",
    "            trainfile = traindir+\"clean.12\"+task+\"train\"+author+item+\".txt\"\n",
    "            filevector = readthefile(trainfile, author, debug, True)\n",
    "            trainfiles[trainfile] = filevector\n",
    "            av = sparseadd(av,filevector)\n",
    "        authorspace[author] = av\n",
    "print()\n",
    "for wwd in \"the\",\"victor\",\"eat\":\n",
    "    print(wwd,specificity(wwd))\n",
    "for file in trainfiles:\n",
    "    res = {}\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(trainfiles[file],authorspace[author])\n",
    "    key = file[-6:-5]\n",
    "    print(key, sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "print()\n",
    "testfiles = {}\n",
    "for task in \"A\":\n",
    "    logger(\"testing at \"+time.ctime(),monitor)\n",
    "    for item in \"1\", \"2\", \"3\", \"4\", \"5\", \"6\":\n",
    "        testfile = traindir+\"clean.12\"+task+\"test\"+\"0\"+item+\".txt\"\n",
    "        filevector = readthefile(testfile, author, debug, False)\n",
    "        testfiles[testfile] = filevector\n",
    "facit = {\"1\":\"B\", \"2\":\"A\", \"3\":\"A\", \"4\":\"C\", \"5\":\"C\", \"6\":\"B\"}\n",
    "print()\n",
    "for file in testfiles:\n",
    "    res = {}\n",
    "    key = file[-5]\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(testfiles[file],authorspace[author])\n",
    "    print(key, facit[key], sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "    \n",
    "idx = 0\n",
    "dx = 0\n",
    "dc = 0\n",
    "dd = 0\n",
    "ds = 1\n",
    "print(\"S\")\n",
    "trainfiles = {}\n",
    "authorspace = {}\n",
    "indexspace = {}\n",
    "utterancespace = {}\n",
    "permutationcollection = {}\n",
    "for task in \"A\":\n",
    "    for author in \"A\", \"B\", \"C\":\n",
    "        logger(\"\\nauthor \"+author+\" at \"+time.ctime(),monitor)\n",
    "        av = {}\n",
    "        for item in \"1\", \"2\":\n",
    "            trainfile = traindir+\"clean.12\"+task+\"train\"+author+item+\".txt\"\n",
    "            filevector = readthefile(trainfile, author, debug, True)\n",
    "            trainfiles[trainfile] = filevector\n",
    "            av = sparseadd(av,filevector)\n",
    "        authorspace[author] = av\n",
    "print()\n",
    "for file in trainfiles:\n",
    "    res = {}\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(trainfiles[file],authorspace[author])\n",
    "    key = file[-6:-5]\n",
    "    print(key, sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "print()\n",
    "testfiles = {}\n",
    "for task in \"A\":\n",
    "    logger(\"testing at \"+time.ctime(),monitor)\n",
    "    for item in \"1\", \"2\", \"3\", \"4\", \"5\", \"6\":\n",
    "        testfile = traindir+\"clean.12\"+task+\"test\"+\"0\"+item+\".txt\"\n",
    "        filevector = readthefile(testfile, author, debug, False)\n",
    "        testfiles[testfile] = filevector\n",
    "facit = {\"1\":\"B\", \"2\":\"A\", \"3\":\"A\", \"4\":\"C\", \"5\":\"C\", \"6\":\"B\"}\n",
    "print()\n",
    "for file in testfiles:\n",
    "    res = {}\n",
    "    key = file[-5]\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(testfiles[file],authorspace[author])\n",
    "    print(key, facit[key], sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "    \n",
    "    \n",
    "idx = 0\n",
    "dc = 1\n",
    "dd = 0\n",
    "ds = 0\n",
    "print(\"C\")\n",
    "trainfiles = {}\n",
    "authorspace = {}\n",
    "indexspace = {}\n",
    "utterancespace = {}\n",
    "permutationcollection = {}\n",
    "for task in \"A\":\n",
    "    for author in \"A\", \"B\", \"C\":\n",
    "        logger(\"\\nauthor \"+author+\" at \"+time.ctime(),monitor)\n",
    "        av = {}\n",
    "        for item in \"1\", \"2\":\n",
    "            trainfile = traindir+\"clean.12\"+task+\"train\"+author+item+\".txt\"\n",
    "            filevector = readthefile(trainfile, author, debug, True)\n",
    "            trainfiles[trainfile] = filevector\n",
    "            av = sparseadd(av,filevector)\n",
    "        authorspace[author] = av\n",
    "print()\n",
    "for file in trainfiles:\n",
    "    res = {}\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(trainfiles[file],authorspace[author])\n",
    "    key = file[-6:-5]\n",
    "    print(key, sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "print()\n",
    "testfiles = {}\n",
    "for task in \"A\":\n",
    "    logger(\"testing at \"+time.ctime(),monitor)\n",
    "    for item in \"1\", \"2\", \"3\", \"4\", \"5\", \"6\":\n",
    "        testfile = traindir+\"clean.12\"+task+\"test\"+\"0\"+item+\".txt\"\n",
    "        filevector = readthefile(testfile, author, debug, False)\n",
    "        testfiles[testfile] = filevector\n",
    "facit = {\"1\":\"B\", \"2\":\"A\", \"3\":\"A\", \"4\":\"C\", \"5\":\"C\", \"6\":\"B\"}\n",
    "print()\n",
    "for file in testfiles:\n",
    "    res = {}\n",
    "    key = file[-5]\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(testfiles[file],authorspace[author])\n",
    "    print(key, facit[key], sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "    \n",
    "print(\"D\")\n",
    "idx = 0\n",
    "dc = 0\n",
    "dd = 1\n",
    "ds = 0\n",
    "trainfiles = {}\n",
    "authorspace = {}\n",
    "indexspace = {}\n",
    "utterancespace = {}\n",
    "permutationcollection = {}\n",
    "for task in \"A\":\n",
    "    for author in \"A\", \"B\", \"C\":\n",
    "        logger(\"author \"+author+\" at \"+time.ctime(),monitor)\n",
    "        av = {}\n",
    "        for item in \"1\", \"2\":\n",
    "            trainfile = traindir+\"clean.12\"+task+\"train\"+author+item+\".txt\"\n",
    "            filevector = readthefile(trainfile, author, debug, True)\n",
    "            trainfiles[trainfile] = filevector\n",
    "            av = sparseadd(av,filevector)\n",
    "        authorspace[author] = av\n",
    "print()\n",
    "for file in trainfiles:\n",
    "    res = {}\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(trainfiles[file],authorspace[author])\n",
    "    key = file[-6:-5]\n",
    "    print(key, sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")\n",
    "print()\n",
    "testfiles = {}\n",
    "for task in \"A\":\n",
    "    logger(\"testing at \"+time.ctime(),monitor)\n",
    "    for item in \"1\", \"2\", \"3\", \"4\", \"5\", \"6\":\n",
    "        testfile = traindir+\"clean.12\"+task+\"test\"+\"0\"+item+\".txt\"\n",
    "        filevector = readthefile(testfile, author, debug, False)\n",
    "        testfiles[testfile] = filevector\n",
    "facit = {\"1\":\"B\", \"2\":\"A\", \"3\":\"A\", \"4\":\"C\", \"5\":\"C\", \"6\":\"B\"}\n",
    "print()\n",
    "for file in testfiles:\n",
    "    res = {}\n",
    "    key = file[-5]\n",
    "    for author in authorspace:\n",
    "        res[author] = sparsecosine(testfiles[file],authorspace[author])\n",
    "    print(key, facit[key], sorted(res.items(),key=lambda i: i[1], reverse=True), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B 6 B 0.927\n",
      "A 6 B 0.9593\n",
      "C 6 B 0.956\n",
      "B 1 B 0.8952\n",
      "A 1 B 0.9546\n",
      "C 1 B 0.9519\n",
      "B 5 C 0.9003\n",
      "A 5 C 0.9584\n",
      "C 5 C 0.9568\n",
      "B 3 A 0.8985\n",
      "A 3 A 0.9712\n",
      "C 3 A 0.9621\n",
      "B 4 C 0.7994\n",
      "A 4 C 0.8718\n",
      "C 4 C 0.8695\n",
      "B 2 A 0.926\n",
      "A 2 A 0.975\n",
      "C 2 A 0.9587\n"
     ]
    }
   ],
   "source": [
    "for l in testfiles:\n",
    "    for k in authorspace:\n",
    "        print(k,l[-5],facit[l[-5]],sparsecosine(authorspace[k],testfiles[l])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
